{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['JDBC_HOST'] = 'jrtest01-splice-hregion'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup-- \n",
    "import os\n",
    "import pyspark\n",
    "from splicemachine.spark.context import PySpliceContext\n",
    "\n",
    "\n",
    "# make sure pyspark tells workers to use python3 not 2 if both are installed\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"https://doc.splicemachine.com/zeppelin/css/zepstyles2.css\" />\n",
    "\n",
    "# Using our Native Spark DataSource\n",
    "This notebook demonstrates using the Spark Adapter with Python, in these steps:\n",
    "\n",
    "1. *Create the `PySpliceContextClass` to interface with the Python API.*\n",
    "2. *Use Jupyter to create a Spark context.*\n",
    "3. *Create a simple table in Splice Machine.*\n",
    "4. *Create a Spark dataframe and insert that into Splice Machine.*\n",
    "5. *Run a simple Splice Machine transaction using the Spark context.*\n",
    "6. *Rollback that transaction using the same context.*\n",
    "\n",
    "## About the Native Spark DataSource\n",
    "Data Scientists have adopted Spark as the de facto data science platform, and Splice Machine provides an industry leading in-process integration to a Spark cluster. This means data scientists and data engineers can adopt the full power of Spark and manipulate dataframes but also get the power of full ANSI, ACID-compliant SQL.\n",
    "\n",
    "The Splice Machine Spark adapter provides:\n",
    "\n",
    "* A durable, ACID compliant persistence model for Spark Dataframes.\n",
    "* Lazy result sets returned as Spark Dataframes.\n",
    "* Access to Spark libraries such as MLLib and GraphX.\n",
    "* Avoidance of expensive ETL of data from OLTP to OLAP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import the PySpliceContext Class\n",
    "\n",
    "Your first step is to import the `PySpliceContext` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splicemachine.spark.context import PySpliceContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create a Spark context and PySpliceContext\n",
    "\n",
    "Next, we create a spark session.\n",
    "Then, we use the `PySpliceContext` to create a connection to Splice Machine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from splicemachine.spark.context import PySpliceContext\n",
    "jdbc_host = os.environ['JDBC_HOST']\n",
    "\n",
    "conf = pyspark.SparkConf()\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "splicejdbc=f'jdbc:splice://{jdbc_host}:1527/splicedb;user=splice;password=admin'\n",
    "\n",
    "splice = PySpliceContext(spark, splicejdbc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create a Simple Table\n",
    "\n",
    "Now we create simple table in Splice Machine that we'll subsequently populate:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "\n",
    "create table DS.foo (I int, F float, V varchar(100), primary key (I));\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create a Spark Dataframe and Insert into Splice Machine\n",
    "\n",
    "Then we use `spark.pyspark` to create a Spark dataframe from some sample data, and insert that into our Splice Machine table.\n",
    "\n",
    "<p class=\"noteNote\">You can ignore the <code>RuntimeWarning:</code> warning messages that may display when you run the code in the next paragraph.</p>\n",
    "\n",
    "After inserting the data, we do a `select *` to display the contents of the Splice Machine table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "l = [(0,3.14,'Turing'), (1,4.14,'Newell'), (2,5.14,'Simon'), (3,6.14,'Minsky')]\n",
    "rdd = sc.parallelize(l)\n",
    "rows = rdd.map(lambda x: Row(I=x[0], F=float(x[1]), V=str(x[2])))\n",
    "schemaRows = sqlContext.createDataFrame(rows)\n",
    "splice.insert(schemaRows,'DS.foo')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Native Spark Datasource\n",
    "\n",
    "If you look closely, you'll see that we went straight from a Spark Dataframe into Splice Machine's database. \n",
    "\n",
    "This is Splice Machine's Native Spark Datasource.  Not only is this mechanism convenient, it is also very performant, leveraging parallelism in large datasets.  The main API for the Python version we just used is just in the paragraph at the top of this notebook.\n",
    "\n",
    "Here's the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "select * from DS.foo;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run a Simple Splice Machine Transaction\n",
    "\n",
    "Now we'll add more data to that table in a transactional context: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = splice.getConnection()\n",
    "conn.setAutoCommit(False)\n",
    "l = [(4,3.14,'Turing'), (5,4.14,'Newell'), (6,5.14,'Simon'), (7,6.14,'Minsky')]\n",
    "rdd = sc.parallelize(l)\n",
    "rows = rdd.map(lambda x: Row(I=x[0], F=float(x[1]), V=str(x[2])))\n",
    "schemaRows = sqlContext.createDataFrame(rows)\n",
    "splice.insert(schemaRows,'DS.foo')\n",
    "df = splice.df(\"select * from DS.foo\")\n",
    "df.collect\n",
    "z.show(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Rollback the transaction\n",
    "\n",
    "Finally, we'll rollback the transaction we just ran:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.rollback()\n",
    "df = splice.df(\"select * from DS.foo\")\n",
    "df.collect\n",
    "z.show(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to Go Next\n",
    "\n",
    "Now let's explore Machine Learning with these Splice Machine, starting with our next notebook: [*Machine Learning with MLlib*](./g.%20Machine%20Learning%20with%20Spark%20MLlib.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
