{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['JDBC_HOST'] = 'jrtest01-splice-hregion'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup-- \n",
    "import os\n",
    "import pyspark\n",
    "from splicemachine.spark.context import PySpliceContext\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "!conda install plotly\n",
    "\n",
    "# make sure pyspark tells workers to use python3 not 2 if both are installed\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3'\n",
    "jdbc_host = os.environ['JDBC_HOST']\n",
    "\n",
    "conf = pyspark.SparkConf()\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "splicejdbc=f\"jdbc:splice://{jdbc_host}:1527/splicedb;user=splice;password=admin\"\n",
    "\n",
    "splice = PySpliceContext(spark, splicejdbc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMeans\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMeans\n",
    "KMeans is an unsupervised-learning, clustering algorithm used to determine similarities and trends within a given dataset\n",
    "KMeans is an iterative process, where K clusters are created by the user and continualy computed on a given dataset until the data converges and the algorithm ends.\n",
    "## Setting up a KMeans\n",
    "To implement KMeans, you will need two things:\n",
    "\n",
    "* A dataset of structured data. Learn about structured data [here](https://www.quora.com/What-are-Structured-semi-structured-and-unstructured-data-in-Big-Data/answer/Manoj-R-Patil?srid=33JGI)\n",
    "* A value for K. K can be computed a number of ways, none of which are necessarily incorrect. It is dependent on the specific dataset you are working with. It is suggested to first plot your data and do trials with multiple values of K. Learn more about choosing a good K [here](https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set)\n",
    "\n",
    "## Computing KMeans\n",
    "A KMeans algorithm is computed in three main steps:\n",
    "1. K clusters are created and assigned locations, either randomly generated or randomly taken from K datapoints\n",
    "2. For each datapoint in your dataset, the square Euclidian Distance is computed against all clusters until a minimum is found. That datapoint is assigned to the cluster of minimum distance\n",
    "3. After all datapoints are assigned, clusters are recomputed and reassigned locations using the mean distance of its assigned datapoints.\n",
    "\n",
    "#### Steps 2 and 3 are repeated until one of the following:\n",
    "* A set number of iterations occurs\n",
    "* No datapoints are reassigned to new clusters\n",
    "* Minimum distance changes occur within clusters\n",
    "\n",
    "Learn more about KMeans algorithm [here](https://www.datascience.com/blog/introduction-to-k-means-clustering-algorithm-learn-data-science-tutorials)\n",
    "In-depth KMeans clustering documentation [here](http://scikit-learn.org/stable/modules/clustering.html#k-means)\n",
    "\n",
    "#### We will now create a simple KMeans example.\n",
    "To follow along with this data, download [here](https://raw.githubusercontent.com/datascienceinc/learn-data-science/master/Introduction-to-K-means-Clustering/Data/data_1024.csv)\n",
    "More KMeans and Scala examples [here](https://github.com/apache/spark/tree/master/examples/src/main/scala/org/apache/spark/examples/mllib)\n",
    "\n",
    "Learn about Sum of Squares for Errors (used later in algorithm) [here](http://www.wikihow.com/Calculate-the-Sum-of-Squares-for-Error-(SSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%scala \n",
    "import org.apache.spark.{SparkConf, SparkContext}\n",
    "import org.apache.spark.mllib.clustering.{KMeans, KMeansModel}\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "\n",
    "//Grabbing data and parsing to vectors\n",
    "\n",
    "val data = sc.textFile(\"s3a://splice-demo/sample_kmeans_data.txt\")\n",
    "val formatted =  data.map(s => s.split(\" \"))\n",
    "val parsedData = formatted.map(s => Vectors.dense(s))\n",
    "val rddData = sc.parallelize(parsedData)\n",
    "\n",
    "//Clustering data into 3 groups\n",
    "val K = 4\n",
    "val maxIterations = 5000\n",
    "val clusters = KMeans.train(rddData, K, maxIterations)\n",
    "\n",
    "//Compute cost Within Set Sum of Squared Errors\n",
    "val WSSSE = clusters.computeCost(rddData)\n",
    "println(\"Within Set Sum of Squared Errors: \" + WSSSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "import plotly.express as px\n",
    "\n",
    "data = spark.createDataFrame(px.data.iris()).drop('species_id')\n",
    "\n",
    "# Convert species column into int type\n",
    "si = StringIndexer(inputCol='species', outputCol='species_vec')\n",
    "\n",
    "# Create a vector of features\n",
    "cols = [c for c in data.columns if c != 'species' and c != 'petal_length']\n",
    "va = VectorAssembler(inputCols=cols, outputCol='features')\n",
    "\n",
    "# Define stages of a Pipeline for Spark\n",
    "pipeline = Pipeline(stages = [si, va])\n",
    "\n",
    "data = pipeline.fit(data).transform(data)\n",
    "\n",
    "# Show the final dataset\n",
    "data.orderBy('sepal_width').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the kmeans algorithm and print cluster results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains a k-means model.\n",
    "kmeans = KMeans().setK(3).setSeed(1)\n",
    "model = kmeans.fit(data)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(data)\n",
    "\n",
    "# Evaluate clustering by computing Silhouette score\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n",
    "\n",
    "# Shows the result.\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we will combine the cluster data to the dataset for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "centers = []\n",
    "for center in model.clusterCenters():\n",
    "    centers.append(center)\n",
    "# Match the schema of the centers dataframe to the predictions dataframe\n",
    "cents = pd.DataFrame(centers,columns=['sepal_length','sepal_width','petal_width'])\n",
    "# The 3 labels we are trying to cluster on\n",
    "cents['species'] = ['setosa','virginica','versicolor']\n",
    "# Add a column with the value 10 to all cluster center datapoints. This is for plotting purposes\n",
    "cents.insert(0,'center',[10]*len(cents))\n",
    "cents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the center datapoints to the predictions dataframe\n",
    "preds = predictions.toPandas()\n",
    "# Add a column with the value of 2 to all non-cluster-center datapoints. Again, for plotting purposes\n",
    "preds.insert(0, \"center\", [2]*len(preds), True)\n",
    "# Insert the cluster center data\n",
    "preds = preds.append(cents, sort=False)\n",
    "preds[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can plot the datapoints, colored by cluster, with the cluster centers as the large, diamond datapoints in the \"center\"\n",
    "The cluster center datapoints are larger due to the data formatting we performed in the cells above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use plotly to visualize our data\n",
    "px.scatter_3d(preds, x='sepal_length', y='sepal_width', z='petal_width',\n",
    "              color='species', symbol='center', size='center')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
