{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['JDBC_HOST'] = 'jrtest01-splice-hregion'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup-- \n",
    "import os\n",
    "import pyspark\n",
    "from splicemachine.spark.context import PySpliceContext\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# make sure pyspark tells workers to use python3 not 2 if both are installed\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3'\n",
    "jdbc_host = os.environ['JDBC_HOST']\n",
    "\n",
    "conf = pyspark.SparkConf()\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "splicejdbc=f'jdbc:splice://{jdbc_host}:1527/splicedb;user=splice;password=admin'\n",
    "\n",
    "splice = PySpliceContext(spark, splicejdbc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"https://doc.splicemachine.com/zeppelin/css/zepstyles2.css\" />\n",
    "\n",
    "# Machine Learning with Spark MLlib\n",
    "\n",
    "This notebook contains code that uses the Machine Learning (<em>ML</em>) Library embedded in Spark, *MLlib*, with the Splice Machine Spark Adapter to realize in-process machine learning. Specifically, the example in this notebook uses data that tracks international shipments to learn, and then predicts how late a shipment will be, based on various factors.\n",
    "\n",
    "If you're not familiar with Machine Learning with Spark MLlib, you can learn more about this library here: <a href=\"https://spark.apache.org/docs/latest/ml-guide.html\" target=\"_blank\">https://spark.apache.org/docs/latest/ml-guide.html</a>.\n",
    "\n",
    "The code in this project was written in the Scala programming language as well as the Python programming language.\n",
    "\n",
    "The remainder of this notebook contains these sections:\n",
    "\n",
    "* <em>Basic Terminology</em> defines a few major ML terms used in this notebook.\n",
    "* <em>About Our Sample Data</em> introduces the shipping data that we use. \n",
    "* <em>About our Learning Model</em> describes the learning model method we're using.\n",
    "* <em>Creating our Splice Machine Database</em> walks you through setting up our database with our sample data.\n",
    "* <em>Creating, Training, and Deploying our Learning Model</em> walks you through our Machine Learning sample code.\n",
    "* <em>Program Listing</em> contains a listing of all of the code used in this notebook.\n",
    "\n",
    "## Basic Terminology\n",
    "\n",
    "Here's some basic terminology you need to be familiar with to understand the code in this notebook. These descriptions are paraphrased from the <a href=\"https://spark.apache.org/docs/latest/ml-guide.html\" target=\"_blank\">above-mentioned Spark MLlib guide.</a>\n",
    "\n",
    "<table class=\"splicezep\">\n",
    "    <col />\n",
    "    <col />\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>Term</th>\n",
    "            <th>Description</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td class=\"ItalicFont\">DataFrame</td>\n",
    "            <td>A DataFrame is a basic Spark SQL concept. A DataFrame is similar to a table in a database: it contains rows of data with columns of varying types. The MLlib operates on datasets that are organized in DataFrames. </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td class=\"ItalicFont\">Pipeline</td>\n",
    "            <td>In MLlib, you chain together a sequence of algorithms, or <em>stages</em> that operate on your DataFrame into a <em>pipeline</em> that learns.</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td class=\"ItalicFont\">Transformer</td>\n",
    "            <td>An algorithm that transforms a DataFrame into another DataFrame. Each transformer implements a method named <code>transform</code> that converts the DataFrame, typically by appending additional columns to it. A <em>model</em> is a kind of transformer.</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td class=\"ItalicFont\">Estimator</td>\n",
    "            <td>A learning algorithm that trains or <em>fits</em> on a DataFrame and produces a <code>model</code>. Each estimator implements a method named <code>fit</code> that produces a model.</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "    \n",
    "\n",
    "## About our Sample Data\n",
    "\n",
    "We've obtained some actual shipping data that tracks international shipments between ports, and have imported that data into a Splice Machine database that we've named `ASN.` The tables of interest are named `SHIPMENT_IN_TRANSIT` and `SHIPMENT_HISTORY;` you'll see these table used in the sample code below. We also create a database table named `Features` that forms the basis of the DataFrame we use for our learning model; this is the table you'll see featured in this notebook's code. The idea of this model is to predict, in real-time, how late a specific shipment will be, based on past data and other factors. Over time, as more data is processed by the model, the predictions become more accurate. \n",
    "\n",
    "## About our Learning Model\n",
    "\n",
    "We use a Logistic Regression *estimator* as the final stage in our pipeline to produce a Logistic Regression Model of lateness from our data, and then deploy that model on a dataset to predict lateness.\n",
    "\n",
    "The estimator operates on data that is formatted into vectors of integers. Since most of the fields in  our input dataset contain string values, we need to convert any data that will be used by the estimator into this format, as you'll see below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Create our Splice Machine Database\n",
    "\n",
    "Before working with the MLlib, we need to create a Splice Machine database that contains the shipping data we're using. We:\n",
    "\n",
    "1. Connect to your database via JDBC\n",
    "2. Create the schema and tables\n",
    "3. Import the data\n",
    "4. Create our features table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Connect to your Database via JDBC\n",
    "First we'll configure the URL we'll use in our JDBC connection to Splice Machine. For this class, you can simply use the `defaultJDBCURL` assignment in the next paragraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defaultJDBCURL = \"\"\"jdbc:splice://localhost:1527/splicedb;user=splice;password=admin\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create the Schema and Tables\n",
    "\n",
    "We'll now create our new schema, make it our default schema, and then create the tables for the `shipment_in_transit` and `shipment_history` data that we will import.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "\n",
    "CREATE SCHEMA DS_ASN;\n",
    "SET SCHEMA DS_ASN;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "\n",
    "DROP TABLE IF EXISTS SHIPMENT_IN_TRANSIT;\n",
    "CREATE TABLE SHIPMENT_IN_TRANSIT(\n",
    "    SHIPMENTID VARCHAR(11) NOT NULL PRIMARY KEY,\n",
    "    STATUS VARCHAR(50),\n",
    "    SHIPMODE VARCHAR(30),\n",
    "    PRODUCT_DESCRIPTION VARCHAR(500),\n",
    "    CONSIGNEE VARCHAR(200),\n",
    "    SHIPPER VARCHAR(100),\n",
    "    ARRIVAL_DATE TIMESTAMP,\n",
    "    GROSS_WEIGHT_LB INTEGER,\n",
    "    GROSS_WEIGHT_KG INTEGER,\n",
    "    FOREIGN_PORT VARCHAR(50),\n",
    "    US_PORT VARCHAR(50),\n",
    "    VESSEL_NAME VARCHAR(40),\n",
    "    COUNTRY_OF_ORIGIN VARCHAR(40),\n",
    "    CONSIGNEE_ADDRESS VARCHAR(150),\n",
    "    SHIPPER_ADDRESS VARCHAR(150),\n",
    "    ZIPCODE VARCHAR(20),\n",
    "    NO_OF_CONTAINERS INTEGER,\n",
    "    CONTAINER_NUMBER VARCHAR(200),\n",
    "    CONTAINER_TYPE VARCHAR(80),\n",
    "    QUANTITY INTEGER,\n",
    "    QUANTITY_UNIT VARCHAR(10),\n",
    "    MEASUREMENT INTEGER,\n",
    "    MEASUREMENT_UNIT VARCHAR(5),\n",
    "    BILL_OF_LADING VARCHAR(20),\n",
    "    HOUSE_VS_MASTER CHAR(1),\n",
    "    DISTRIBUTION_PORT VARCHAR(40),\n",
    "    MASTER_BL VARCHAR(20),\n",
    "    VOYAGE_NUMBER VARCHAR(10),\n",
    "    SEAL VARCHAR(300),\n",
    "    SHIP_REGISTERED_IN VARCHAR(40),\n",
    "    INBOND_ENTRY_TYPE VARCHAR(30),\n",
    "    CARRIER_CODE VARCHAR(10),\n",
    "    CARRIER_NAME VARCHAR(40),\n",
    "    CARRIER_CITY VARCHAR(40),\n",
    "    CARRIER_STATE VARCHAR(10),\n",
    "    CARRIER_ZIP VARCHAR(10),\n",
    "    CARRIER_ADDRESS VARCHAR(200),\n",
    "    NOTIFY_PARTY VARCHAR(50),\n",
    "    NOTIFY_ADDRESS VARCHAR(200),\n",
    "    PLACE_OF_RECEIPT VARCHAR(50),\n",
    "    DATE_OF_RECEIPT TIMESTAMP\n",
    "    );\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "DROP TABLE IF EXISTS SHIPMENT_HISTORY;\n",
    "CREATE TABLE SHIPMENT_HISTORY(\n",
    "    SHIPMENTID VARCHAR(11) NOT NULL PRIMARY KEY,\n",
    "    STATUS VARCHAR(50),\n",
    "    SHIPMODE VARCHAR(30),\n",
    "    PRODUCT_DESCRIPTION VARCHAR(500),\n",
    "    CONSIGNEE VARCHAR(200),\n",
    "    SHIPPER VARCHAR(100),\n",
    "    ARRIVAL_DATE TIMESTAMP,\n",
    "    GROSS_WEIGHT_LB INTEGER,\n",
    "    GROSS_WEIGHT_KG INTEGER,\n",
    "    FOREIGN_PORT VARCHAR(50),\n",
    "    US_PORT VARCHAR(50),\n",
    "    VESSEL_NAME VARCHAR(40),\n",
    "    COUNTRY_OF_ORIGIN VARCHAR(40),\n",
    "    CONSIGNEE_ADDRESS VARCHAR(150),\n",
    "    SHIPPER_ADDRESS VARCHAR(150),\n",
    "    ZIPCODE VARCHAR(20),\n",
    "    NO_OF_CONTAINERS INTEGER,\n",
    "    CONTAINER_NUMBER VARCHAR(200),\n",
    "    CONTAINER_TYPE VARCHAR(80),\n",
    "    QUANTITY INTEGER,\n",
    "    QUANTITY_UNIT VARCHAR(10),\n",
    "    MEASUREMENT INTEGER,\n",
    "    MEASUREMENT_UNIT VARCHAR(5),\n",
    "    BILL_OF_LADING VARCHAR(20),\n",
    "    HOUSE_VS_MASTER CHAR(1),\n",
    "    DISTRIBUTION_PORT VARCHAR(40),\n",
    "    MASTER_BL VARCHAR(20),\n",
    "    VOYAGE_NUMBER VARCHAR(10),\n",
    "    SEAL VARCHAR(300),\n",
    "    SHIP_REGISTERED_IN VARCHAR(40),\n",
    "    INBOND_ENTRY_TYPE VARCHAR(30),\n",
    "    CARRIER_CODE VARCHAR(10),\n",
    "    CARRIER_NAME VARCHAR(40),\n",
    "    CARRIER_CITY VARCHAR(40),\n",
    "    CARRIER_STATE VARCHAR(10),\n",
    "    CARRIER_ZIP VARCHAR(10),\n",
    "    CARRIER_ADDRESS VARCHAR(200),\n",
    "    NOTIFY_PARTY VARCHAR(50),\n",
    "    NOTIFY_ADDRESS VARCHAR(200),\n",
    "    PLACE_OF_RECEIPT VARCHAR(50),\n",
    "    DATE_OF_RECEIPT TIMESTAMP\n",
    ");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the Data\n",
    "\n",
    "Next we import the shipping data, which is in csv format, into our Splice Machine database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "call SYSCS_UTIL.IMPORT_DATA (\n",
    "     'DS_ASN',\n",
    "     'SHIPMENT_IN_TRANSIT',\n",
    "     null,\n",
    "     's3a://splice-demo/shipment/shipment_in_transit.csv',\n",
    "     '|',\n",
    "     null,\n",
    "     'yyyy-MM-dd HH:mm:ss.SSSSSS',\n",
    "     'yyyy-MM-dd',\n",
    "     null,\n",
    "     -1,\n",
    "     '/tmp',\n",
    "     true, null);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "call SYSCS_UTIL.IMPORT_DATA (\n",
    "     'DS_ASN',\n",
    "     'SHIPMENT_HISTORY',\n",
    "     null,\n",
    "     's3a://splice-demo/shipment/shipment_history.csv',\n",
    "     '|',\n",
    "     null,\n",
    "     'yyyy-MM-dd HH:mm:ss.SSSSSS',\n",
    "     'yyyy-MM-dd',\n",
    "     null,\n",
    "     -1,\n",
    "     '/tmp',\n",
    "     true, null);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create our Features Table\n",
    "\n",
    "We create a features table in our database that we use with our learning model. We add three computed fields in the `features` table that are important to our model:\n",
    "\n",
    "* `quantity_bin` categorizes shipping quantities into bins, to improve learning accuracy \n",
    "* `lateness` computes how many days late a shipment was\n",
    "* `label` categorizes lateness into one of four values:\n",
    "\n",
    "<table class=\"spliceZepNoBorder\" style=\"margin: 0 0 100px 50px;\">\n",
    "    <tbody>\n",
    "            <tr><td>0</td><td>0 days late</td></tr>\n",
    "            <tr><td>1</td><td>1-5 days late</td></tr>\n",
    "            <tr><td>2</td><td>5-10 days late</td></tr>\n",
    "            <tr><td>3</td><td>10 days or more late</td></tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "drop table IF EXISTS DS_ASN.FEATURES;\n",
    "CREATE table DS_ASN.FEATURES AS\n",
    "    SELECT \n",
    "    SHIPMENTID,\n",
    "    STATUS,\n",
    "    SHIPMODE,\n",
    "    PRODUCT_DESCRIPTION,\n",
    "    CONSIGNEE,\n",
    "    SHIPPER,\n",
    "    ARRIVAL_DATE,\n",
    "    GROSS_WEIGHT_LB,\n",
    "    GROSS_WEIGHT_KG,\n",
    "    FOREIGN_PORT,\n",
    "    US_PORT,\n",
    "    VESSEL_NAME,\n",
    "    COUNTRY_OF_ORIGIN,\n",
    "    CONSIGNEE_ADDRESS,\n",
    "    SHIPPER_ADDRESS,\n",
    "    ZIPCODE,\n",
    "    NO_OF_CONTAINERS,\n",
    "    CONTAINER_NUMBER,\n",
    "    CONTAINER_TYPE,\n",
    "    QUANTITY,\n",
    "    QUANTITY_UNIT,\n",
    "    MEASUREMENT,\n",
    "    MEASUREMENT_UNIT,\n",
    "    BILL_OF_LADING,\n",
    "    HOUSE_VS_MASTER,\n",
    "    DISTRIBUTION_PORT,\n",
    "    MASTER_BL,\n",
    "    VOYAGE_NUMBER,\n",
    "    SEAL,\n",
    "    SHIP_REGISTERED_IN,\n",
    "    INBOND_ENTRY_TYPE,\n",
    "    CARRIER_CODE,\n",
    "    CARRIER_NAME,\n",
    "    CARRIER_CITY,\n",
    "    CARRIER_STATE,\n",
    "    CARRIER_ZIP,\n",
    "    CARRIER_ADDRESS,\n",
    "    NOTIFY_PARTY,\n",
    "    NOTIFY_ADDRESS,\n",
    "    PLACE_OF_RECEIPT,\n",
    "    DATE_OF_RECEIPT,\n",
    "    CASE\n",
    "    WHEN DS_ASN.SHIPMENT_HISTORY.QUANTITY > 10\n",
    "    THEN\n",
    "        CASE\n",
    "            WHEN DS_ASN.SHIPMENT_HISTORY.QUANTITY > 100\n",
    "            THEN\n",
    "                CASE\n",
    "                    WHEN DS_ASN.SHIPMENT_HISTORY.QUANTITY > 1000\n",
    "                    THEN 3\n",
    "                    ELSE 2\n",
    "                END\n",
    "            ELSE 1\n",
    "    END\n",
    "    ELSE 0\n",
    "    END AS QUANTITY_BIN,\n",
    "    DS_ASN.SHIPMENT_HISTORY.DATE_OF_RECEIPT - DS_ASN.SHIPMENT_HISTORY.ARRIVAL_DATE as LATENESS,\n",
    "    CASE\n",
    "    WHEN  DS_ASN.SHIPMENT_HISTORY.DATE_OF_RECEIPT - DS_ASN.SHIPMENT_HISTORY.ARRIVAL_DATE > 0\n",
    "    THEN\n",
    "        CASE\n",
    "            WHEN  DS_ASN.SHIPMENT_HISTORY.DATE_OF_RECEIPT - DS_ASN.SHIPMENT_HISTORY.ARRIVAL_DATE > 5\n",
    "            THEN\n",
    "                CASE\n",
    "                    WHEN  DS_ASN.SHIPMENT_HISTORY.DATE_OF_RECEIPT - DS_ASN.SHIPMENT_HISTORY.ARRIVAL_DATE > 10\n",
    "                    THEN 3\n",
    "                    ELSE 2\n",
    "                END\n",
    "            ELSE 1\n",
    "    END\n",
    "    ELSE 0\n",
    "    END AS LABEL\n",
    "FROM DS_ASN.SHIPMENT_HISTORY "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create, Train, and Deploy our Learning Model\n",
    "\n",
    "The remainder of this notebook walks you through the code we use to create, train, and deploy our learning model, in these steps:\n",
    "\n",
    "1. Perform Spark+MLlib Setup Tasks\n",
    "2. Create our DataFrame\n",
    "3. Create Pipeline Stages\n",
    "4. Assemble the Pipeline>Train our Model\n",
    "5. Deploy our Model\n",
    "\n",
    "We include the entire program at the end of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Perform Spark + MLlib Setup Tasks\n",
    "\n",
    "The Python Splice Machine API communicates with your database through the `PySparkContext` class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "We use the following code to instantiate `PySpliceContext` and import our modules:\n",
    "\n",
    "```\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "splice = PySpliceContext(spark, defaultJDBCURL)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create our DataFrame\n",
    "\n",
    "Now we create a Python DataFrame from the results of a SQL `SELECT` query from the database. This allows us to manipulate our Splice Machine data as a Spark DataFrame:\n",
    "\n",
    "```\n",
    "df_with_uppercase_schema = PySpliceContext.df(\"select * from ASN.Features\")\n",
    "newNames = [\n",
    "    \"consignee\",\n",
    "    \"shipper\",\n",
    "    \"shipmode\",\n",
    "    \"gross_weight_lb\",\n",
    "    \"foreign_port\",\n",
    "    \"us_port\",\n",
    "    \"vessel_name\",\n",
    "    \"country_of_origin\",\n",
    "    \"container_number\",\n",
    "    \"container_type\",\n",
    "    \"quantity\",\n",
    "    \"ship_registered_in\",\n",
    "    \"carrier_code\",\n",
    "    \"carrier_city\",\n",
    "    \"notify_party\",\n",
    "    \"place_of_receipt\",\n",
    "    \"zipcode\",\n",
    "    \"quantity_bin\"\n",
    "    ]\n",
    "df = df_with_uppercase_schema.toDF(newNames)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create Pipeline Stages\n",
    "\n",
    "Our pipeline stages are fairly simple:\n",
    "\n",
    "* Transform each row of data in the input dataset into an integer vector.\n",
    "* Assemble the vectors into a DataFrame\n",
    "* Use a Logistic Regression Estimator to create our model\n",
    "\n",
    "#### Transform each row of data into an integer vector\n",
    "\n",
    "The Logistic Regression estimator operates on integer vectors, so we need to convert each row in our input dataframe into an integer vector. Remember that each row contains only the fields from our database that are of interest to our model: the fields that previously included in our sequence and concatenated onto our DataFrame.\n",
    "\n",
    "Spark includes a `StringIndexer` function that does exactly that, so we create a `StringIndexer` for each field, and we'll later use each of these as a stage in our learning pipeline. The `StringIndexer` transforms the data from a specified input column in our DataFrame and stores the output in a specified and new output column. By convention, we name each string indexer with the name of the field+`Indexer,` and name the output column the name of the field+`Index,` e.g. we create a transformer named `consigneeIndexer` to transform the input column `consignee` into the new output column `consigneeIndex.`\n",
    "\n",
    "```\n",
    "// Transform strings into numbers\n",
    "consigneeIndexer =  StringIndexer(inputCol=\"consignee\", outputCol=\"consigneeIndex\", handleInvalid=\"skip\")\n",
    "shipperIndexer = StringIndexer(inputCol=\"shipper\", outputCol=\"shipperIndex\", handleInvalid=\"skip\")\n",
    "shipmodeIndexer = StringIndexer(inputCol=\"shipmode\", outputCol=\"shipmodeIndex\", handleInvalid=\"skip\")\n",
    "gross_weight_lbIndexer = StringIndexer(inputCol=\"gross_weight_lb\", outputCol=\"gross_weight_lbIndex\", handleInvalid=\"skip\")\n",
    "foreign_portIndexer =  StringIndexer(inputCol=\"foreign_port\", outputCol=\"foreign_portIndex\", handleInvalid=\"skip\")\n",
    "us_portIndexer = StringIndexer(inputCol=\"us_port\", outputCol=\"us_portIndex\", handleInvalid=\"skip\")\n",
    "vessel_nameIndexer = StringIndexer(inputCol=\"vessel_name\", outputCol=\"vessel_nameIndex\",  handleInvalid=\"skip\")\n",
    "country_of_originIndexer = StringIndexer(inputCol=\"country_of_origin\", outputCol=\"country_of_originIndex\",  handleInvalid=\"skip\")\n",
    "container_numberIndexer = StringIndexer(inputCol=\"container_number\", outputCol=\"container_numberIndex\", handleInvalid=\"skip\")\n",
    "container_typeIndexer = StringIndexer(inputCol=\"container_type\", outputCol=\"container_typeIndex\", handleInvalid=\"skip\")\n",
    "ship_registered_inIndexer = StringIndexer(inputCol=\"ship_registered_in\", outputCol=\"ship_registered_inIndex\", handleInvalid=\"skip\")\n",
    "carrier_codeIndexer = StringIndexer(inputCol=\"carrier_code\", outputCol=\"carrier_codeIndex\", handleInvalid=\"skip\")\n",
    "carrier_cityIndexer = StringIndexer(inputCol=\"carrier_city\", outputCol=\"carrier_cityIndex\", handleInvalid=\"skip\")\n",
    "notify_partyIndexer = StringIndexer(inputCol=\"notify_party\", outputCol=\"notify_partyIndex\", handleInvalid=\"skip\")\n",
    "place_of_receiptIndexer = StringIndexer(inputCol=\"place_of_receipt\", outputCol=\"place_of_receiptIndex\", handleInvalid=\"skip\")\n",
    "zipcodeIndexer = StringIndexer(inputCol=\"zipcode\", outputCol=\"zipcodeIndex\", handleInvalid=\"skip\")\n",
    "```\n",
    "#### Assemble the Vectors\n",
    "\n",
    "After our pipeline has transformed data into numbers, we need to assemble those into vectors. Spark includes a `VectorAssembler` object that does just that, transforming a set of input columns into a vector that is stored in the `features` column in the DataFrame:\n",
    "\n",
    "```\n",
    "//assemble raw features\n",
    "assembler = VectorAssembler(inputCols=[\n",
    "                    \"shipmodeIndex\",\n",
    "                    \"consigneeIndex\",\n",
    "                    \"shipperIndex\",\n",
    "                    \"gross_weight_lbIndex\",\n",
    "                    \"foreign_portIndex\",\n",
    "                    \"us_portIndex\",\n",
    "                    \"vessel_nameIndex\",\n",
    "                    \"country_of_originIndex\",\n",
    "                    \"container_numberIndex\",\n",
    "                    \"container_typeIndex\",\n",
    "                    \"quantity_bin\",\n",
    "                    \"ship_registered_inIndex\",\n",
    "                    \"carrier_codeIndex\",\n",
    "                    \"carrier_cityIndex\",\n",
    "                    \"notify_partyIndex\",\n",
    "                    \"place_of_receiptIndex\",\n",
    "                    \"zipcodeIndex\",\n",
    "                    \"quantity_bin\"\n",
    "                    ], outputCol='features')\n",
    "```\n",
    "\n",
    "#### Create the Estimator\n",
    "\n",
    "Creating the estimator is a simple matter of specifying a few parameters, including which column in the DataFrame is the label, and which column contains the feature set:\n",
    "\n",
    "```\n",
    "//Create ML analytic\n",
    "lr = LogisticRegression(maxIter=30, labelCol=\"label\", featuresCol=\"features\", regParam=0.3)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Assemble our Pipeline\n",
    "\n",
    "```\n",
    "// Chain indexers and tree in a Pipeline\n",
    "lrPipeline = Pipeline(stages=\n",
    "        [consigneeIndexer,\n",
    "        shipperIndexer,\n",
    "        shipmodeIndexer,\n",
    "        gross_weight_lbIndexer,\n",
    "        foreign_portIndexer,\n",
    "        us_portIndexer,\n",
    "        vessel_nameIndexer,\n",
    "        country_of_originIndexer,\n",
    "        container_numberIndexer,\n",
    "        container_typeIndexer,\n",
    "        ship_registered_inIndexer,\n",
    "        carrier_codeIndexer,\n",
    "        carrier_cityIndexer,\n",
    "        notify_partyIndexer,\n",
    "        place_of_receiptIndexer,\n",
    "        zipcodeIndexer,\n",
    "        assembler,\n",
    "        lr]\n",
    "        )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Train our Model\n",
    "Now that our pipeline is set up, all we need to do to train our model is feed our dataframe into the pipeline's `fit` method, which learns from the data. \n",
    "```\n",
    "// Train model. \n",
    "lrModel = lrPipeline.fit(df)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Materialize the Model\n",
    "\n",
    "After training our model, we can apply it to real data and display the results. For simplicity sake, in this example, we'll simply apply the model to our feature table itself.\n",
    "\n",
    "```\n",
    "lrModel.transform(df).select(\"prediction\", \"probability\", \"features\").show(100)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "select *  from DS_ASN.features { limit 100 }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Python Code\n",
    "\n",
    "Our Python code is listed in the next paragraph.\n",
    "\n",
    "\n",
    "<p class=\"noteNote\">You can ignore the <code>RuntimeWarning:</code> warning messages that may display when you run the code in the next paragraph.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import print_function\n",
    "import string\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "# Query Features\n",
    "#splice was created above\n",
    "query_results = splice.df(\"select * from DS_ASN.Features\")\n",
    "newNames = [\"shipmentid\",\n",
    "            \"status\",\n",
    "            \"shipmode\",\n",
    "            \"product_description\",\n",
    "            \"consignee\",\n",
    "            \"shipper\",\n",
    "            \"arrival_date\",\n",
    "            \"gross_weight_lb\",\n",
    "            \"gross_weight_kg\",\n",
    "            \"foreign_port\",\n",
    "            \"us_port\",\n",
    "            \"vessel_name\",\n",
    "            \"country_of_origin\",\n",
    "            \"consignee_address\",\n",
    "            \"shipper_address\",\n",
    "            \"zipcode\",\n",
    "            \"no_of_containers\",\n",
    "            \"container_number\",\n",
    "            \"container_type\",\n",
    "            \"quantity\",\n",
    "            \"quantity_unit\",\n",
    "            \"measurement\",\n",
    "            \"measurement_unit\",\n",
    "            \"bill_of_lading\",\n",
    "            \"house_vs_master\",\n",
    "            \"distribution_port\",\n",
    "            \"master_bl\",\n",
    "            \"voyage_number\",\n",
    "            \"seal\",\n",
    "            \"ship_registered_in\",\n",
    "            \"inbond_entry_type\",\n",
    "            \"carrier_code\",\n",
    "            \"carrier_name\",\n",
    "            \"carrier_city\",\n",
    "            \"carrier_state\",\n",
    "            \"carrier_zip\",\n",
    "            \"carrier_address\",\n",
    "            \"notify_party\",\n",
    "            \"notify_address\",\n",
    "            \"place_of_receipt\",\n",
    "            \"date_of_receipt\",\n",
    "            \"quantity_bin\",\n",
    "            \"lateness\",\n",
    "            \"label\"]\n",
    "\n",
    "df = query_results.toDF(*newNames)\n",
    "\n",
    "# Assemble Vectors\n",
    "assembler = VectorAssembler(inputCols=[\n",
    "    \"shipmodeIndex\",\n",
    "    \"consigneeIndex\",\n",
    "    \"shipperIndex\",\n",
    "    \"gross_weight_lbIndex\",\n",
    "    \"foreign_portIndex\",\n",
    "    \"us_portIndex\",\n",
    "    \"vessel_nameIndex\",\n",
    "    \"country_of_originIndex\",\n",
    "    \"container_numberIndex\",\n",
    "    \"container_typeIndex\",\n",
    "    \"ship_registered_inIndex\",\n",
    "    \"carrier_codeIndex\",\n",
    "    \"carrier_cityIndex\",\n",
    "    \"notify_partyIndex\",\n",
    "    \"place_of_receiptIndex\",\n",
    "    \"zipcodeIndex\",\n",
    "    \"quantity_bin\"\n",
    "], outputCol='features')\n",
    "\n",
    "# Transform strings into numbers\n",
    "zipcodeIndexer = StringIndexer(inputCol=\"zipcode\", outputCol=\"zipcodeIndex\", handleInvalid=\"skip\")\n",
    "consigneeIndexer = StringIndexer(inputCol=\"consignee\", outputCol=\"consigneeIndex\", handleInvalid=\"skip\")\n",
    "shipperIndexer = StringIndexer(inputCol=\"shipper\", outputCol=\"shipperIndex\", handleInvalid=\"skip\")\n",
    "statusIndexer = StringIndexer(inputCol=\"status\", outputCol=\"statusIndex\", handleInvalid=\"skip\")\n",
    "shipmodeIndexer = StringIndexer(inputCol=\"shipmode\", outputCol=\"shipmodeIndex\", handleInvalid=\"skip\")\n",
    "gross_weight_lbIndexer = StringIndexer(inputCol=\"gross_weight_lb\", outputCol=\"gross_weight_lbIndex\",\n",
    "                                       handleInvalid=\"skip\")\n",
    "foreign_portIndexer = StringIndexer(inputCol=\"foreign_port\", outputCol=\"foreign_portIndex\", handleInvalid=\"skip\")\n",
    "us_portIndexer = StringIndexer(inputCol=\"us_port\", outputCol=\"us_portIndex\", handleInvalid=\"skip\")\n",
    "vessel_nameIndexer = StringIndexer(inputCol=\"vessel_name\", outputCol=\"vessel_nameIndex\", handleInvalid=\"skip\")\n",
    "country_of_originIndexer = StringIndexer(inputCol=\"country_of_origin\", outputCol=\"country_of_originIndex\",\n",
    "                                         handleInvalid=\"skip\")\n",
    "container_numberIndexer = StringIndexer(inputCol=\"container_number\", outputCol=\"container_numberIndex\",\n",
    "                                        handleInvalid=\"skip\")\n",
    "container_typeIndexer = StringIndexer(inputCol=\"container_type\", outputCol=\"container_typeIndex\",\n",
    "                                      handleInvalid=\"skip\")\n",
    "distribution_portIndexer = StringIndexer(inputCol=\"distribution_port\", outputCol=\"distribution_portIndex\",\n",
    "                                         handleInvalid=\"skip\")\n",
    "ship_registered_inIndexer = StringIndexer(inputCol=\"ship_registered_in\", outputCol=\"ship_registered_inIndex\",\n",
    "                                          handleInvalid=\"skip\")\n",
    "inbond_entry_typeIndexer = StringIndexer(inputCol=\"inbond_entry_type\", outputCol=\"inbond_entry_typeIndex\",\n",
    "                                         handleInvalid=\"skip\")\n",
    "carrier_codeIndexer = StringIndexer(inputCol=\"carrier_code\", outputCol=\"carrier_codeIndex\", handleInvalid=\"skip\")\n",
    "carrier_cityIndexer = StringIndexer(inputCol=\"carrier_city\", outputCol=\"carrier_cityIndex\", handleInvalid=\"skip\")\n",
    "carrier_stateIndexer = StringIndexer(inputCol=\"carrier_state\", outputCol=\"carrier_stateIndex\", handleInvalid=\"skip\")\n",
    "carrier_zipIndexer = StringIndexer(inputCol=\"carrier_zip\", outputCol=\"carrier_zipIndex\", handleInvalid=\"skip\")\n",
    "notify_partyIndexer = StringIndexer(inputCol=\"notify_party\", outputCol=\"notify_partyIndex\", handleInvalid=\"skip\")\n",
    "place_of_receiptIndexer = StringIndexer(inputCol=\"place_of_receipt\", outputCol=\"place_of_receiptIndex\",\n",
    "                                        handleInvalid=\"skip\")\n",
    "\n",
    "lr = LogisticRegression(maxIter=30, labelCol=\"label\", featuresCol=\"features\", regParam=0.3)\n",
    "\n",
    "lrPipeline = Pipeline(stages=\n",
    "                      [consigneeIndexer,\n",
    "                       shipperIndexer,\n",
    "                       shipmodeIndexer,\n",
    "                       gross_weight_lbIndexer,\n",
    "                       foreign_portIndexer,\n",
    "                       us_portIndexer,\n",
    "                       vessel_nameIndexer,\n",
    "                       country_of_originIndexer,\n",
    "                       container_numberIndexer,\n",
    "                       container_typeIndexer,\n",
    "                       ship_registered_inIndexer,\n",
    "                       carrier_codeIndexer,\n",
    "                       carrier_cityIndexer,\n",
    "                       notify_partyIndexer,\n",
    "                       place_of_receiptIndexer,\n",
    "                       zipcodeIndexer,\n",
    "                       assembler,\n",
    "                       lr]\n",
    "                      )\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=lrPipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=2)  # use 3+ folds in practice\n",
    "\n",
    "# Run cross-validation, and choose the best parameters to demonstate GridSearch in Spark MLlib\n",
    "lrModel = crossval.fit(df)\n",
    "\n",
    "transformed_df = lrModel.transform(df)\n",
    "transformed_df.createOrReplaceTempView(\"res_view\")\n",
    "results = sqlContext.sql('SELECT prediction, probability, features from res_view')\n",
    "results.show(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "drop table IF EXISTS DS_ASN.TEST_FEATURES;\n",
    "CREATE table DS_ASN.TEST_FEATURES AS\n",
    "    SELECT \n",
    "    SHIPMENTID,\n",
    "    STATUS,\n",
    "    SHIPMODE,\n",
    "    PRODUCT_DESCRIPTION,\n",
    "    CONSIGNEE,\n",
    "    SHIPPER,\n",
    "    ARRIVAL_DATE,\n",
    "    GROSS_WEIGHT_LB,\n",
    "    GROSS_WEIGHT_KG,\n",
    "    FOREIGN_PORT,\n",
    "    US_PORT,\n",
    "    VESSEL_NAME,\n",
    "    COUNTRY_OF_ORIGIN,\n",
    "    CONSIGNEE_ADDRESS,\n",
    "    SHIPPER_ADDRESS,\n",
    "    ZIPCODE,\n",
    "    NO_OF_CONTAINERS,\n",
    "    CONTAINER_NUMBER,\n",
    "    CONTAINER_TYPE,\n",
    "    QUANTITY,\n",
    "    QUANTITY_UNIT,\n",
    "    MEASUREMENT,\n",
    "    MEASUREMENT_UNIT,\n",
    "    BILL_OF_LADING,\n",
    "    HOUSE_VS_MASTER,\n",
    "    DISTRIBUTION_PORT,\n",
    "    MASTER_BL,\n",
    "    VOYAGE_NUMBER,\n",
    "    SEAL,\n",
    "    SHIP_REGISTERED_IN,\n",
    "    INBOND_ENTRY_TYPE,\n",
    "    CARRIER_CODE,\n",
    "    CARRIER_NAME,\n",
    "    CARRIER_CITY,\n",
    "    CARRIER_STATE,\n",
    "    CARRIER_ZIP,\n",
    "    CARRIER_ADDRESS,\n",
    "    NOTIFY_PARTY,\n",
    "    NOTIFY_ADDRESS,\n",
    "    PLACE_OF_RECEIPT,\n",
    "    DATE_OF_RECEIPT,\n",
    "    CASE\n",
    "    WHEN DS_ASN.SHIPMENT_IN_TRANSIT.QUANTITY > 10\n",
    "    THEN\n",
    "        CASE\n",
    "            WHEN DS_ASN.SHIPMENT_IN_TRANSIT.QUANTITY > 100\n",
    "            THEN\n",
    "                CASE\n",
    "                    WHEN DS_ASN.SHIPMENT_IN_TRANSIT.QUANTITY > 1000\n",
    "                    THEN 3\n",
    "                    ELSE 2\n",
    "                END\n",
    "            ELSE 1\n",
    "\tEND\n",
    "    ELSE 0\n",
    "    END AS QUANTITY_BIN\n",
    "    FROM DS_ASN.SHIPMENT_IN_TRANSIT;\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "## Testing the Code\n",
    "\n",
    "Now we'll test our code on the `testing` features table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_data_with_uppercase_schema = splice.df('SELECT * FROM DS_ASN.TEST_FEATURES')\n",
    "\n",
    "newNames = [\"shipmentid\",\n",
    "            \"status\",\n",
    "            \"shipmode\",\n",
    "            \"product_description\",\n",
    "            \"consignee\",\n",
    "            \"shipper\",\n",
    "            \"arrival_date\",\n",
    "            \"gross_weight_lb\",\n",
    "            \"gross_weight_kg\",\n",
    "            \"foreign_port\",\n",
    "            \"us_port\",\n",
    "            \"vessel_name\",\n",
    "            \"country_of_origin\",\n",
    "            \"consignee_address\",\n",
    "            \"shipper_address\",\n",
    "            \"zipcode\",\n",
    "            \"no_of_containers\",\n",
    "            \"container_number\",\n",
    "            \"container_type\",\n",
    "            \"quantity\",\n",
    "            \"quantity_unit\",\n",
    "            \"measurement\",\n",
    "            \"measurement_unit\",\n",
    "            \"bill_of_lading\",\n",
    "            \"house_vs_master\",\n",
    "            \"distribution_port\",\n",
    "            \"master_bl\",\n",
    "            \"voyage_number\",\n",
    "            \"seal\",\n",
    "            \"ship_registered_in\",\n",
    "            \"inbond_entry_type\",\n",
    "            \"carrier_code\",\n",
    "            \"carrier_name\",\n",
    "            \"carrier_city\",\n",
    "            \"carrier_state\",\n",
    "            \"carrier_zip\",\n",
    "            \"carrier_address\",\n",
    "            \"notify_party\",\n",
    "            \"notify_address\",\n",
    "            \"place_of_receipt\",\n",
    "            \"date_of_receipt\",\n",
    "            \"quantity_bin\"]\n",
    "\n",
    "df= test_data_with_uppercase_schema.toDF(*newNames)\n",
    "lrPredictions = lrModel.transform(df)\n",
    "lrPredictions.createOrReplaceTempView(\"pred_view\")\n",
    "results = sqlContext.sql('SELECT prediction, probability, features FROM pred_view ORDER BY features')\n",
    "results.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "DROP TABLE IF EXISTS DS_ASN.PREDICTIONS;\n",
    "CREATE TABLE DS_ASN.PREDICTIONS (\n",
    "    SHIPMENTID VARCHAR(11) NOT NULL PRIMARY KEY,\n",
    "    PREDICTION DOUBLE\n",
    "    );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions = lrPredictions.select(\"SHIPMENTID\", \"PREDICTION\")\n",
    "predictions.printSchema()\n",
    "splice.insert(predictions, 'DS_ASN.predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "select * from DS_ASN.predictions;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to Go Next\n",
    "Now we're ready to move onto exploring other examples of Machine Learning with Splice Machine, starting with our [*KMeans Example*]((./g.%20KMeans%20Example.ipynb))."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
