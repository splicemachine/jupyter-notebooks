{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['JDBC_HOST'] = 'jrtest01-splice-hregion'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link rel=\"stylesheet\" href=\"https://doc.splicemachine.com/jupyter/css/custom.css\">\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<link rel=\"stylesheet\" href=\"https://doc.splicemachine.com/jupyter/css/custom.css\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup-- \n",
    "import os\n",
    "import pyspark\n",
    "from splicemachine.spark.context import PySpliceContext\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# make sure pyspark tells workers to use python3 not 2 if both are installed\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3'\n",
    "jdbc_host = os.environ['JDBC_HOST']\n",
    "\n",
    "conf = pyspark.SparkConf()\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "'''jdbc:splice://{FRAMEWORKNAME}-proxy.marathon.mesos:1527/splicedb;user=splice;password=admin'''\n",
    "\n",
    "splicejdbc=f'jdbc:splice://{jdbc_host}:1527/splicedb;user=splice;password=admin'\n",
    "\n",
    "splice = PySpliceContext(spark, splicejdbc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"https://doc.splicemachine.com/zeppelin/css/zepstyles2.css\" />\n",
    "\n",
    "# Exercises: Data Scientists\n",
    "\n",
    "This notebook contains follow-on exercises for the material that we covered in this class. You can complete these exercises and run the cells in this notebook to verify your work.\n",
    "\n",
    "You'll be performing the following actions in these exercises:\n",
    "\n",
    "1. Creating Tables\n",
    "2. Importing Data\n",
    "3. Visualizing Data\n",
    "4. Performing some basic Machine Learning\n",
    "\n",
    "The data you'll be loading is on your local machine, which will prove useful if you need to debug the data import process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisite Database Tables\n",
    "We'll start with three tables from a simple Movie Rating schema:\n",
    "\n",
    "* The `rating_data` table, which stores movie ratings, was introduced in the exercises for our *Developer Training, Part I* class.\n",
    "* The `user_data` table, which stores reviewer information, was added in the exercise for our *Developer Training, Part II* class.\n",
    "* In this notebook, you'll create and load the `item_data` table, which stores movie title and genre information.\n",
    "\n",
    "If you've completed the developer classes, you may already have these tables loaded in your database. If so, you can skip the next cell and start with the following cell, __1. Creating Tables.__\n",
    "\n",
    "Otherwise, please run the next cell to create and load the `RATING_DATA` and `USER_DATA` tables, before proceeding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "drop table if exists rating_data;\n",
    "create table RATING_DATA (\n",
    "    user_id bigint,\n",
    "    item_id bigint,\n",
    "    rating integer, \n",
    "    time_entered timestamp,\n",
    "    primary key (user_id, item_id)\n",
    ");\n",
    "\n",
    "call syscs_util.import_data('SPLICE', 'RATING_DATA', null, '/opt/data/rating.csv', '|', null, null, null, null, 0, '/opt/data/', null, null);\n",
    "analyze table rating_data;\n",
    "\n",
    "drop table if exists user_data;\n",
    "create table USER_DATA (\n",
    "  user_id bigint primary key,\n",
    "  age integer,\n",
    "  gender varchar(1),\n",
    "  occupation varchar(20),\n",
    "  zip varchar(10)\n",
    ");\n",
    "call syscs_util.import_data('SPLICE', 'USER_DATA', null, '/opt/data/user.csv', '|', null, null, null, null, 0, '/opt/data/', null, null);\n",
    "analyze table user_data;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating Tables\n",
    "\n",
    "Here we'll load one more table to enhance our Movie Data schema; this table will categorization information for individual movie titles. As you'll see, this will be useful for our Machine Learning exercices.\n",
    "\n",
    "### Our Sample Data\n",
    "\n",
    "The sample movie data that we're using is a table of movie titles and genre information. The raw data looks like:\n",
    "\n",
    "```\n",
    "1|Toy Story|0|0|1|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0\n",
    "2|GoldenEye|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0|1|0|0\n",
    "3|Four Rooms|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|1|0|0\n",
    "```\n",
    "\n",
    "The data fields contains the following fields:\n",
    "\n",
    "`item_id | movie title | Action | Adventure | Animation | Children's | Comedy | Crime | Documentary | Drama | Fantasy |`\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; `Film-Noir | Horror | Musical | Mystery | Romance | Sci-Fi | Thriller | War | Western`\n",
    "    \n",
    "Each of the columns after `movie title` is a genre flag; each flag has a value of either `1`, indicating that the genre does apply to this movie, or `0`, indicating that it does not.\n",
    "\n",
    "\n",
    "### Create the Table Definition\n",
    "\n",
    "Now, let's create a table specification for the movie and genre data shown above, and call it `ITEM_DATA`. Be sure to put in a Primary Key definition.\n",
    "\n",
    "<p class=\"noteQuestion\">What do you think the Primary Key should be?</p>\n",
    "\n",
    "Insert the SQL to create the table in the next cell and run it to create the table in your database.\n",
    "\n",
    "For help with the syntax, review the notebooks in this class, or read about creating tables in <a href=\"doc.splicemachine.com/sqlref_statements_createtable.html\" target=\"_blank\">our documentation.</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importing Data\n",
    "\n",
    "Now we'll import all of our movie/genre. We've copied the data file into this docker image, so you can examine it if needed; you'll find the data here:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; `opt/data/item.csv`\n",
    "\n",
    "Enter the proper `IMPORT` call to load the data in the next cell, then run the cell to load the data into the table in your database. You can review examples from this class or in our documentation for any required help.\n",
    "\n",
    "<p class=\"noteHint\">Use <code>/opt/data</code> as your BAD records file directory; if you have trouble with the import, you'll find valuable information in that directory.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualizing Data\n",
    "\n",
    "Now we have 3 tables loaded:\n",
    "\n",
    "* Movie Ratings\n",
    "* Movie Reviewers\n",
    "* Movies and their Genres\n",
    "\n",
    "And now our visualizations of data should become more interesting now as well. Explore using different visualizations by:\n",
    "\n",
    "1. Running the pre-entered queries in the next cells.\n",
    "2. Applying different visualization techniques to look for any possible interesting correlations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "select occupation, avg(cast(age as float)), avg(cast(rating as float)), count(*) from rating_data r, user_data u, item_data i where r.user_id = u.user_id and i.item_id = r.item_id and i.animation = 1 and i.childrens = 1 and comedy = 1 group by occupation order by avg(cast(rating as float)) desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "select occupation, count(*), avg(cast(rating as float)) from rating_data r, user_data u, item_data i where r.user_id = u.user_id and i.item_id = r.item_id group by occupation order by avg(cast(rating as float)) desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "select occupation, count(*), avg(cast(rating as float)) from rating_data r, user_data u, item_data i where r.user_id = u.user_id and i.item_id = r.item_id and horror = 1 group by occupation order by avg(cast(rating as float)) desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Now try your own queries and visualizations.  Can you find any correlations?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Machine Learning Example\n",
    "\n",
    "In the future we'll have a more in-depth Machine Learning exercise with the Movie Rating Data we have gone through.  But in the meantime we'll drill into a more streamlined example on Spam detection.\n",
    "\n",
    "Here we'll create a table to hold a set of SMS records, where each record has been labeled as \"ham\" or \"spam\" (where the spam label indicates a spam record).  We'll then apply a set of transformations and create a model for our own Spam predictor.\n",
    "\n",
    "First we'll create the table for the data, and go ahead and import it. To get you to the Machine Learning, we have provided the schema and import statements for you to run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "drop table if exists sms;\n",
    "CREATE TABLE SMS (\n",
    "    LABEL VARCHAR(10),\n",
    "    SMS_CONTENT VARCHAR(10000)\n",
    ");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We went ahead and provided the import statement for you to run here.  A couple of points:\n",
    "\n",
    "1.  This is tab-separated data, hence we are using `\\t` as the value of the field separator.\n",
    "2.  For string delimiters, we can't use our default since the data may contain double-quote characters.  Therefore the best option is to include an unprintable character (in this case `^A`) that we don't expect to see in the SMS strings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "call syscs_util.import_data ('SPLICE', 'SMS', null, '/opt/data/sms.csv', '\\t', '\u0001', null, null, null, 0, '/opt/data/', null, null);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's good to take a look at the data, perhaps filtering on the label to see what we have.  (Note - the data has NOT been cleansed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Native Spark DataSource to create DataFrame from Splice table\n",
    "Now create a variable `df` which is a dataframe of all records from the SMS table.  Additionally, pipeline the result of that dataframe with a call to `withColumnRenamed` from `LABEL` to `correct`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Preprocess the Data and Define generate_pipeline()\n",
    "\n",
    "We're now going to preprocess the data before learning:\n",
    "\n",
    "1. Tokenize the content into words (using `Tokenizer()`)\n",
    "2. Filter out words of little significance (using `StopWordsRemover()`)\n",
    "3. Hashing to get term frequency (using `HashingTF()`)\n",
    "4. IDF to establish term significance (using `IDF()`)\n",
    "5. Converting _ham_ and _spam_ to 0's and 1's\n",
    "\n",
    "Finish out the following definition of `generate_pipeline()`. We've started you off with the Tokenizer, Hashing, and IDF.  \n",
    "\n",
    "* Add a `StopWordsRemover()` and name it `remover` to make it work, based on the other code you see here.\n",
    "* Consult the Spark documentation for `StopWordsRemover` as necessary. Don't worry about any warnings when it runs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "FEATURE_NUM_HASHING = 7500 \n",
    "df = df.withColumnRenamed(\"LABEL\", \"correct\")\n",
    "\n",
    "def generate_pipeline(predicting=False):\n",
    "    tokenizer = Tokenizer().setInputCol(\"SMS_CONTENT\").setOutputCol(\"words\")\n",
    "\n",
    "    # StopWordsRemover code goes here to set up remover variable:\n",
    "    \n",
    "    \n",
    "    hashingTF = HashingTF().setNumFeatures(FEATURE_NUM_HASHING).setInputCol(\"filtered\").setOutputCol(\"rawFeatures\")\n",
    "    idf = IDF().setInputCol(\"rawFeatures\").setOutputCol(\"features\").setMinDocFreq(0)\n",
    "    stages = [tokenizer, remover, hashingTF, idf]\n",
    "    \n",
    "    if not predicting: # ignore label if we are predicting\n",
    "        labelidx = StringIndexer().setInputCol(\"correct\").setOutputCol(\"label\")\n",
    "        stages.append(labelidx)\n",
    "        \n",
    "    pipe = Pipeline(stages=stages)\n",
    "    return pipe\n",
    "    \n",
    "pipe = generate_pipeline()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation, modeling, and evaluation\n",
    "\n",
    "We'll use random iteration averaging to train and evaluate random samples from the dataset. This prevents overfitting.  Then we'll follow up with Logistic Regression for training and testing.\n",
    "\n",
    "Put in the proper `LogisticRegression()` code below (assiging to variable `lr`)\n",
    "\n",
    "Questions:\n",
    "1.  What is our training set size?  What is our testing set size?\n",
    "2.  Why is it important not to overfit?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from splicemachine.ml.utilities import SpliceBinaryClassificationEvaluator\n",
    "\n",
    "CV_ITERATIONS = 4\n",
    "TRAIN_SIZE = 0.7\n",
    "\n",
    "evaluator = SpliceBinaryClassificationEvaluator(spark)\n",
    "\n",
    "for iteration in range(1, CV_ITERATIONS + 1):\n",
    "    transformed = pipe.fit(df).transform(df)\n",
    "    train, test = transformed.randomSplit([TRAIN_SIZE, 1 - TRAIN_SIZE])\n",
    "    \n",
    "    # LogisticRegression initialization goes here\n",
    "\n",
    "    \n",
    "    fitlr = lr.fit(train)\n",
    "    predicted = fitlr.transform(test)\n",
    "    print(\"ITERATION {iteration}\".format(iteration=iteration))\n",
    "    evaluator.input(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use our model to make Predictions\n",
    "\n",
    "Now that we have trained and tested our model, it's time to make predictions.  This code is ready to go, so you don't need to make any changes.  Note that there is a text input field to enter sample SAS strings and get a prediction (Spam or Not Spam). \n",
    "\n",
    "Tests to run:\n",
    "1. _Run the following cell, in which we are testing the following SMS:_\n",
    "\n",
    "   ```\n",
    "   free gummies. call 1800-393-2939 to claim your prize.\n",
    "   ```\n",
    "\n",
    "   This evaluates the text in place.  Do you agree with the results?\n",
    "   <br />\n",
    "\n",
    "2. _Replace the SMS contents with:_\n",
    "\n",
    "   ```\n",
    "   George I got your message, please call me.\n",
    "   ```\n",
    "\n",
    "   Run the cell again.  Do you agree with the results?\n",
    "   <br />\n",
    "   \n",
    "3. _Try other strings as well._  Do you agree with the results?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StringType\n",
    "from splicemachine.ml.utilities import display\n",
    "\n",
    "text_contents = str(z.input(\"SMS Message Contents\"))\n",
    "if len(text_contents) > 1:\n",
    "    pred_df = sqlContext.createDataFrame([text_contents], StringType()).withColumnRenamed('value', 'SMS_CONTENT')\n",
    "    predPipe = generate_pipeline(predicting=True)\n",
    "\n",
    "    pred = predPipe.fit(df).transform(pred_df)\n",
    "    predictions = fitlr.transform(pred)\n",
    "    prob = predictions.select('probability').collect()[0][0][0]\n",
    "    if prob > 0.75:\n",
    "        display(\"<h2>This message is <b><i>Not Spam</i></b></h2>\")\n",
    "    else:\n",
    "        display(\"<h2>This message is <b><i>Spam</i></b></h2>\")\n",
    "   \n",
    "    display(\"<h3>Spam Probability: \" + str(round(1-prob, 4) * 100) + \"%</h3>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to Go Next\n",
    "Congratulations! You've just completed the *Splice Machine Data Science Class*. \n",
    "\n",
    "Visit [*Our Training Classes*](../About/Our%20Training%20Classes.ipynb) notebook to learn about our other training classes.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
