{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['JDBC_HOST'] = 'jrtest01-splice-hregion'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup-- \n",
    "import os\n",
    "import pyspark\n",
    "from splicemachine.spark.context import PySpliceContext\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# make sure pyspark tells workers to use python3 not 2 if both are installed\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3'\n",
    "jdbc_host = os.environ['JDBC_HOST']\n",
    "\n",
    "conf = pyspark.SparkConf()\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "'''jdbc:splice://{FRAMEWORKNAME}-proxy.marathon.mesos:1527/splicedb;user=splice;password=admin'''\n",
    "\n",
    "splicejdbc=f'jdbc:splice://{jdbc_host}:1527/splicedb;user=splice;password=admin'\n",
    "\n",
    "splice = PySpliceContext(spark, splicejdbc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "%defaultDatasource jdbc:splice://jrtest01-splice-hregion:1527/splicedb;user=splice;password=admin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"https://doc.splicemachine.com/zeppelin/css/zepstyles2.css\" />\n",
    "\n",
    "# ETL Pipeline Example\n",
    "\n",
    "This notebook presents a simple example of an ETL pipeline that reads a dataset from a public URL, performs simple transformations, and inserts data into a Splice Machine database.\n",
    "\n",
    "### Extract\n",
    "First we'll extract some data from a public URL, containing historical data about Telco customers and whether they discontinued service.  This is a good dataset for demonstrating machine learning algorithms for predicting customer churn:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import urllib2\n",
    "from io import StringIO\n",
    "import csv\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "#get file\n",
    "url = \"https://community.watsonanalytics.com/wp-content/uploads/2015/03/WA_Fn-UseC_-Telco-Customer-Churn.csv?cm_mc_uid=93701979699314920973859&cm_mc_sid_50200000=1492097385&cm_mc_sid_52640000=1492097385\"\n",
    "response = urllib2.urlopen(url)\n",
    "telcoText = csv.reader(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform\n",
    "\n",
    "Next we'll do some transformation of the data stream.  In this case we are doing some simple data cleansing and type conversions, and adding an additional column whose value is a function of other data in the existing columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#grab header and rows\n",
    "header = telcoText.next()\n",
    "header = [name.upper() for name in header]\n",
    "#handle some missing values\n",
    "rows = [row for row in telcoText if row[19] != ' ']\n",
    "#convert SeniorCitizen to boolean\n",
    "for i in range(len(rows)):\n",
    "    rows[i][2] = bool(int(rows[i][2]))\n",
    "    rows[i][5] = int(rows[i][5])\n",
    "    rows[i][18] = float(rows[i][18])\n",
    "    rows[i][19] = float(rows[i][19])\n",
    "\n",
    "#get fields for schema\n",
    "fields = []\n",
    "for col in header:\n",
    "    if col == 'SeniorCitizen':\n",
    "        t = BooleanType()\n",
    "    elif col == 'tenure':\n",
    "        t = IntegerType()\n",
    "    elif col == 'MonthlyCharges' or col=='TotalCharges':\n",
    "        t == DoubleType()\n",
    "    else:\n",
    "        t = StringType()\n",
    "    fields.append(StructField(col, t, True))\n",
    "#create schema\n",
    "schema = StructType(fields)\n",
    "\n",
    "#create DF\n",
    "telcoDF = sc.parallelize(rows).toDF(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "telcoDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load\n",
    "\n",
    "Now that we have the data we want to load into the database, we'll define the schema and load it directly in using the Native Spark DataSource:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "\n",
    "create table DS.etl_example(\n",
    "        CUSTOMERID  varchar(150),\n",
    "        gender varchar(50),\n",
    "        SeniorCitizen boolean,\n",
    "        Partner varchar(150),\n",
    "        Dependents varchar(150),\n",
    "        tenure int,\n",
    "        PhoneService varchar(150),\n",
    "        MultipleLines varchar(150),\n",
    "        InternetService varchar(150),\n",
    "        OnlineSecurity varchar(150),\n",
    "        OnlineBackup varchar(150),\n",
    "        DeviceProtection varchar(150),\n",
    "        TechSupport varchar(150),\n",
    "        StreamingTV varchar(150),\n",
    "        StreamingMovies varchar(150),\n",
    "        Contract varchar(150),\n",
    "        PaperlessBilling varchar(150),\n",
    "        PaymentMethod varchar(150),\n",
    "        MonthlyCharges double,\n",
    "        TotalCharges double,\n",
    "        Churn varchar(150));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "splice = PySpliceContext('jdbc:splice://localhost:1527/splicedb;user=splice;password=admin', sqlContext)\n",
    "\n",
    "splice.insert(telcoDF, 'DS.etl_example')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "select * from DS.etl_example;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to Go Next\n",
    "To complete this class, please complete the exercises in the  [*Exercises for This Class*](./j.%20Exercises.ipynb) notebook.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
