{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "os.environ['JDBC_HOST'] = 'jrtest01-splice-hregion'\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# setup-- \n",
                "import os\n",
                "import pyspark\n",
                "from splicemachine.spark.context import PySpliceContext\n",
                "from pyspark.conf import SparkConf\n",
                "from pyspark.sql import SparkSession\n",
                "\n",
                "# make sure pyspark tells workers to use python3 not 2 if both are installed\n",
                "os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3'\n",
                "jdbc_host = os.environ['JDBC_HOST']\n",
                "\n",
                "conf = pyspark.SparkConf()\n",
                "sc = pyspark.SparkContext(conf=conf)\n",
                "\n",
                "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
                "'''jdbc:splice://{FRAMEWORKNAME}-proxy.marathon.mesos:1527/splicedb;user=splice;password=admin'''\n",
                "\n",
                "splicejdbc=f'jdbc:splice://{jdbc_host}:1527/splicedb;user=splice;password=admin'\n",
                "\n",
                "splice = PySpliceContext(spark, splicejdbc)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<link rel=\"stylesheet\" href=\"https://doc.splicemachine.com/zeppelin/css/zepstyles2.css\" />\n",
                "\n",
                "# Splice Machine Basics\n",
                "\n",
                "Splice Machine is a scale-out SQL RDBMS, Data Warehouse, and Machine Learning Platform in one, seamlessly integrating analytics and AI into your mission-critical applications.\n",
                "\n",
                "This notebook gives a brief introduction to Splice Machine's capabilities in the following sections:\n",
                "\n",
                "<ul class=\"italic\">\n",
                "    <li>Hybrid Transactional and Analytical Processing</li>\n",
                "    <li>ANSI SQL Coverage</li>\n",
                "    <li>Architecture Overview</li>\n",
                "    <li>Technology Stack Overview</li>\n",
                "    <li>Internal Storage Using HBase</li>\n",
                "</ul>\n",
                "\n",
                "Once you've read through this notebook, we encourage you to spend an hour or two learning more about Splice Machine by engaging in one or more of our [Training Classes](../About/Our%20Training%20Classes.ipynb).\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Hybrid Transactional and Analytical Processing\n",
                "\n",
                "Splice Machine has a unique *Dual Engine* architecture that it uses to provide outstanding performance for concurrent transactional (OLTP) and analytical (OLAP) workloads. The SQL parser and cost-based optimizer analyze an incoming query and then determine the best execution plan based on query type, data sizes, available indexes and more. Based on that analysis, Splice Machine:\n",
                "\n",
                "* Deploys HBase for OLTP-type lookups, inserts and short range scans\n",
                "* Uses Spark for lightning-fast in-memory processing of analytical workloads.\n",
                "\n",
                "The Dual Engine architecture gives you the best of multiple worlds in a hybrid database: the performance, scale-out, and resilience of HBase, the in-memory analytics performance of Spark, and the performance of a cost-based optimizer.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ANSI SQL Coverage\n",
                "\n",
                "Unlike other Big Data systems, Splice Machine supports full [ANSI SQL-2003](https://doc.splicemachine.com/sqlref_sqlsummary.html); here's a quick summary of our coverage:\n",
                "\n",
                "<table class=\"splicezep\" summary=\"Summary of SQL features available in Splice Machine.\">\n",
                "    <colgroup>\n",
                "       <col>\n",
                "      <col>\n",
                "    </colgroup>\n",
                "    <thead>\n",
                "        <tr>\n",
                "            <th>Feature</th>\n",
                "            <th>Examples</th>\n",
                "        </tr>\n",
                "    </thead>\n",
                "   <tbody>\n",
                "        <tr>\n",
                "            <td><em>Aggregation functions</em></td>\n",
                "            <td><code>AVG, COUNT, MAX, MIN, STDDEV_POP, STDDEV_SAMP, SUM</code></td>\n",
                "        </tr>\n",
                "        <tr>\n",
                "            <td><em>Conditional functions</em></td>\n",
                "            <td><code>CASE, searched CASE</code></td>\n",
                "        </tr>\n",
                "        <tr>\n",
                "            <td><em>Data Types</em></td>\n",
                "            <td><code>INTEGER, REAL, CHARACTER, DATE, BOOLEAN, BIGINT</code></td>\n",
                "        </tr>\n",
                "        <tr>\n",
                "            <td><em>DDL</em></td>\n",
                "            <td><code>CREATE TABLE, CREATE&nbsp;SCHEMA, CREATE&nbsp;INDEX, ALTER&nbsp;TABLE, DELETE, UPDATE</code></td>\n",
                "        </tr>\n",
                "        <tr>\n",
                "            <td><em>DML</em></td>\n",
                "            <td><code>INSERT, DELETE, UPDATE, SELECT</code></td>\n",
                "        </tr>\n",
                "        <tr>\n",
                "            <td><em>Isolation Levels</em></td>\n",
                "            <td>Snapshot isolation</td>\n",
                "        </tr>\n",
                "        <tr>\n",
                "            <td><em>Joins</em></td>\n",
                "            <td><code>INNER&nbsp;JOIN, LEFT&nbsp;OUTER&nbsp;JOIN, RIGHT&nbsp;OUTER&nbsp;JOIN</code></td>\n",
                "        </tr>\n",
                "        <tr>\n",
                "            <td><em>Predicates</em></td>\n",
                "            <td><code>IN, BETWEEN, LIKE, EXISTS</code></td>\n",
                "        </tr>\n",
                "        <tr>\n",
                "            <td><em>Privileges</em></td>\n",
                "            <td>Privileges for <code>SELECT, DELETE, INSERT, EXECUTE</code></td>\n",
                "        </tr>\n",
                "        <tr>\n",
                "            <td><em>Query Specification</em></td>\n",
                "            <td><code>SELECT&nbsp;DISTINCT, GROUP&nbsp;BY, HAVING</code></td>\n",
                "        </tr>\n",
                "        <tr>\n",
                "            <td><em>SET&nbsp;functions</em></td>\n",
                "            <td><code>UNION, ABS, MOD, ALL, CHECK</code></td>\n",
                "        </tr>\n",
                "        <tr>\n",
                "            <td><em>String functions</em></td>\n",
                "            <td><code>CHAR, Concatenation (||), INSTR, LCASE&nbsp;(LOWER), LENGTH,<br>LTRIM, REGEXP_LIKE, REPLACE, RTRIM, SUBSTR, UCASE&nbsp;(UPPER), VARCHAR</code></td>\n",
                "        </tr>\n",
                "        <tr>\n",
                "            <td><em>Sub-queries</em></td>\n",
                "            <td>Yes</td>\n",
                "        </tr>\n",
                "        <tr>\n",
                "            <td><em>Transactions</em></td>\n",
                "            <td><code>COMMIT, ROLLBACK</code></td>\n",
                "        </tr>\n",
                "        <tr>\n",
                "            <td><em>Triggers</em></td>\n",
                "            <td>Yes</td>\n",
                "        </tr>\n",
                "        <tr>\n",
                "            <td><em>User-defined functions (UDFs)</em></td>\n",
                "            <td>Yes</td>\n",
                "        </tr>\n",
                "        <tr>\n",
                "            <td><em>Views</em></td>\n",
                "            <td>Including grouped views</td>\n",
                "        </tr>\n",
                "        <tr>\n",
                "            <td><em>Window functions</em></td>\n",
                "            <td><code>AVG, COUNT, DENSE_RANK, FIRST_VALUE, LAG, LAST_VALUE, LEAD, MAX, MIN, RANK, ROW_NUMBER, STDDEV_POP, STDDEV_SAMP, SUM</code></td>\n",
                "        </tr>\n",
                "    </tbody>\n",
                "</table>\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Architecture Overview\n",
                "\n",
                "The following diagram is a high-level representation of the architecture of Splice Machine:\n",
                "\n",
                "<img class=\"fitwidth\" src=\"https://doc.splicemachine.com/zeppelin/images/spliceArch1.png\">\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Technology Stack Overview\n",
                "\n",
                "Splice Machine is built on open-sourced, proven, distributed database technology, including HBase/Hadoop and Spark.\n",
                "\n",
                "### HBase/Hadoop\n",
                "\n",
                "The persistent, durable storage of operational data in Splice Machine resides in the Apache HBase key-value store. HBase:\n",
                "\n",
                "* is a non-relational (NoSQL) database that runs on top of HDFS\n",
                "* provides real-time read/write access to large datasets\n",
                "* scales linearly to handle huge data sets with billions of rows and millions of columns\n",
                "* is stored row-based and sorted by a primary key to deliver 1ms-10ms lookup speeds and short-range scans\n",
                "\n",
                "<img class=\"tiny\" src=\"https://hbase.apache.org/images/hbase_logo_with_orca.png\">\n",
                "\n",
                "HBase uses the Hadoop Distributed File System (HDFS) for reliable and replicated storage. HBase/HDFS provides auto-sharding and failover technology for scaling database tables across multiple servers. It is the only technology proven to scale to dozens of petabytes on commodity servers.\n",
                "\n",
                "### Spark In-Memory Computation Engine\n",
                "\n",
                "Splice Machine uses Spark for analytical processing.\n",
                "\n",
                "Apache Spark is a fast and general-purpose cluster computing system. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports a general execution graph on sets of data.\n",
                "\n",
                "<img class=\"tiny\" src=\"https://spark.apache.org/docs/latest/img/spark-logo-hd.png\">\n",
                "\n",
                "Spark has very efficient in-memory processing that can spill to disk (instead of dropping the query) if the query processing exceeds available memory. Spark is also unique in its resilience to node failures, which may occur in a commodity cluster. Other in-memory technologies will drop all queries associated with a failed node, while Spark uses ancestry (as opposed to replicating data) to regenerate its in-memory Resilient Distributed Datasets (RDDs) on another node.\n",
                "\n",
                "The main abstraction Spark provides is a resilient distributed dataset (RDD), which is a collection of elements partitioned across the nodes of the cluster that can be operated on in parallel. RDDs are created by starting with a file in the Hadoop file system (or any other Hadoop-supported file system), or an existing Scala collection in the driver program, and transforming it. Users may also ask Spark to persist an RDD in memory, allowing it to be reused efficiently across parallel operations. Finally, RDDs automatically recover from node failures.\n",
                "\n",
                "Spark is optimized to work on DataFrames, which are the main structure used by Spark. A DataFrame is a distributed collection of data (an RDD) organized into named columns, with a schema that specifies data types, that is designed to support efficiently operating on scalable, massive datasets.\n",
                "\n",
                "#### Spark RDD Operations\n",
                "\n",
                "RDDs support two types of operations: transformations, which create a new dataset from an existing one, and actions, which return a value to the driver program after running a computation on the dataset. For example, map is a transformation that passes each dataset element through a function and returns a new RDD representing the results. On the other hand, reduce is an action that aggregates all the elements of the RDD using some function and returns the final result to the driver program (although there is also a parallel reduceByKey that returns a distributed dataset).\n",
                "\n",
                "All transformations in Spark are lazy, in that they do not compute their results right away. Instead, they just remember the transformations applied to some base dataset (e.g. a file). The transformations are only computed when an action requires a result to be returned to the driver program. This design enables Spark to run more efficiently. For example, we can realize that a dataset created through map will be used in a reduce and return only the result of the reduce to the driver, rather than the larger mapped dataset.\n",
                "\n",
                "#### Spark Acceleration\n",
                "\n",
                "Splice Machine accelerates generation of Spark RDDs by reading HBase HFiles in HDFS and augmenting it with any changes in Memstore that have not been flushed to HFiles. Splice Machine then uses the RDDs and Spark operators to distribute processing across Spark Workers.\n",
                "\n",
                "### Resource Isolation\n",
                "\n",
                "Splice Machine isolates the resources allocated to HBase and Spark from each other, so each can progress independent of the workload of the other. Combined with the MVCC locking mechanism, this ensures that the performance level of transactional workloads can remain high, even if large reports or analytic processes are running.\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Internal Storage Using HBase\n",
                "\n",
                "Splice Machine uses HBase to internally store data. HBase is modeled after Google Big Table, which is a large, distributed associative map stored as a Log-Structured Merge Tree. In HBase:\n",
                "\n",
                "* Users store data rows in labelled tables.\n",
                "* Each data row has a sortable key and an aribtrary number of columns.\n",
                "* \n",
                "HBase is often misunderstood because many call it a column-oriented datastore. This just means columns are grouped in separately separately stored column families. But all data is still ordered by row.\n",
                "\n",
                "An HBase cluster has a service known as the *HBase Master* that coordinates the HBase Cluster and is responsible for administrative operations.\n",
                "\n",
                "Splice Machine also uses ZooKeeper, which is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services on a cluster. \n",
                "\n",
                "Here's a diagram showing how HBase operates in Splice Machine:\n",
                "<img class=\"splice\" src=\"https://s3.amazonaws.com/splice-examples/images/tutorials/hbases_storage_architecture2.png\">\n",
                "<br />\n",
                "### Region Servers and Regions\n",
                "\n",
                "HBase auto-shards the data in a table across *Region Servers*:\n",
                "\n",
                "* Each region server has a set of *Regions*.\n",
                "* Each region is a set of rows sorted by a primary key.\n",
                "* \n",
                "When a region server fails to respond, HBase makes its regions accessible on other region servers. HBase is resilient to region server failures as well as to failure of Hadoop Data Nodes. \n",
                "\n",
                "### HBase Data Storage\n",
                "\n",
                "HBase writes data to an in-memory store, called *memstore*. Once this memstore reaches a certain size, it is flushed to disk into a *store file*; everything is also written immediately to a log file for durability. \n",
                "\n",
                "The store files created on disk are immutable. Sometimes the store files are merged together, this is done by a process called *compaction*. Store files are on the Hadoop Distributed File System (<em>HDFS</em>) and are replicated for fault-tolerance. \n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Where to Go Next\n",
                "\n",
                "We recommend that you work through the Splice Machine training class that best fits how you plan to work with Splice Machine; these are described in the [*Our Training Classes*](../About/Our%20Training%20Classes.ipynb) notebook. Each of our classes ranges from 1-3 hours time to complete.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.3"
        },
        "toc": {
            "base_numbering": 1,
            "nav_menu": {},
            "number_sections": false,
            "sideBar": false,
            "skip_h1_title": false,
            "title_cell": "Table of Contents",
            "title_sidebar": "Contents",
            "toc_cell": false,
            "toc_position": {},
            "toc_section_display": false,
            "toc_window_display": false
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}