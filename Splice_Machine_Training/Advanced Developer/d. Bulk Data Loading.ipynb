{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link rel=\"stylesheet\" href=\"https://doc.splicemachine.com/jupyter/css/custom.css\">\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<link rel=\"stylesheet\" href=\"https://doc.splicemachine.com/jupyter/css/custom.css\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['JDBC_HOST'] = 'jrtest01-splice-hregion'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bulk Data Loading\n",
    "\n",
    "This notebook introduces the Splice Machine Bulk HFile Import mechanism, which you can use to import data into your database in a highly performant manner. \n",
    "We'll walk you through a simple example, after which you will be able to bulk import data into your database. \n",
    "\n",
    "<p class=\"noteIcon\">We recommend using Bulk HFile importing; however, if your input might contain data errors that need checking, you must use our our basic import procedures, <code>IMPORT_DATA</code> or <code>MERGE_DATA_FROM_FILE</code> instead, because they perform constraint checking during ingestion.</p>\n",
    "\n",
    "This notebook contains these sections:\n",
    "\n",
    "* *About Bulk Import* summarizes how bulk import works.\n",
    "* The *Bulk Data Import Checklist* summarizes important details about the format of the data you're importing.\n",
    "* The *Automatic Bulk Data Import* method shows you how to quickly bulk import your data.\n",
    "* The *Manual Bulk Data Import* allows you to control the splitting of data.\n",
    "* *Using the `BULK_IMPORT_HFILE` Command* shows the syntax for importing data using the `BULK_IMPORT_HFILE` system procedure.\n",
    "\n",
    "Lastly we will walk you through step-by-step examples of using both the automatic and manual process bulk data import processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Bulk Import\n",
    "\n",
    "Bulk importing splits a data file into temporary HFiles (store files), and then imports the data into your database by directly loading the generated StoreFiles into your Region Servers. \n",
    "\n",
    "When you use bulk HFile to import your data, files are essentially split into temporary Hadoop files (HFiles) that are directly loaded into the region servers. Ideally, the process creates HFiles of equal size, which allows the data to be spread across all region servers equally. There are two methods for splitting the data into hfiles.\n",
    "\n",
    "* With *Automatic Splitting,* the system samplse the data in an attempt to determine the optimal split points. This is the quicker and easier method, but does not always ensure you have the ideal splitting of the data.\n",
    "* Using *Manual Splitting* improves the performance of the bulk data import process. Instead of having the system samples the data for split point, you provide the split points that tell the procedure where to split your data. This requires you to know the data well enough to provide the split points.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bulk Data Import Checklist\n",
    "\n",
    "When you bulk import data from flat files into your database, you need to specify a number of details about your data files to get them correctly imported. Before starting this process, please make sure that your data formats will work. The following table calls out areas issues you should check:\n",
    "\n",
    "\n",
    "<table class=\"splicezepOddEven\">\n",
    "    <col width=\"30%\" />\n",
    "    <col />\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>Data File Detail</th>\n",
    "            <th>Specific Requirements</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>Field delimited?</td>\n",
    "            <td>The fields in each row <strong>must</strong> have delimiters between them.</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Rows terminated?</td>\n",
    "            <td>Each row <strong>must</strong> be terminated with a newline character.</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>Header row included?</td>\n",
    "            <td>Header rows are not allowed; if your data contains one, you <strong>must</strong> remove it.</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><code>Date</code>, <code>time</code>, <code>timestamp</code> data types</td>\n",
    "            <td> If you are using <code>date</code>, <code>time</code>, and/or <code>timestamp</code> data types in the target table, you need to know how that data is represented in the flat file; your file <strong>must</strong> use a consistent representation, and you must specify that format when using the import command.</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><code>Char</code> and <code>Varchar</code> data</td>\n",
    "            <td><p>If any of your <code>char</code> or <code>varchar</code> data contains your delimiter character, you <strong>need to use</strong> a special character delimiter.</p>\n",
    "                <p>If any of your <code>char</code> or <code>varchar</code> data contains newline characters, you <strong>need to use</strong> the <code>oneLineRecords</code> parameter.</p>\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>\n",
    "\n",
    "\n",
    "<p class=\"noteIcon\">It is a good idea to test your import on a small amount of data before loading all of your data. This allows you to verify that your delimiters, date formatting, and other details are in proper order. That's what we'll do in this notebook.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Bulk Data Import\n",
    "\n",
    "If you choose to use the automatic bulk data import process, all you need to do is run the `BULK_IMPORT_HFILE` system procedure. Splice Machine will automatically generate the split points by sampling the data that is being imported.  Before you execute `BULK_IMPORT_HFILE`, be sure to create the table and indexes first in Splice Machine.\n",
    "\n",
    "If you discover that the automatic bulk data import process is not as performant as expected, you should consider the manual bulk data import process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Bulk Data Import\n",
    "\n",
    "When using the manual bulk data import process, you need to pre-split the data by providing a CSV file that specifies all of the split points. Follow these steps to manually bulk import your data:\n",
    "\n",
    "1. *Create the table in your Splice Machine database*\n",
    "\n",
    "2. *Optionally, create any index(es) on the table*\n",
    "\n",
    "3. *Pre-split the table*\n",
    "\n",
    "4. *Call the `SPLIT_TABLE_OR_INDEX` system procedure for the table*\n",
    "\n",
    "5. *Optionally, pre-split any index(es) on the table*\n",
    "\n",
    "6. *Optionally, call the `SPLIT_TABLE_OR_INDEX` system procedure for the index(es)*\n",
    "\n",
    "7. *Call the `BULK_IMPORT_HFILE` system procedure*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create the table in your Splice Machine database\n",
    "\n",
    "The table you're importing into must exist in your Splice Machine database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Optionally, create any index(es) on the table\n",
    "\n",
    "If you’re going to index the table you’re importing, Splice Machine recommends that you create the index prior to using bulk import. This allows the index to also be pre-split into regions, which will prevent downstream bottlenecks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Pre-split the table\n",
    "\n",
    "When you pre-split a table, you are essentially identifying primary key values that can horizontally split the data into roughly equal parts. For example: let's say you have some order data that you want to load. The data contains a column for order id that has values from 1 to 999,999. We can pre-split this table into 4 roughly equal parts by using the order id column and specifying the values 250000, 500000, and 750000.\n",
    "\n",
    "Create a file that contains the following. Note that we specify three split values to create four splits:\n",
    "\n",
    "```\n",
    "250000\n",
    "500000\n",
    "750000\n",
    "```\n",
    "\n",
    "<div class=\"noteIcon\"><p>Split key values can be multiple columns, in which case each column value would be delimited by a pipe character (|). For example:<p>\n",
    "&nbsp;&nbsp;&nbsp;<code>200000|2019-01-01</code></p></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Call SPLIT_TABLE_OR_INDEX on the table\n",
    "\n",
    "Now call the `SPLIT_TABLE_OR_INDEX` system procedure. The syntax for this command looks like this:\n",
    "\n",
    "```\n",
    "call SYSCS_UTIL.SYSCS_SPLIT_TABLE_OR_INDEX (\n",
    "    schemaName,\n",
    "    tableName,\n",
    "    indexName,\n",
    "    columnList | null,\n",
    "    fileName,\n",
    "    columnDelimiter | null,\n",
    "    characterDelimiter | null,\n",
    "    timestampFormat | null,\n",
    "    dateFormat | null,\n",
    "    timeFormat | null,\n",
    "    maxBadRecords,\n",
    "    badRecordDirectory | null,\n",
    "    oneLineRecords | null,\n",
    "    charset | null,\n",
    ");\n",
    "```\n",
    "Notice that many of the parameters allow you to apply the default value by specifying `null`.\n",
    "\n",
    "<p class=\"noteNote\">You can find full details about these parameters, including the default value for each, in <a href=\"https://doc.splicemachine.com/sqlref_sysprocs_splittable.html\" target=\"_blank\">our <code>SPLIT_TABLE_OR_INDEX</code> documentation.</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Optionally, pre-split any index(es) on the table\n",
    "\n",
    "If you have any indexes for the table you need to repeat step 3 for each index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Optionally, call SPLIT_TABLE_OR_INDEX for the index(es)\n",
    "\n",
    "If you have any indexes for the table you need to repeat step 4 for each index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Call the BULK_IMPORT_HFILE system procedure\n",
    "\n",
    "Lastly, call the `BULK_IMPORT_HFILE` command to bulk load your data. We will go over a complete example later in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BULK_IMPORT_HFILE Command\n",
    "\n",
    "Syntax for the `BULK_IMPORT_HFILE` command looks like this:\n",
    "```\n",
    "call SYSCS_UTIL.BULK_IMPORT_HFILE (\n",
    "    schemaName,\n",
    "    tableName,\n",
    "    insertColumnList | null,\n",
    "    fileOrDirectoryName,\n",
    "    columnDelimiter | null,\n",
    "    characterDelimiter | null,\n",
    "    timestampFormat | null,\n",
    "    dateFormat | null,\n",
    "    timeFormat | null,\n",
    "    maxBadRecords,\n",
    "    badRecordDirectory | null,\n",
    "    oneLineRecords | null,\n",
    "    charset | null,\n",
    "    bulkImportDirectory,\n",
    "    skipSampling\n",
    ");\n",
    "```\n",
    "Notice that many of the parameters allow you to apply the default value by specifying `null`.\n",
    "\n",
    "<p class=\"noteNote\">You can find full details about these parameters, including the default value for each, in <a href=\"https://doc.splicemachine.com/sqlref_sysprocs_importhfile.htmll\" target=\"_blank\">our <code>BULK_IMPORT_HFILE</code> documentation.</a></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-by-Step Example of Automatic Bulk Data Import\n",
    "\n",
    "This example walks you through the automatic bulk data import process one step at a time:\n",
    "\n",
    "1. *Create a Database Schema and Table*\n",
    "2. *Run the Automatic Bulk Import Data Procedure*\n",
    "3. *Review Imported Data*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create a Database Schema and Table\n",
    "\n",
    "You can get started by running the next paragraph in this Notebook, which uses the `%%sql` magic function to:\n",
    "\n",
    "* Create a new schema named `DEV3` in your database.\n",
    "\n",
    "* Create the `customer_bulk_import_example1` table in your database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "\n",
    "CREATE TABLE DEV3.CUSTOMER_BULK_IMPORT_EXAMPLE1 (\n",
    " C_CUSTKEY INTEGER NOT NULL PRIMARY KEY,\n",
    " C_NAME VARCHAR(25),\n",
    " C_ADDRESS VARCHAR(40),\n",
    " C_NATIONKEY INTEGER NOT NULL,\n",
    " C_PHONE VARCHAR(15),\n",
    " C_ACCTBAL DECIMAL(15,2),\n",
    " C_MKTSEGMENT VARCHAR(10),\n",
    " C_COMMENT VARCHAR(117)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Run the Automatic Bulk Import Data Procedure\n",
    "\n",
    "Now we'll bulk import our data, which is the customer table data for TPCH-100. We'll use the automatic method for generating the split keys.\n",
    "\n",
    "The first two rows of the data look like this:\n",
    "\n",
    "<pre>1,1,1,0.2548,GC,BARBARBAR,amotastqqsxqk,50000.0,-10.0,10.0,1,0,xzgtmkfcc,ylkolttoyrgtoypvu,vihxvtlufla,UC,808711111,7664160137823585,2016-08-26 15:09:58.574,OE,kmmmpivhrpyzdrcxiznqsujdvhnvbtvktvzncdigrmfrnbvmuqfdrgyzsacziwobfxcwbqrctbvyyhcefmpsitdjlebpphhtihhnserrolqbjeqpggnkvfowexxgtqoglwqrhhbjdnfrmbeieubdmtkfqicfjtwwbdwbbcacqvukmxhmnpekydlesjzatacwuntakarbcbfgrvxcztgzzfcbkppjfpznjxpnanktiswszzrgxvlccrksbojbzywtziijmdwfwdxyzrwwllzcnjdbkfoxzudqfdowuwuopemcfhxhrskhsrgydkklsgzaujzbqagycvqdnkpmtligujssjierqetnwxzipykpbxbkdunzrvekjpsonlqhtvmntabhlfpzvrzzbwbuzsqdvkbebissivntknptkfpxfcgcwmymncyfqifaqecxp\n",
    "\n",
    "2,1,1,0.2728,GC,BARBARBAR,maqolvlurgl,50000.0,-10.0,10.0,1,0,aommxpkuezwgcfyl,allgnqktgezptiui,rphjjqoevqwdpugh,ZL,115811111,8576454984888259,2016-08-26 15:09:58.575,OE,ozcazgrmfwgpdjcgeestwqmnygzqvvuxsrnnyjbxostljnlplmrdmcdbtxxoadyxeidhbffnevyierxsfgaqjuvpvgpcokbkshiqmyxlnqxwwukdcxswquhivmfkmmapjokweswzvwrpckizbjqjqlmvjjhdnjtdfxwfxfgufdjnfjudkmnbygatwvjbvgahmwwvchbsjeibfoqsfxcsbjkpxyhipsymwolcrokuhxumxkaafrgjzpxyuhkkqbijpgzcmzculivdhwjewwdepktddpswulbzwioeqvhjtorzeyqztitiagbqreoaydsqzixungqygjpiysoexyunbnuzupywuyjxavzvuccooszqbxgioulozbojsumejqoajofbzckjprjjcmmgugntnao</pre>\n",
    "\n",
    "Import this data by running the next paragraph, which calls our `BULK_IMPORT_HFILE` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "\n",
    "call SYSCS_UTIL.BULK_IMPORT_HFILE('DEV3', 'CUSTOMER_BULK_IMPORT_EXAMPLE1', null, 's3a://splice-benchmark-data/flat/TPCH/100/customer', '|', null, null, null, null, 0, '/tmp', true, null, '/tmp/TMP_HFILE', false);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br/>\n",
    "You'll notice that after you run the paragraph, you see a short report that indicates how many rows were successfully loaded and how many failed to load. In this example, all 15 million rows were successfully loaded.\n",
    "\n",
    "You have probably also noticed that we used default values by specifying `null` for all of the parameters that have defaults; here's what those defaults mean:\n",
    "\n",
    "<table class=\"splicezepOddEven\">\n",
    "    <col />\n",
    "    <col />\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>Parameter</th>\n",
    "            <th>NULL Value Details</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td><code>insertColumnList</code></td>\n",
    "            <td>Our column list exactly matches the columns and ordering of columns in the table, so there's not need to specify a list.</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><code>columnDelimiter</code></td>\n",
    "            <td>Our data uses the default comma character (<code>,</code>) to delimit columns.</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><code>stringDelimiter</code></td>\n",
    "            <td>None of our data fields contain the comma character, so we don't need a string delimiter character.</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><code>timestampFormat</code></td>\n",
    "            <td>Our data matches the default timestamp format, which is <code>yyyy-MM-dd HH:mm:ss</code>.</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><code>dateFormat</code></td>\n",
    "            <td>Our data doesn't contain any date columns, so there's no need to specify a format.</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><code>timeFormat</code></td>\n",
    "            <td>Our data doesn't contain any time columns, so there's no need to specify a format.</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Review Imported Data\n",
    "\n",
    "Let's take a look at the data we just imported by running the next paragraph, which selects all the data from the `CUSTOMER_BULK_IMPORT_EXAMPLE1` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "\n",
    "SELECT * FROM DEV3.CUSTOMER_BULK_IMPORT_EXAMPLE1;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-by-Step Example of Manual Bulk Data Import\n",
    "\n",
    "This example will walk you through the manual bulk data import process one step at a time. In these steps you will:\n",
    "\n",
    "1. *Create a Database Schema and Table*\n",
    "2. *Manually Split the Table*\n",
    "2. *Run the Manual Bulk Import Data Procedure*\n",
    "3. *Review Imported Data*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create a Database Schema and Table\n",
    "\n",
    "You can get started by clicking running the next paragraph in this Notebook, which creates the `customer_bulk_import_example2` table in your database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "\n",
    "CREATE TABLE DEV3.CUSTOMER_BULK_IMPORT_EXAMPLE2 (\n",
    " C_CUSTKEY INTEGER NOT NULL PRIMARY KEY,\n",
    " C_NAME VARCHAR(25),\n",
    " C_ADDRESS VARCHAR(40),\n",
    " C_NATIONKEY INTEGER NOT NULL,\n",
    " C_PHONE VARCHAR(15),\n",
    " C_ACCTBAL DECIMAL(15,2),\n",
    " C_MKTSEGMENT VARCHAR(10),\n",
    " C_COMMENT VARCHAR(117)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Manually Split the Table\n",
    "\n",
    "Now we need to create the split keys for our data. The customer data file that we are loading contains a customer key column that has values from 1 to 15,000,000. For this exercise, we want to evenly split the data into 4 hfiles, so the keys look like this:\n",
    "\n",
    "```\n",
    "3750000\n",
    "7500000\n",
    "11250000\n",
    "```\n",
    "\n",
    "For every N lines of split data you specify, you’ll end up with N+1 regions; for example, the above 3 splits will produce these 4 regions:\n",
    "\n",
    "```\n",
    "0 -> 3750000\n",
    "3750001 -> 7500000\n",
    "7500001 -> 11250000\n",
    "11250001 -> (last possible key value)\n",
    "```\n",
    "\n",
    "We have already created this file for you, so all you need to do is run the next paragraph. This calls the `SPLIT_TABLE_OR_INDEX` procedure, passing in the file that contains the split keys as a parameter value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "\n",
    "call SYSCS_UTIL.SYSCS_SPLIT_TABLE_OR_INDEX(\n",
    "    'DEV3',\n",
    "    'CUSTOMER_BULK_IMPORT_EXAMPLE2',\n",
    "    null, \n",
    "    'C_CUSTKEY',\n",
    "    '/opt/data/customer-split-keys.csv',\n",
    "    '|', \n",
    "    null, \n",
    "    null, \n",
    "    null,\n",
    "    null, \n",
    "    -1, \n",
    "    '/tmp', \n",
    "    true, \n",
    "    null\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Run the Manual Bulk Import Data Procedure\n",
    "\n",
    "Now we're ready to bulk import our data. As in the previous example, we are bulk importing the customer table data for TPCH-100. This time we have pre-split the table so we will not need to sample the data before importing. The main difference in this call to `BULK_IMPORT_HFILE` is that the last parameter, `skipSampling`, is now set to `true` to indicate that we've already pre-split the data.\n",
    "\n",
    "Import the data in this file by running the next paragraph, which calls our `BULK_IMPORT_HFILE` procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "\n",
    "call SYSCS_UTIL.BULK_IMPORT_HFILE('DEV3', 'CUSTOMER_BULK_IMPORT_EXAMPLE2', null, 's3a://splice-benchmark-data/flat/TPCH/100/customer', '|', null, null, null, null, 0, '/tmp', true, null, '/tmp/TMP_HFILE', true);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Review Imported Data\n",
    "\n",
    "Lets take a look at the data we just imported by clicking the *Shift + Enter* to run the next paragraph, which selects all of the data in the `CUSTOMER_BULK_IMPORT_EXAMPLE1` table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "\n",
    "SELECT * FROM DEV3.CUSTOMER_BULK_IMPORT_EXAMPLE2;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to Go Next\n",
    "The next notebook in this class, [*Transactions in Splice Machine*](./e.%20Transactions%20in%20Splice%20Machine.ipynb), teaches you how transactions are processed in Splice Machine.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
