{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "os.environ['JDBC_HOST'] = 'jrtest01-splice-hregion'"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Query Optimization\n",
                "\n",
                "This notebook shows you advanced query optimization techniques for boosting the performance of your Splice Machine queries. SQL optimizers convert the SQL statements that you write into semantically equivalent statements with improved performance. If a perfect SQL optimizer existed, you would not need to worry about the efficiency of your SQL statements.\n",
                "\n",
                "In reality, even with a highly evolved optimizer, some SQL statements require some manual tuning or rewriting due to:\n",
                "\n",
                "* Limitations in the optimizer's heuristic rewrite functionality\n",
                "* Limitations in the search space the optimizer explores\n",
                "* Inaccurate statistics and/or cost estimation\n",
                "* Parsing time concerns\n",
                "\n",
                "The optimizer generates and evaluates the execution plan for an SQL query. To better understand optimization and manual tuning, you need to be able to read and understand a query execution plan, and you need to know how to use statistics to understand the characteristics of the tables you're querying. This notebook shows you how to work with plans and statistics, and then addresses specific solutions for some common query performance issues, in the following sections:\n",
                "\n",
                "\n",
                "1. *Understanding the Query Execution Plan*\n",
                "2. *Understanding Database Statistics*\n",
                "3. *Query Performance Problems*\n",
                "\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Understanding the Query Execution Plan\n",
                "\n",
                "This section describes more fully what information is in the Explain plan for a query; the key pieces of information in a plan include the:\n",
                "\n",
                "*  Ordering of the joins and other steps in the query\n",
                "\n",
                "*  Use of Tables vs Indexes\n",
                "\n",
                "*  Need for IndexLookup, which can slow a query down\n",
                "\n",
                "*  Join Strategies employed\n",
                "\n",
                "*  Actual row count and cost estimates at each step\n",
                "\n",
                "*  Presence of predicate pushdowns where available\n",
                "\n",
                "*  Indication of which *engine* will run the query: *control* or *Spark*\n",
                "\n",
                "We'll also delve a bit deeper into pushing down predicates and join ordering/strategies to help you understand plans.\n",
                "\n",
                "### Explain and Predicates\n",
                "\n",
                "Let's start with a query variant that is based on the `customer_bulk_import_example1` and `customer_bulk_import_example2` tables that we created earlier in this class. Click *Shift + Enter* in the next paragraph to display the plan for this query. \n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%sql \n",
                "\n",
                "explain select a.c_custkey, a.c_nationkey from\n",
                "    dev3.customer_bulk_import_example1 a\n",
                "    ,dev3.customer_bulk_import_example2 b\n",
                "     where a.c_custkey = b.c_custkey\n",
                "     and a.c_nationkey = 100\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                " \n",
                "\n",
                "<br/>\n",
                "You\u2019ll notice that on the very right of the plan are two lines with *preds=* on them. *Preds* is short for *predicates*, which in databases are true/false conditions that are tested during query execution.\n",
                "\n",
                "### About Predicates\n",
                "\n",
                "Starting on the bottom line, we see a `TableScan` with the preds specification on it; this is called a *Predicate Pushdown*. A pushdown means: when we perform this `TableScan`, we'll bring this predicate (`A.C_NATIONKEY = 100`) along with us, and will perform the scan using this predicate, passing up to the next part of the plan ONLY the rows that match. Predicate pushdowns are extremely efficient when performed on keyed results (primary keys or indexes), because only the minimal number of rows are pushed up to the next step.\n",
                "\n",
                "The other kind of predicate shown here is of the form `[(A.C_CUSTKEY[4:1] = B.C_CUSTKEY[4:3])]`. You can ignore the numbers for now; the key part is `A.C_CUSTKEY = B.C_CUSTKEY`. You can see that this is the join predicate, required for the actual join operation.\n",
                "\n",
                "The main takeaway is that, as with most databases: when you can *push down* a predicate that filters a lot of data with a keyed filter, it helps create efficient scans for that step. If the filter is not keyed, this becomes a potential opportunity for adding an index."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Join Ordering\n",
                "\n",
                "The actual join ordering is part of the optimization process: do I get a better cost when I start with the `customer_bulk_import_example1` table and join table `customer_bulk_import_example2` with it, or the other way around?\n",
                "\n",
                "Smart join ordering depends a lot on the situation. Generally speaking, the sooner you can filter out rows (thus working with fewer rows at each step of the query), the faster the query will run.\n",
                "\n",
                "Remember that explain plans are ordered from the *bottom up*, which means that the first step in the plan is at the bottom. Another way to view this is to look at the counts on each row of the plan (n=1, n=2, etc.), which specifies the table ordering being used."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Join Strategies\n",
                "\n",
                "These are the available join strategies in Splice Machine:\n",
                "\n",
                "<table class=\"splicezep\">\n",
                "    <thead>\n",
                "        <tr>\n",
                "            <th>Join Strategy</th>\n",
                "            <th>Description</th>\n",
                "        </tr>\n",
                "    </thead>\n",
                "    <tbody>\n",
                "        <tr>\n",
                "            <td><code>BROADCAST</code></td>\n",
                "            <td><p>Read the results of the Right Result Set (RHS) into memory, then for each row in the left result set (LHS), perform a local lookup to determine the right side of the join.<p>\n",
                "                <p><em>BROADCAST</em> will only work if at least one of the following is true:</p>\n",
                "                <ul>\n",
                "                    <li>There is at least one equijoin (=) predicate that does not include a function call.</li>\n",
                "                    <li>There is at least one inequality join predicate, the RHS is a base table, and the join is evaluated in Spark.</li>\n",
                "                </ul>\n",
                "            </td>\n",
                "        </tr>\n",
                "        </tr>\n",
                "</tr>            <td><code>SORTMERGE</code></td>\n",
                "            <td><p>Re-sort both the left and right sides according to the join keys, then perform a <em>MERGE</em> join on the results.</p>\n",
                "                <p><em>SORTMERGE</em> requires an equijoin predicate with no function calls.</p>\n",
                "            </td>\n",
                "        </tr>\n",
                "        <tr>\n",
                "            <td><code>MERGE</code></td>\n",
                "            <td><p>Read the Right and Left result sets simultaneously in order and join them together as they are read.</p>\n",
                "                <p><em>MERGE</em> joins require that both the left and right result sets be sorted according to the join keys. <em>MERGE</em> requires an equijoin predicate that does not include a function call.</p>\n",
                "            </td>\n",
                "        </tr>\n",
                "        <tr>\n",
                "            <td><code>NESTEDLOOP</code></td>\n",
                "            <td><p>For each row on the left, fetch the values on the right that match the join.</p>\n",
                "                <p><em>NESTEDLOOP</em> is the only join that can work with any join predicate of any type; however this type of join is generally very slow.</p>\n",
                "            </td>\n",
                "        </tr>\n",
                "    </tbody>\n",
                "</table>\n",
                "\n",
                "In our example above we see that the plan uses a `MergeJoin` to join the `CUSTOMER_BULK_IMPORT_EXAMPLE1` table with the `CUSTOMER_BULK_IMPORT_EXAMPLE2` table.\n",
                "\n",
                "```\n",
                "->  MergeJoin(n=3,totalCost=77202.976,outputRows=651515625,outputHeapSize=1.324 GB,partitions=145,preds=[(A.C_CUSTKEY[4:1] = B.C_CUSTKEY[4:3])])\n",
                "    ->  TableScan[CUSTOMER_BULK_IMPORT_EXAMPLE2(1664)](n=2,totalCost=236254,scannedRows=118125000,outputRows=118125000,outputHeapSize=1.324 GB,partitions=145)\n",
                "    ->  TableScan[CUSTOMER_BULK_IMPORT_EXAMPLE1(1648)](n=1,totalCost=1447816.5,scannedRows=723906250,outputRows=651515625,outputHeapSize=1.214 GB,partitions=145,preds=[(A.C_NATIONKEY[0:2] = 100)])\n",
                "```\n",
                "\n",
                "Reading this from the bottom up we see:\n",
                "\n",
                "1. `CUSTOMER_BULK_IMPORT_EXAMPLE1` is scanned and becomes the left hand side of the join\n",
                "\n",
                "2. `CUSTOMER_BULK_IMPORT_EXAMPLE2` is scanned and becomes the right hand side of the join\n",
                "\n",
                "3. The `MERGE` join strategy is used"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Understanding Database Statistics\n",
                "\n",
                "Database statistics are a form of metadata (data about data) that assists the Splice Machine query optimizer; the statistics help the optimizer select the most efficient approach to running a query, based on information that has been gathered about the tables involved in the query.\n",
                "\n",
                "In this section we show you how to:\n",
                "\n",
                "* Collect Statistics\n",
                "* View Statistics\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Collecting Statistics\n",
                "\n",
                "You can collect statistics on a schema or table using the `analyze` command. \n",
                "\n",
                "Here is the syntax for collecting statistics for a schema:\n",
                "\n",
                "&nbsp;&nbsp;&nbsp;<code>ANALYZE SCHEMA <em>schemaName</em></code>\n",
                "\n",
                "Here is the syntax for collecting statistics for a table:\n",
                "\n",
                "&nbsp;&nbsp;&nbsp;<code>ANALYZE SCHEMA <em>schemaName.tableName</em></code>\n",
                "\n",
                "Let's try collecting statistics on our `DEV3` schema:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%sql \n",
                "\n",
                "analyze schema DEV3;"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now go back and rerun the explain for our query example. You should notice that the plan has changed.\n",
                "\n",
                "This was the explain plan before we collected stats on the tables in the schema:\n",
                "\n",
                "<pre>\n",
                "Cursor(n=6,rows=651515625,updateMode=READ_ONLY (1),engine=Spark)\n",
                "  ->  ScrollInsensitive(n=5,totalCost=13126628.907,outputRows=651515625,outputHeapSize=1.324 GB,partitions=145)\n",
                "    ->  ProjectRestrict(n=4,totalCost=236254,outputRows=118125000,outputHeapSize=1.324 GB,partitions=145)\n",
                "      ->  MergeJoin(n=3,totalCost=77202.976,outputRows=651515625,outputHeapSize=1.324 GB,partitions=145,preds=[(A.C_CUSTKEY[4:1] = B.C_CUSTKEY[4:3])])\n",
                "        ->  TableScan[CUSTOMER_BULK_IMPORT_EXAMPLE2(1664)](n=2,totalCost=236254,scannedRows=118125000,outputRows=118125000,outputHeapSize=1.324 GB,partitions=145)\n",
                "        ->  TableScan[CUSTOMER_BULK_IMPORT_EXAMPLE1(1648)](n=1,totalCost=1447816.5,scannedRows=723906250,outputRows=651515625,outputHeapSize=1.214 GB,partitions=145,preds=[(A.C_NATIONKEY[0:2] = 100)])\n",
                "</pre>\n",
                "\n",
                "This is the new explain plan after we collected the statistics:\n",
                "\n",
                "<pre>\n",
                "Cursor(n=6,rows=1,updateMode=READ_ONLY (1),engine=Spark)\n",
                "  ->  ScrollInsensitive(n=5,totalCost=268.81,outputRows=1,outputHeapSize=19 B,partitions=145)\n",
                "    ->  ProjectRestrict(n=4,totalCost=4.003,outputRows=1,outputHeapSize=19 B,partitions=145)\n",
                "      ->  NestedLoopJoin(n=3,totalCost=260.8,outputRows=1,outputHeapSize=19 B,partitions=145)\n",
                "        ->  TableScan[CUSTOMER_BULK_IMPORT_EXAMPLE2(1664)](n=2,totalCost=4.003,scannedRows=1,outputRows=1,outputHeapSize=19 B,partitions=145,preds=[(A.C_CUSTKEY[1:1] = B.C_CUSTKEY[2:1])])\n",
                "        ->  TableScan[CUSTOMER_BULK_IMPORT_EXAMPLE1(1648)](n=1,totalCost=37804,scannedRows=15000000,outputRows=1,outputHeapSize=0 B,partitions=145,preds=[(A.C_NATIONKEY[0:2] = 100)])\n",
                "</pre>\n",
                "\n",
                "With statistics collected the cost values are more accurate which allows the optimizer to choose a better plan. The new plan chooses the `NestedLoopJoin` join strategy because it now knows that the right hand side table, `CUSTOMER_BULK_IMPORT_EXAMPLE2`, can have the predicate applied thus filtering the results to just one `scannedRow`.\n",
                "\n",
                "This is a simple example on a small dataset but you can see how database statistics can help the optimizer choose a better plan for executing a query. The point is to ensure the best performance it is critical to collect statistics on your database tables in Splice Machine.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Viewing Statistics\n",
                "\n",
                "Splice Machine provides two system tables that you can query to view the statistics that have been collected for your database:\n",
                "\n",
                "* `SYS.SYSTABLESTATISTICS`\n",
                "* `SYS.SYSCOLUMNSTATISTICS`\n",
                "\n",
                "Let's now view the contents of each of these system tables:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%sql \n",
                "\n",
                "SELECT * FROM SYS.SYSTABLESTATISTICS;\n",
                "SELECT * FROM SYS.SYSCOLUMNSTATISTICS;"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Next, we'll query  `SYS.SYSTABLESTATISTICS` to understand the characteristics of the `DEV3.CUSTOMER_BULK_IMPORT_EXAMPLE1` table:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%sql \n",
                "\n",
                "select total_row_count, total_size, stats_type, sample_fraction from sys.systablestatistics where schemaname='DEV3' and tablename='CUSTOMER_BULK_IMPORT_EXAMPLE1';"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We see that the `DEV3.CUSTOMER_BULK_IMPORT_EXAMPLE1` table has:\n",
                "\n",
                "* 15000000 rows\n",
                "* a total size of 2294789186 bytes\n",
                "* a `statsType` value of 2\n",
                "* `sampleFraction` value of 0.\n",
                "\n",
                "For reference refer to these tables for an explanation of the `statsType` and `sampleFraction`\n",
                "\n",
                "#### Statistics Type Values\n",
                "\n",
                "The following table describes the `statsType` values:\n",
                "\n",
                "<table class=\"splicezep\">\n",
                "    <thead>\n",
                "        <tr>\n",
                "            <th>Statistic Type Value</th>\n",
                "            <th>Description</th>\n",
                "        </tr>\n",
                "    </thead>\n",
                "    <tbody>\n",
                "        <tr>\n",
                "            <td class=\"ItalicFont\">0</td>\n",
                "            <td>Full table (not sampled) statistics that reflect the unmerged partition values.</td>\n",
                "        </tr>\n",
                "            <td class=\"ItalicFont\">1</td>\n",
                "            <td>Sampled statistics that reflect the unmerged partition values.</td>\n",
                "        </tr>\n",
                "        <tr>\n",
                "            <td class=\"ItalicFont\">2</td>\n",
                "            <td>Full table (not sampled) statistics that reflect the table values after all partitions have been merged.</td>\n",
                "        </tr>\n",
                "        <tr>\n",
                "            <td class=\"ItalicFont\">3</td>\n",
                "            <td>Sampled statistics that reflect the table values after all partitions have been merged.</td>\n",
                "        </tr>\n",
                "    </tbody>\n",
                "</table>\n",
                "\n",
                "#### Sample Fraction Values\n",
                "\n",
                "The sampling percentage, `sampleFraction`, is specified as a value in the ranges 0.0 to 1.0:\n",
                "\n",
                "* If `statsType=0` (full statistics), this value is not used, and is shown as `0`.\n",
                "* If `statsType=1`, this value is the percentage or rows to be sampled. A value of `0` means no rows, and a value of `1` means all rows (full statistics).\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Query Performance Issues\n",
                "\n",
                "There can be several reasons why a query doesn't perform at the level that you expect. In this section, we take a look at some of the more common problems that can lead to poor query performance and how you can resolve them. \n",
                "\n",
                "<p class=\"noteIcon\">The most important thing to remember when looking at query performance is to make sure statistics have been collected on the tables you're querying.</p>\n",
                "\n",
                "We'll take a look at these common issues:\n",
                "\n",
                "* Data Skew\n",
                "* Access Path\n",
                "* Nested Loop Joins\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Data Skew\n",
                "\n",
                "Data skew, in the simplest terms, refers to a non-uniform distribution of data in a dataset. For example, let's say you have a column in a table whose range of values is from 1-10. The data in this column would be considered skewed if there is a disproportionally large number of rows for a particular value. So, if the table contains 10 million rows and 9 million of those rows have the value of `5`, then the data would be considered skewed. This is particularly problematic when the column is used in a join condition. \n",
                "\n",
                "When data is skewed, a few tasks have to do significantly more work than other tasks, which reduces parallelism and can lead to out-of-memory errors. Skewness may exist in the base table on certain columns, and it can also occur after certain joins. When data is skewed, problems usually arise during the `MergeSortJoin` step or during grouped aggregates.\n",
                "\n",
                "#### Detecting Skew\n",
                "\n",
                "If your query is executed in Spark, you can use the *Database Console* (Spark UI) to determine if your query may possibly affected by data skew. You can find your query in the Database Console and look at the Summary Metrics for the stage:\n",
                "\n",
                "<img src=\"https://splice-training.s3.amazonaws.com/external/images/skew1.png\" class=\"splice\">\n",
                "\n",
                "Here we see that the `Shuffle Read Size` for the `Min`, `25th Percentile`, `Median`, and `75th Percentile` are relatively the same. However, for the `Max`, the amount of data being read is significantly larger. This indicates that this stage in the query execution is suffering from data skewness.\n",
                "\n",
                "Another way we can detect skew is to look at the individual tasks for a stage in the Database Console:\n",
                "\n",
                "<img src=\"https://splice-training.s3.amazonaws.com/external/images/skew2.png\" class=\"splice\">\n",
                "\n",
                "Here we see that the first task listed has a `Shuffle Read Size / Records` value that is significantly larger than the other tasks. This is also an indication that this query is under-performing due to skew issues in the data.\n",
                "\n",
                "You can also use SQL to determine if there is skew in your data. We've created some skewed data for you in the following example; run the next paragraph to import and analyze skewed data.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%sql \n",
                "\n",
                "CREATE TABLE DEV3.LINEITEM_WITH_SKEW (\n",
                " L_ORDERKEY BIGINT NOT NULL,\n",
                " L_PARTKEY INTEGER NOT NULL,\n",
                " L_SUPPKEY INTEGER NOT NULL, \n",
                " L_LINENUMBER INTEGER NOT NULL, \n",
                " L_QUANTITY DECIMAL(15,2),\n",
                " L_EXTENDEDPRICE DECIMAL(15,2),\n",
                " L_DISCOUNT DECIMAL(15,2),\n",
                " L_TAX DECIMAL(15,2),\n",
                " L_RETURNFLAG VARCHAR(1), \n",
                " L_LINESTATUS VARCHAR(1),\n",
                " L_SHIPDATE DATE,\n",
                " L_COMMITDATE DATE,\n",
                " L_RECEIPTDATE DATE,\n",
                " L_SHIPINSTRUCT VARCHAR(25),\n",
                " L_SHIPMODE VARCHAR(10),\n",
                " L_COMMENT VARCHAR(44),\n",
                " PRIMARY KEY(L_ORDERKEY,L_LINENUMBER)\n",
                " );\n",
                "\n",
                "call SYSCS_UTIL.IMPORT_DATA ('DEV3', 'LINEITEM_WITH_SKEW', null, 's3a://splice-training/external/data/lineitem-with-skew.csv.gz', null, null, null, null, null, 0, '/tmp', true, null);\n",
                "\n",
                "ANALYZE TABLE DEV3.LINEITEM_WITH_SKEW;"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now use SQL to detect the skewness of the data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%sql \n",
                "\n",
                "SELECT COUNT(*) AS NUM_RECORDS, MIN(CC) AS SMALLEST_VALUE, MAX(CC) AS LARGEST_VALUE, AVG(CC) AS AVERAGE_VALUE FROM\n",
                "(SELECT L_ORDERKEY, COUNT(*) AS CC\n",
                " FROM DEV3.LINEITEM_WITH_SKEW\n",
                " GROUP BY 1) DT;\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Handling Skew\n",
                "\n",
                "The query we just ran checked for skewness on the `L_ORDERKEY` column. This query groups the rows by the `L_ORDERKEY` and counts the number of records for each `L_ORDERKEY` value. \n",
                "\n",
                "We can see that there's skew in this data in two ways:\n",
                "\n",
                "* The difference between the `SMALLEST_VALUE` and `LARGEST_VALUE` is very large.\n",
                "* The difference between the `AVERAGE_VALUE` and the `LARGEST_VALUE` is also very large: although there's an average of 10 records per order key value, there is one order key that has 10,485,766 rows.\n",
                "\n",
                "We can't change the data to eliminate skew, but there are a few things that we can try that will help alleviate and in some cases avoid skewness of data, as described in the remainder of this section.\n",
                "\n",
                "##### Using Broadcast Join for Skewed Data\n",
                "\n",
                "In most cases, the shuffling of data during a `mergesort` join is problematic when there is skewness of data in one of the join columns. You can see this in the list of Spark tasks,  where one task is seen reading the majority of the data and taking much longer to complete than all other tasks for the stage. \n",
                "\n",
                "If the right hand side of the join is small enough, you can try hinting the join to use the `BROADCAST` join strategy. For example:\n",
                "\n",
                "```\n",
                "SELECT * FROM DEV3.ORDERS O\n",
                "JOIN DEV3.LINEITEM_WITH_SKEW L --SPLICE-PROPERTIES joinStrategy=BROADCAST\n",
                "ON O.O_ORDERKEY = L.L_ORDERKEY;\n",
                "```\n",
                "\n",
                "Note that we don't actually recommend the query above, because we know the right hand side table `LINEITEM_WITH_SKEW` is a large table. The example is purely for demonstrating how to apply a hint to use the `BROADCAST` join strategy.\n",
                "\n",
                "##### Splitting the Skewed Table and Using Union All\n",
                "\n",
                "Another method for handling skew is to:\n",
                "\n",
                "1. Split the query into two parts, with one part extracting the skewed value, and the second part handling the remaining values.\n",
                "2. Then, use a `UNION ALL` to merge the result sets. \n",
                "\n",
                "Here is an example of a rewrite, in which we know that our skewed data is on the order key value of `1`:\n",
                "\n",
                "```\n",
                "SELECT * FROM DEV3.ORDERS O\n",
                "JOIN DEV3.LINEITEM_WITH_SKEW L\n",
                "ON O.O_ORDERKEY = L.L_ORDERKEY\n",
                "WHERE O.O_ORDERKEY = 1\n",
                "UNION ALL\n",
                "SELECT * FROM DEV3.ORDERS O\n",
                "JOIN DEV3.LINEITEM_WITH_SKEW L\n",
                "ON O.O_ORDERKEY = L.L_ORDERKEY\n",
                "WHERE O.O_ORDERKEY <> 1\n",
                "```\n",
                "\n",
                "##### Introducing a Non-Skewed Join Column\n",
                "\n",
                "Another option is to introduce a non-skewed join column to the query. This is typically accomplished by rewriting the query to use the `WITH` statement. For example:\n",
                "\n",
                "```\n",
                "WITH DT as (SELECT * FROM DEV3.ORDERS O)\n",
                "SELECT * FROM DT\n",
                "WHERE EXISTS (SELECT 1 FROM DEV3.LINEITEM_WITH_SKEW L WHERE L.L_ORDERKEY = DT.O_ORDERKEY)\n",
                "```\n",
                "\n",
                "##### Other Methods and Future Improvements for Skewed Data\n",
                "\n",
                "If you are joining multiple tables, you may be able to alleviate skew issues by delaying the skewed join. This can be accomplished by using the `joinOrder=FIXED` method, and by experimenting with the order of tables in which they are joined.\n",
                "\n",
                "Splice Machine is constantly instituting improvements to the optimizer to help with skewness and reduce the need for rewrites or query hints. Some improvements that are being worked on include salting skewed values to make them unique and pushiing aggregation down before the join.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Access Path\n",
                "\n",
                "The access path to the data, which is how we read the data, can have a huge effect on the performance of a query: Are we scanning the entire table? Are we using a primary key?\n",
                "\n",
                "A full table scan appears as a `TableScan` operation in the explain plan. Primary key access also displays as a `TableScan`, but the number of rows scanned will be smaller than the total number of rows in the table\n",
                "\n",
                "Run the next paragraph to see the explain plan for selecting from a table using a full table scan."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%sql \n",
                "\n",
                "explain select * from DEV3.LINEITEM_WITH_SKEW;\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<br/>\n",
                "You can see that the `TableScan` operation is performed on the `LINEITEM_WITH_SKEW` table. Note that the number of `scannedRows` is 16486975. This is the total number of rows in the table.\n",
                "\n",
                "Run the next paragraph to see the explain plan for selecting from a table using a primary key access path."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%sql \n",
                "\n",
                "explain select * from DEV3.LINEITEM_WITH_SKEW WHERE L_ORDERKEY = 10;"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "You can see that the `TableScan` operation is performed on the `LINEITEM_WITH_SKEW` table, but notice that the number of `scannedRows` is 3. Reading through 3 rows is a whole lot faster than reading through 16486975 rows.\n",
                "\n",
                "Indexes are another access path that can help improve the peformance of a query. In Splice Machine we refer to indexes as either a covering index or a non-covering index. \n",
                "\n",
                "#### Using a Covering Index\n",
                "\n",
                "If all columns referenced in a query belonging to a particular table are covered by an index defined on that table, that index is called a _covering index_ for the query. When the number of rows accessed is the same, scanning a covering index is usually more favorable than scanning the base table, since the index usually will have a smaller row size.\n",
                "\n",
                "Run the next paragraph to create an index and view the index access path in the explain plan."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%sql \n",
                "\n",
                "CREATE INDEX DEV3.IDX_LINEITEM1 ON DEV3.LINEITEM_WITH_SKEW(L_PARTKEY, L_QUANTITY);\n",
                "\n",
                "EXPLAIN SELECT L_PARTKEY, L_QUANTITY FROM DEV3.LINEITEM_WITH_SKEW; "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "You can see that the `IndexScan` operation is performed using the `IDX_LINEITEM1` index.\n",
                "\n",
                "#### Non-Covering Index\n",
                "\n",
                "If not all columns referenced in a query belonging to a particular table are covered by an index defined on that table, that index is called a _non-covering index_. The use of a non-covering index incurs the extra cost to look up the values of column(s) not covered by the index from the base table for each qualified row. This may or may not be a better choice than a full table scan, depending on the data and the query.\n",
                "\n",
                "Run the next paragraph to view the explain plan for a query that uses a non-covering index."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%sql \n",
                "\n",
                "EXPLAIN SELECT L_PARTKEY, L_QUANTITY, L_EXTENDEDPRICE FROM DEV3.LINEITEM_WITH_SKEW --splice-properties index=IDX_LINEITEM1"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<br/>\n",
                "You can see that there is an additional step `IndexLookup` that needs to be performed for every row returned by the `IndexScan` step. As previously stated, this may or may not be as perfomant when compared to doing a full table scan. It really depends on the amount of data and the particular query.\n",
                "\n",
                "### Nested Loop Joins\n",
                "\n",
                "Nested loop joins work for all kinds of join conditions (equality or non-equality). When an equality join condition is present, the performance of a nested loop join is usually not as good as the other 3 join strategies (broadcast, sortmerge and merge join). The exception is when the table on the left side has a small number of rows to read, and the join with the  table on the right side uses a leading pk/index column with low selectivity.\n",
                "\n",
                "Run the next paragraph to create some tables and load some data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%sql \n",
                "\n",
                "CREATE TABLE DEV3.LINEITEM (\n",
                " L_ORDERKEY BIGINT NOT NULL,\n",
                " L_PARTKEY INTEGER NOT NULL,\n",
                " L_SUPPKEY INTEGER NOT NULL, \n",
                " L_LINENUMBER INTEGER NOT NULL, \n",
                " L_QUANTITY DECIMAL(15,2),\n",
                " L_EXTENDEDPRICE DECIMAL(15,2),\n",
                " L_DISCOUNT DECIMAL(15,2),\n",
                " L_TAX DECIMAL(15,2),\n",
                " L_RETURNFLAG VARCHAR(1), \n",
                " L_LINESTATUS VARCHAR(1),\n",
                " L_SHIPDATE DATE,\n",
                " L_COMMITDATE DATE,\n",
                " L_RECEIPTDATE DATE,\n",
                " L_SHIPINSTRUCT VARCHAR(25),\n",
                " L_SHIPMODE VARCHAR(10),\n",
                " L_COMMENT VARCHAR(44),\n",
                " PRIMARY KEY(L_ORDERKEY,L_LINENUMBER)\n",
                ");\n",
                "\n",
                "CREATE TABLE DEV3.SUPPLIER (\n",
                " S_SUPPKEY INTEGER NOT NULL PRIMARY KEY,\n",
                " S_NAME VARCHAR(25) ,\n",
                " S_ADDRESS VARCHAR(40) ,\n",
                " S_NATIONKEY INTEGER ,\n",
                " S_PHONE VARCHAR(15) ,\n",
                " S_ACCTBAL DECIMAL(15,2),\n",
                " S_COMMENT VARCHAR(101)\n",
                "); \n",
                "\n",
                "call SYSCS_UTIL.IMPORT_DATA ('DEV3', 'LINEITEM', null, 's3a://splice-benchmark-data/flat/TPCH/1/lineitem', '|', null, null, null, null, 0, '/tmp', true, null);\n",
                "\n",
                "call SYSCS_UTIL.IMPORT_DATA ('DEV3', 'SUPPLIER', null, 's3a://splice-benchmark-data/flat/TPCH/1/supplier', '|', null, null, null, null, 0, '/tmp', true, null);\n",
                "\n",
                "ANALYZE TABLE DEV3.LINEITEM;\n",
                "\n",
                "ANALYZE TABLE DEV3.SUPPLIER;"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<br/>\n",
                "Now run the next paragraph to see an example of a perfect use case for a nested loop join."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%sql \n",
                "\n",
                "EXPLAIN select count(*) from \n",
                "dev3.lineitem, dev3.supplier\n",
                "where l_suppkey= s_suppkey and l_partkey = 1 and  L_orderkey = 5120486;"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<br/>\n",
                "You can see that both tables have a very small number of `scannedRows`; this is a perfect case for a nested loop join.\n",
                "\n",
                "If your query uses a nested loop join on tables with many rows on both sides of the join, the recommended solution is to apply a hint to use either a `BROADCAST` or `SORTMERGE` join strategy."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Where to Go Next\n",
                "The next notebook in this class, [*Prepared Statements*](./g.%20Prepared%20Statements.ipynb), teaches you how to use prepared statements for querying your databases.\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.3"
        },
        "toc": {
            "base_numbering": 1,
            "nav_menu": {},
            "number_sections": false,
            "sideBar": false,
            "skip_h1_title": false,
            "title_cell": "Table of Contents",
            "title_sidebar": "Contents",
            "toc_cell": false,
            "toc_position": {},
            "toc_section_display": false,
            "toc_window_display": false
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}