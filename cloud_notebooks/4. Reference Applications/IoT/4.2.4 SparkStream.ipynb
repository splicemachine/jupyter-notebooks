{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup-- \n",
    "import os\n",
    "import pyspark\n",
    "from splicemachine.spark.context import PySpliceContext\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# make sure pyspark tells workers to use python3 not 2 if both are installed\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3'\n",
    "jdbc_host = os.environ['JDBC_HOST']\n",
    "\n",
    "conf = pyspark.SparkConf()\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "splicejdbc=f\"jdbc:splice://{jdbc_host}:1527/splicedb;user=splice;password=admin\"\n",
    "\n",
    "splice = PySpliceContext(spark, splicejdbc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "%defaultDatasource jdbc:splice://jrtest01-splice-hregion:1527/splicedb;user=splice;password=admin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"https://doc.splicemachine.com/zeppelin/css/zepstyles.css\" />\n",
    "\n",
    "# Spark Stream\n",
    "Spark Streaming is used to read data from Kafka Queue, which is then ingested into Splice using Splice Machine Adpater.\n",
    "\n",
    "The steps involved are:\n",
    "\n",
    "* Create Spark Direct Streaming to consume the data from Kafka Queue  \n",
    "* Initialize the Splice Machine Adapter\n",
    "* As data is streamed in, blocks of the data are parsed and ingested into Splice Machine using the Adapter\n",
    "\n",
    "The streaming of data can be viewed in Spark UI \n",
    "In the next notebook, we can query against the streamed data that is ingested in real time.\n",
    "\n",
    "Before running this notebook, ensure that brokerlist and jdbc url are set appropriatly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%scala \n",
    "import _root_.kafka.serializer.DefaultDecoder\n",
    "import _root_.kafka.serializer.StringDecoder\n",
    "import org.apache.spark.storage.StorageLevel\n",
    "import org.apache.spark.sql.Row\n",
    "import scala.collection.mutable.ListBuffer\n",
    "\n",
    "import org.apache.kafka.clients.consumer.ConsumerRecord\n",
    "import org.apache.kafka.common.serialization.StringDeserializer\n",
    "\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.streaming.Seconds\n",
    "import org.apache.spark.sql.SQLContext\n",
    "\n",
    "import org.apache.spark.streaming.StreamingContext\n",
    "import org.apache.spark.streaming.kafka010._\n",
    "import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent\n",
    "import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe\n",
    "import com.splicemachine.spark.splicemachine._\n",
    "import com.splicemachine.derby.utils._\n",
    "\n",
    "\n",
    "//Set properties\n",
    "\n",
    "val topics = Array(\"iotdemo\")\n",
    "\n",
    "val brokerList=\"kafka-0-node.{FRAMEWORKNAME}.mesos:9092\"\n",
    "val consumerGroup=\"kstest\"\n",
    "val batchInterval=5\n",
    "\n",
    "val JDBC_URL=\"jdbc:splice://{FRAMEWORKNAME}-proxy.marathon.mesos:1527/splicedb;user=splice;password=admin;useSpark=true\"\n",
    "val SPLICE_TABLE_ITEM=\"IOTDEMO.ITEMFLOW\"\n",
    "\n",
    "\n",
    "//Create Streaming Context\n",
    "val ssc = new StreamingContext(sc, Seconds(5)) \n",
    "\n",
    "//Set Kafka Queue parameters\n",
    "val kafkaParams = Map[String, Object](\n",
    "    \"bootstrap.servers\" -> brokerList,\n",
    "    \"group.id\"-> consumerGroup,\n",
    "    \"auto.offset.reset\" -> \"earliest\",\n",
    "    \"enable.auto.commit\" -> (false: java.lang.Boolean),\n",
    "    \"key.deserializer\" -> classOf[StringDeserializer],\n",
    "    \"value.deserializer\" -> classOf[StringDeserializer]\n",
    "    )\n",
    "    \n",
    "    \n",
    "//Create Direct Stream\n",
    "val stream = KafkaUtils.createDirectStream[String, String](\n",
    "  ssc,\n",
    "  PreferConsistent,\n",
    "  Subscribe[String, String](topics, kafkaParams)\n",
    ")\n",
    "\n",
    "\n",
    "//Parse the queue messages\n",
    "val toPair = stream.map(record => (record.key, record.value))\n",
    "val msgs = toPair.map(_._2)\n",
    "\n",
    "val splicemachineContext = new SplicemachineContext(JDBC_URL)\n",
    "val schema_item = splicemachineContext.getSchema(SPLICE_TABLE_ITEM)\n",
    " \n",
    "\n",
    " msgs.foreachRDD { rdd =>\n",
    "   \n",
    "   //Create dataframes\n",
    "    val lines =  rdd.map(line => line.split(\",\"))\n",
    "   \n",
    "    val rowRdd_item = lines.map { p => \n",
    "        Row (\n",
    "             p(0).toLong, \n",
    "             if(p(1).length==0) null else SpliceDateFunctions.TO_TIMESTAMP( p(1), \"yyyy-MM-dd HH:mm:ss\"), \n",
    "            if(p(2).length==0) null else SpliceDateFunctions.TO_TIMESTAMP( p(2), \"yyyy-MM-dd HH:mm:ss\"), \n",
    "            p(3), \n",
    "            if(p(4).length==0) null else  p(4).toLong,\n",
    "            if(p(5).length==0) null else BigDecimal(p(5)), \n",
    "            if(p(6).length==0) null else SpliceDateFunctions.TO_TIMESTAMP( p(6), \"yyyy-MM-dd HH:mm:ss\"),\n",
    "            if(p(7).length==0) null else  p(7).toLong, \n",
    "            if(p(8).length==0) null else SpliceDateFunctions.TO_TIMESTAMP( p(8), \"yyyy-MM-dd HH:mm:ss\"),\n",
    "            if(p(9).length==0) null else p(9).toLong, \n",
    "            if(p(10).length==0) null else SpliceDateFunctions.TO_TIMESTAMP( p(10), \"yyyy-MM-dd HH:mm:ss\"),\n",
    "            if(p(11).length==0) null else BigDecimal(p(11)),  \n",
    "            if(p(12).length==0) null else BigDecimal(p(12)), \n",
    "            if(p(13).length==0) null else BigDecimal(p(13)),\n",
    "            if(p(14).length==0) null else SpliceDateFunctions.TO_TIMESTAMP( p(14), \"yyyy-MM-dd HH:mm:ss\"),\n",
    "            if(p.length >15) if(p(15).length==0) null else SpliceDateFunctions.TO_TIMESTAMP( p(15), \"yyyy-MM-dd HH:mm:ss\") else null,\n",
    "             if(p.length >16) if(p(16).length==0) null else  SpliceDateFunctions.TO_TIMESTAMP( p(16), \"yyyy-MM-dd HH:mm:ss\") else null,\n",
    "              if(p.length >17) if(p(17).length==0) null else SpliceDateFunctions.TO_TIMESTAMP( p(17), \"yyyy-MM-dd HH:mm:ss\") else null\n",
    "         )\n",
    "    }\n",
    "    \n",
    "\n",
    "    val df_item = sqlContext.createDataFrame(rowRdd_item, schema_item)\n",
    "  \n",
    "    //If there are records, use Splice Adapter to insert the data to table\n",
    "    if(df_item.count > 0)\n",
    "        splicemachineContext.insert(df_item,  SPLICE_TABLE_ITEM)\n",
    "  \n",
    "    }\n",
    "    \n",
    "  \n",
    "//Stop gracefully when driver is stopped\n",
    " sys.ShutdownHookThread {\n",
    "      ssc.stop(true, true)\n",
    "  }\n",
    "  \n",
    "\n",
    "ssc.start()\n",
    "//ssc.awaitTermination()\n",
    "ssc.awaitTerminationOrTimeout(5000000)\n",
    "ssc.stop(stopSparkContext = false, stopGracefully = true)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
