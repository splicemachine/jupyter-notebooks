{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link rel=\"stylesheet\" href=\"https://doc.splicemachine.com/jupyter/css/custom.css\">\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<link rel=\"stylesheet\" href=\"https://doc.splicemachine.com/jupyter/css/custom.css\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%scala\n",
    "%%spark <> --noUI\n",
    "import java.net.InetAddress\n",
    "val driver_host = InetAddress.getLocalHost.getHostAddress\n",
    "SparkSession.builder()\n",
    "\t.appName(\"jt1test2\")\n",
    "\t.master(\"k8s://https://kubernetes.default.svc.cluster.local:443\")\n",
    "\t.config(\"spark.kubernetes.container.image\", \"splicemachine/sm_k8_spark:0.0.4\")\n",
    "\t.config(\"spark.executor.instances\", \"2\")\n",
    "\t.config(\"spark.submit.deployMode\", \"cluster\")\n",
    "\t.config(\"spark.submit.deployMode\", \"cluster\")\n",
    "\t.config(\"spark.driver.extraClassPath\", \"/opt/spark/conf:/opt/spark/jars/*\")\n",
    "\t.config(\"spark.executor.extraClassPath\", \"./:/opt/hbase/conf:/opt/splicemachine/lib/*:/opt/spark/jars/*:/opt/hbase/lib/*\")\n",
    "\t.config(\"splice.spark.executor.extraLibraryPath\", \"/opt/native\")\n",
    "\t.config(\"spark.files\", \"/opt/spark/conf/hbase-site.xml,/opt/spark/conf/core-site.xml,/opt/spark/conf/hdfs-site.xml,/opt/spark/jars/hbase_sql-2.8.0.1926-cdh5.14.0.jar\")\n",
    "\t.config(\"spark.kubernetes.authenticate.caCertFile\", \"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\")\n",
    "\t.config(\"spark.kubernetes.authenticate.oauthTokenFile\", \"/var/run/secrets/kubernetes.io/serviceaccount/token\")\n",
    "\t.config(\"spark.driver.host\", driver_host)\n",
    "\t.config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"https://doc.splicemachine.com/zeppelin/css/zepstyles.css\" />\n",
    "\n",
    "# Kafka Queue\n",
    "\n",
    "In this notebook, we create a Kafka topic and start the producer. The producer is reads data from a csv file in which each line corresponds to a message. \n",
    "\n",
    "Each message contains comma separated values of data that map to various tables that we created in our *Database Setup* notebook:\n",
    "\n",
    "<table class=\"splicezepOddEven\">\n",
    "    <col />\n",
    "    <col />\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>Message Type</th>\n",
    "            <th>Description</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>Items</td>\n",
    "            <td>Have <code>ID, Serial Number, CreatedTime,</code> and <code>UPCcode</code> fields.</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td>ItemFlow Events</td>\n",
    "            <td><p>These occur at high frequency: 1000's per second. They are ingested into the database.</p>\n",
    "                <p>An event occurs when an <code>Item</code> moves from a <code>Warehouse</code>, arrives at a <code>Store</code>, and is seen at a door, in a dressing room, or at a point-of-sale terminal.</p>\n",
    "                <p class=\"noteNote\">There are 600,000 ItemFlow events in this demo</p>\n",
    "            </td>\n",
    "        </tr>\n",
    "   </tbody>\n",
    "</table>\n",
    "\n",
    "There are multiple warehouses and stores, with location coordinates all in a geographic region east of London."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Kafka Variables\n",
    "\n",
    "Before proceeding, make sure that the Kafka server is already running.\n",
    "\n",
    "We'll first assign values to variables that we'll use when creating our Kafka topic:\n",
    "\n",
    "* topic name\n",
    "* ZooKeeper URL\n",
    "* Broker URL\n",
    "* Data file name\n",
    "\n",
    "<p class=\"noteIcon\">You need to replace the Zookeeper and Broker URL values in the next cell.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%scala \n",
    "z.put(\"topicname\", \"iotdemo\")\n",
    "z.put(\"zookeeper\", \"zookeeper-0-node.{FRAMEWORKNAME}.mesos:2182\")\n",
    "z.put(\"brokers\", \"kafka-0-node.{FRAMEWORKNAME}.mesos:9092\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autoscroll": "auto"
   },
   "source": [
    "## Create the Kafka Topic\n",
    "\n",
    "To create the Kafka topic, we:\n",
    "\n",
    "1. Specify parameters for the queue, including session timeout, connection timeout, number of partitions, and replication factor.\n",
    "2. Create the ZooKeeper client.\n",
    "3. Invoke `AdminUtils` to create the topic.\n",
    "\n",
    "If you've previously run this code and the topic already exists, you'll see an error message; otherwise, the topic has been successfully created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%scala \n",
    "import java.util.Properties\n",
    "import kafka.admin.AdminUtils\n",
    "import kafka.utils.ZkUtils\n",
    "\n",
    "//Properties for zookeeper client\n",
    "val sessionTimeoutMs = 10000\n",
    "val connectionTimeoutMs = 10000\n",
    "\n",
    "//Properties for Kafak Queue\n",
    "val topicName=z.get(\"topicname\").toString\n",
    "val numPartitions = 10\n",
    "val replicationFactor = 1\n",
    "\n",
    "// Create a ZooKeeper client\n",
    "val zkUtils = ZkUtils.apply(z.get(\"zookeeper\").toString, sessionTimeoutMs, connectionTimeoutMs,\n",
    "    false)\n",
    "    \n",
    "\n",
    "// Create  topic\n",
    "val topicConfig = new Properties\n",
    "AdminUtils.createTopic(zkUtils, topicName, numPartitions, replicationFactor, topicConfig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Kafka Producer and Add Data to Queue\n",
    "\n",
    "Make sure that the `filename` value in the next cell is set to the name of the data file containing the messages you want *produced*; for example,the cell is currently set to read and enqueue the `ItemFlow` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%scala \n",
    "import org.apache.commons.io.IOUtils\n",
    "import java.net.URL\n",
    "import java.util.Properties\n",
    "import java.nio.charset.Charset\n",
    "import scala.io.Source\n",
    "import java.io.{FileReader, FileNotFoundException, IOException}\n",
    "import org.apache.kafka.clients.producer.{KafkaProducer, ProducerConfig, ProducerRecord}\n",
    "import org.apache.kafka.common.serialization.StringSerializer\n",
    "\n",
    "\n",
    "//Properties\n",
    "val brokers = z.get(\"brokers\")\n",
    "val topic = z.get(\"topicname\").toString\n",
    "\n",
    "val messagesPerSec=1000\n",
    "val pauseBetweenMessages = 500\n",
    "\n",
    "//val filename = \"https://s3.amazonaws.com/splice-demo/iot/itemflow_small.csv\"\n",
    "val filename = \"https://s3.amazonaws.com/splice-demo/iot/itemflow_200k.csv\"\n",
    "//val filename = \"https://s3.amazonaws.com/splice-demo/iot/itemflow_600k.csv\"\n",
    "     \n",
    "//Add properties\n",
    "val props =new Properties\n",
    "props.put(\"bootstrap.servers\", brokers)\n",
    "props.put(\"acks\", \"all\")\n",
    "props.put(\"retries\",new Integer( 0))\n",
    "props.put(\"batch.size\",new Integer( 16384))\n",
    "props.put(\"linger.ms\",new Integer( 1))\n",
    "props.put(\"buffer.memory\", new Integer(33554432))\n",
    "props.put(\"key.serializer\",\n",
    "        \"org.apache.kafka.common.serialization.StringSerializer\")\n",
    "props.put(\"value.serializer\",\n",
    "        \"org.apache.kafka.common.serialization.StringSerializer\")\n",
    "    \n",
    "//Create Kafka producer\n",
    "val producer = new KafkaProducer[String, String](props)\n",
    "    \n",
    "    \n",
    "//Read data file\n",
    "val s3fileData = sc.parallelize(\n",
    "    IOUtils.toString(new URL( filename), Charset.forName(\"utf8\")).split(\"\\n\"))\n",
    "    \n",
    "\n",
    "//Put each line from file onto Queue in batchs specified by properties\n",
    "var i = 0\n",
    "s3fileData.collect().foreach(line =>  {\n",
    "        val message =  new ProducerRecord[String, String](topic, null, line)\n",
    "        producer.send(message)\n",
    "        i= i+1;\n",
    "        if (i >= messagesPerSec) {\n",
    "            i = 0;\n",
    "            Thread.sleep(pauseBetweenMessages)\n",
    "         }\n",
    "   }\n",
    ")\n",
    "    \n",
    "println (\"DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to Go Next\n",
    "\n",
    "Now we're ready to [Stream the data into Splice Machine using Spark streaming](../IoT/4.2.4%20SparkStream.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
