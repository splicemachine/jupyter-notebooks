{
 "cells": [
  {
    "cell_type": "code",
    "execution_count": null,
    "metadata": {},
    "outputs": [],
    "source": [
        "%%scala\n",
        "%%spark <> --noUI\n",
        "import java.net.InetAddress\n",
        "val driver_host = InetAddress.getLocalHost.getHostAddress\n",
        "SparkSession.builder()\n",
        "\t.appName(\"jt1test2\")\n",
        "\t.master(\"k8s://https://kubernetes.default.svc.cluster.local:443\")\n",
        "\t.config(\"spark.kubernetes.container.image\", \"splicemachine/sm_k8_spark:0.0.4\")\n",
        "\t.config(\"spark.executor.instances\", \"2\")\n",
        "\t.config(\"spark.submit.deployMode\", \"cluster\")\n",
        "\t.config(\"spark.submit.deployMode\", \"cluster\")\n",
        "\t.config(\"spark.driver.extraClassPath\", \"/opt/spark/conf:/opt/spark/jars/*\")\n",
        "\t.config(\"spark.executor.extraClassPath\", \"./:/opt/hbase/conf:/opt/splicemachine/lib/*:/opt/spark/jars/*:/opt/hbase/lib/*\")\n",
        "\t.config(\"splice.spark.executor.extraLibraryPath\", \"/opt/native\")\n",
        "\t.config(\"spark.files\", \"/opt/spark/conf/hbase-site.xml,/opt/spark/conf/core-site.xml,/opt/spark/conf/hdfs-site.xml,/opt/spark/jars/hbase_sql-2.8.0.1926-cdh5.14.0.jar\")\n",
        "\t.config(\"spark.kubernetes.authenticate.caCertFile\", \"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\")\n",
        "\t.config(\"spark.kubernetes.authenticate.oauthTokenFile\", \"/var/run/secrets/kubernetes.io/serviceaccount/token\")\n",
        "\t.config(\"spark.driver.host\", driver_host)\n",
        "\t.config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark\")\n"
    ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"https://doc.splicemachine.com/zeppelin/css/zepstyles.css\" />\n",
    "\n",
    "# Kafka Queue\n",
    "\n",
    "In this notebook, kafka topic is created and the data producer is started.  The producer is currently reading data from a csv file, with each line corresponding to a message.  The message contains comma separated values of data that map to various tables in the splice database. \n",
    "\n",
    "* There are \"Items\" that have a ID, Serial Number, CreatedTime and UPCcode.  \n",
    "* There are \"ItemFlow\" events that occur at high frequency (1,000's per second, 600,000 in total in this demo) and are ingested into Splice\n",
    "* An event occurs when an Item moves from Warehouse, arrives at a store, is seen at the POS terminal, in a Dressingroom or at a Door\n",
    "* There are multiple warehouses and stores, with location coordinates all in a geographic region east of London.\n",
    "\n",
    "After verifying/setting the values for the following, the notebook (all paragraphs) can be run by just selecting the Run from top tool bar or each paragraph can be run individually in the order they appear.\n",
    "\n",
    "* Topic Name\n",
    "* Zookeeper URL\n",
    "* Broker URL\n",
    "* File Name of the data file  \n",
    "\n",
    "<p class=\"noteIcon\">\n",
    "Note: This assumes kafka server is already running. The Zookeeper and Broker need to set appropriately in the next paragraph 'Set Parameters'.\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%scala \n",
    "z.put(\"topicname\", \"iotdemo\")\n",
    "z.put(\"zookeeper\", \"zookeeper-0-node.{FRAMEWORKNAME}.mesos:2182\")\n",
    "z.put(\"brokers\", \"kafka-0-node.{FRAMEWORKNAME}.mesos:9092\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autoscroll": "auto"
   },
   "source": [
    "Next we will create kafka topic\n",
    "The steps to create kafka topic are\n",
    "<ui>\n",
    "<li> Specify the Queue parameters : like session timeout, connectiontimeout, number of partitions and replication factor.\n",
    "<li> Create Zookeper client\n",
    "<li> Invoke AdminUtils to create topic\n",
    "</ui>\n",
    "\n",
    "When this paragraph is run, the topic is created or error is displayed if the topic already exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%scala \n",
    "import java.util.Properties\n",
    "import kafka.admin.AdminUtils\n",
    "import kafka.utils.ZkUtils\n",
    "\n",
    "//Properties for zookeeper client\n",
    "val sessionTimeoutMs = 10000\n",
    "val connectionTimeoutMs = 10000\n",
    "\n",
    "//Properties for Kafak Queue\n",
    "val topicName=z.get(\"topicname\").toString\n",
    "val numPartitions = 10\n",
    "val replicationFactor = 1\n",
    "\n",
    "// Create a ZooKeeper client\n",
    "val zkUtils = ZkUtils.apply(z.get(\"zookeeper\").toString, sessionTimeoutMs, connectionTimeoutMs,\n",
    "    false)\n",
    "    \n",
    "\n",
    "// Create  topic\n",
    "val topicConfig = new Properties\n",
    "AdminUtils.createTopic(zkUtils, topicName, numPartitions, replicationFactor, topicConfig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running this, ensure appropriate file is set to filename variable in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%scala \n",
    "import org.apache.commons.io.IOUtils\n",
    "import java.net.URL\n",
    "import java.util.Properties\n",
    "import java.nio.charset.Charset\n",
    "import scala.io.Source\n",
    "import java.io.{FileReader, FileNotFoundException, IOException}\n",
    "import org.apache.kafka.clients.producer.{KafkaProducer, ProducerConfig, ProducerRecord}\n",
    "import org.apache.kafka.common.serialization.StringSerializer\n",
    "\n",
    "\n",
    "//Properties\n",
    "val brokers = z.get(\"brokers\")\n",
    "val topic = z.get(\"topicname\").toString\n",
    "\n",
    "val messagesPerSec=1000\n",
    "val pauseBetweenMessages = 500\n",
    "\n",
    "//val filename = \"https://s3.amazonaws.com/splice-demo/iot/itemflow_small.csv\"\n",
    "val filename = \"https://s3.amazonaws.com/splice-demo/iot/itemflow_200k.csv\"\n",
    "//val filename = \"https://s3.amazonaws.com/splice-demo/iot/itemflow_600k.csv\"\n",
    "     \n",
    "//Add properties\n",
    "val props =new Properties\n",
    "props.put(\"bootstrap.servers\", brokers)\n",
    "props.put(\"acks\", \"all\")\n",
    "props.put(\"retries\",new Integer( 0))\n",
    "props.put(\"batch.size\",new Integer( 16384))\n",
    "props.put(\"linger.ms\",new Integer( 1))\n",
    "props.put(\"buffer.memory\", new Integer(33554432))\n",
    "props.put(\"key.serializer\",\n",
    "        \"org.apache.kafka.common.serialization.StringSerializer\")\n",
    "props.put(\"value.serializer\",\n",
    "        \"org.apache.kafka.common.serialization.StringSerializer\")\n",
    "    \n",
    "//Create Kfka producer\n",
    "val producer = new KafkaProducer[String, String](props)\n",
    "    \n",
    "    \n",
    "//Read data file\n",
    "val s3fileData = sc.parallelize(\n",
    "    IOUtils.toString(new URL( filename), Charset.forName(\"utf8\")).split(\"\\n\"))\n",
    "    \n",
    "\n",
    "//Put each line from file onto Queue in batchs specified by properties\n",
    "var i = 0\n",
    "s3fileData.collect().foreach(line =>  {\n",
    "        val message =  new ProducerRecord[String, String](topic, null, line)\n",
    "        producer.send(message)\n",
    "        i= i+1;\n",
    "        if (i >= messagesPerSec) {\n",
    "            i = 0;\n",
    "            Thread.sleep(pauseBetweenMessages)\n",
    "         }\n",
    "   }\n",
    ")\n",
    "    \n",
    "println (\"DONE\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
