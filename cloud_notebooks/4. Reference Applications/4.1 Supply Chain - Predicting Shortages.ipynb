{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup-- \n",
    "import os\n",
    "import pyspark\n",
    "from splicemachine.spark.context import PySpliceContext\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# make sure pyspark tells workers to use python3 not 2 if both are installed\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3'\n",
    "jdbc_host = os.environ['JDBC_HOST']\n",
    "\n",
    "conf = pyspark.SparkConf()\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "splicejdbc=f\"jdbc:splice://{jdbc_host}:1527/splicedb;user=splice;password=admin\"\n",
    "\n",
    "splice = PySpliceContext(spark, splicejdbc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "%defaultDatasource jdbc:splice://jrtest01-splice-hregion:1527/splicedb;user=splice;password=admin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<link rel=\"stylesheet\" href=\"https://doc.splicemachine.com/zeppelin/css/zepstyles.css\" />\n",
    "\n",
    "# Crystal Ball\n",
    "The Crystal Ball application is a supply chain reference application relevant to manufacturers, distributors, retailers, and e-commerce companies. With the crystall ball you can:\n",
    "\n",
    "1. Perform Available-to-Promise (ATP) inquiries in seconds on real-time inventory changes due to purchases, manufacturing, sales, and shipments\n",
    "2. Learn when shipments are likely to be late\n",
    "3. Anticipate stock outs due to predicted late orders\n",
    "4. Determine what customers or downstream orders are affected by anticipated stockouts.\n",
    "\n",
    "When supply chain managers have the crystal ball, they can:\n",
    "-- plan around stock outs\n",
    "-- warn down stream consumers so they can re-plan.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%scala \n",
    "  println(\"Please copy and paste your JDBC URL. You can find it at the bottom right of your cluster dashboard\")\n",
    "//   val defaultJDBCURL = z.input(\"JDBCurl\",\"\"\"jdbc:splice://{FRAMEWORKNAME}-proxy.marathon.mesos:1527/splicedb;user=splice;password=admin;useSpark=true\"\"\").toString\n",
    "  val defaultJDBCURL = \"jdbc:splice://jrtest01-splice-hregion:1527/splicedb;user=splice;password=admin\"\n",
    "  val localJDBCURL = \"\"\"jdbc:splice://localhost:1527/splicedb;user=splice;password=admin\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%scala \n",
    "import java.sql.{Connection,Timestamp}\n",
    "import java.util.Date\n",
    "import com.splicemachine.si.api.txn.WriteConflict\n",
    "import org.apache.spark.sql.execution.datasources.jdbc.{JDBCOptions, JdbcUtils}\n",
    "import com.splicemachine.spark.splicemachine._\n",
    "    \n",
    "  val table = \"TimeLine_Int\"\n",
    "  val schema = \"TimeLine\"\n",
    "  val internalTN = schema + \".\" + table\n",
    "  val startOfTimeStr = \"1678-01-01 00:00:00\"\n",
    "  val endOfTimeStr = \"2261-12-31 00:00:00\"\n",
    "  val startOfTime = java.sql.Timestamp.valueOf(startOfTimeStr)\n",
    "  val endOfTime = java.sql.Timestamp.valueOf(endOfTimeStr)\n",
    "  val MAX_RETRIES: Integer = 2\n",
    "\n",
    "  val SQL_ID = 1\n",
    "  val SQL_ST = 2\n",
    "  val SQL_ET = 3\n",
    "  val SQL_VAL = 4\n",
    "  val DF_ID = 0\n",
    "  val DF_ST = 1\n",
    "  val DF_ET = 2\n",
    "  val DF_VAL = 3\n",
    "  \n",
    "  val columnsWithPrimaryKey: String  = \"(Timeline_Id bigint, \" + \"ST timestamp, \" + \"ET timestamp, \" + \"Val bigint, \" + \"primary key (Timeline_ID, ST)\" +\")\"\n",
    "  val columnsWithoutPrimaryKey = \"(\" + \"Timeline_Id bigint, \" + \"ST timestamp, \" + \"ET timestamp, \" + \"Val bigint \" + \")\"\n",
    "  val primaryKeys = Seq(\"Timeline_ID, ST\")\n",
    "  val columnsInsertString = \"(\" + \"Timeline_Id, \" + \"ST, \" + \"ET, \" + \"Val\" + \") \"\n",
    "  val columnsSelectString = \"Timeline_Id, \" + \"ST, \" + \"ET, \" + \"Value\"\n",
    "  val columnsInsertStringValues = \"values (?,?,?,?)\"\n",
    "\n",
    "\n",
    "  /* (t1<=ST and t2>ST) or (t1>ST and t1<ET)  (t1 t2 t1 t1 )*/\n",
    "  val overlapCondition = \"where Timeline_Id = ? and ((ST >=? and ST <?) or ((ST < ?) and (ET > >?)))\"\n",
    "\n",
    "\n",
    "  val internalOptions = Map(\n",
    "    org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.JDBC_TABLE_NAME -> internalTN,\n",
    "    org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.JDBC_URL -> defaultJDBCURL\n",
    "  )\n",
    "\n",
    "  val internalJDBCOptions = new org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions(internalOptions)\n",
    "  val splicemachineContext =  new com.splicemachine.spark.splicemachine.SplicemachineContext(defaultJDBCURL)\n",
    "\n",
    "\n",
    "\n",
    "  /**\n",
    "    *\n",
    "    * createTimeline (table)\n",
    "    *\n",
    "    * @param table table name of timeline\n",
    "    * @return\n",
    "    */\n",
    "  def createTimeline(table: String, columnsWithPrimaryKey: String , internalJDBCOptions: JDBCOptions = internalJDBCOptions ): Unit = {\n",
    "    val conn = JdbcUtils.createConnectionFactory(internalJDBCOptions)()\n",
    "    if (splicemachineContext.tableExists(table)){\n",
    "      conn.createStatement().execute(\"drop table \" + table)\n",
    "    }\n",
    "    conn.createStatement().execute(\"create table \" + table + columnsWithPrimaryKey)\n",
    "  }\n",
    "  \n",
    "  \n",
    "  \n",
    "  /**\n",
    "    *\n",
    "    * initialize (id startOfTime endOfTime value)\n",
    "    *\n",
    "    * @param table table name of timeline\n",
    "    * @param id id of timeline\n",
    "    * @param value initial value of timeline\n",
    "    * @return\n",
    "    */\n",
    " def initialize(table: String, id: Integer, value: Integer, columnsInsertString : String = columnsInsertString ,columnsInsertStringValues :String = columnsInsertStringValues  , internalJDBCOptions :JDBCOptions = internalJDBCOptions ): Unit = {\n",
    "    val conn = JdbcUtils.createConnectionFactory(internalJDBCOptions)()\n",
    "    val start: Timestamp = startOfTime\n",
    "    val end: Timestamp = endOfTime\n",
    "    try {\n",
    "      var ps = conn.prepareStatement(\"delete from \" + table + \" where timeline_id = \" + id)\n",
    "      ps.execute()\n",
    "      ps = conn.prepareStatement(\"insert into \" + table + columnsInsertString + columnsInsertStringValues)\n",
    "      ps.setInt(SQL_ID, id)\n",
    "      ps.setTimestamp(SQL_ST, start)\n",
    "      ps.setTimestamp(SQL_ET, end)\n",
    "      ps.setInt(SQL_VAL, value)\n",
    "      ps.execute()\n",
    "    } finally {\n",
    "      conn.close()\n",
    "    }\n",
    "  }\n",
    "\n",
    "  val CHANGE_AT_ST = 0\n",
    "  val CHANGE_AT_ET = 1\n",
    "  val CHANGE_BETWEEN_ST_ET =2\n",
    "  \n",
    "\n",
    "  /**\n",
    "    * splitMiddle - The new delta interval is subsumed by one interval.\n",
    "    *\n",
    "    *  ST------------ET\n",
    "    *      t1---t2         ==>   ST---t1 t1----t2 t2----ET\n",
    "    *\n",
    "    * Change the original interval to end at the start of the new delta interval\n",
    "    * Create a new record for the delta and apply the delta value\n",
    "    * Create a new record for the interval from the delta to the end of the original interval\n",
    "    *\n",
    "    * @param id - the id of the timeline to update\n",
    "    * @param t1 - the start of new delta\n",
    "    * @param t2 - the end of the new delta\n",
    "    * @param delta - an integer increment to the timeline\n",
    "    * @param persistence - CHANGE_AT_ST persists delta from t1 onwards\n",
    "    *                      CHANGE_AT_ET persists delta from t2 onwards\n",
    "    *                      CHANGE_BETWEEN_ST_ET persists delta during [t1 t2]\n",
    "    *\n",
    "    */\n",
    "    \n",
    "                \n",
    "  def splitMiddle(id: Integer,\n",
    "                  t1: java.sql.Timestamp, t2: java.sql.Timestamp,\n",
    "                  delta: Long,\n",
    "                  persistence: Int,\n",
    "                  internalTN : String =internalTN ,\n",
    "                  internalOptions : Map[String,String] = internalOptions): Unit = {\n",
    "    val df = sqlContext.read.options(internalOptions).splicemachine.where(s\"TIMELINE_ID = $id AND ST < to_utc_timestamp('$t1','GMT') AND ET > to_utc_timestamp('$t2','GMT')\")\n",
    "    if (df.count() > 0) {\n",
    "\n",
    "      /* Save old values */\n",
    "      var oldVal = df.first().getLong(DF_VAL)\n",
    "      var oldET = df.first().getTimestamp(DF_ET)\n",
    "\n",
    "      /* Update containing interval to be the begin split */\n",
    "      val updatedDF = df\n",
    "        .filter(s\"TIMELINE_ID = $id AND ST < to_utc_timestamp('$t1','GMT') AND ET > to_utc_timestamp('$t2','GMT')\")\n",
    "        .select(\"TIMELINE_ID\", \"ST\", \"ET\", \"VAL\")\n",
    "      .withColumn(\"ET\", lit(t1))\n",
    "      splicemachineContext.update(updatedDF, internalTN)\n",
    "\n",
    "      /* calculate persistence */\n",
    "      val firstValue: Long = persistence match {\n",
    "        case CHANGE_AT_ST          => oldVal + delta\n",
    "        case CHANGE_AT_ET          => oldVal\n",
    "        case CHANGE_BETWEEN_ST_ET  => oldVal + delta\n",
    "        case _                     => 0\n",
    "      }\n",
    "\n",
    "      /* Insert the two new splits */\n",
    "      /* Note - the second new split will have delta added\n",
    "\t\tin the persistAfter method\n",
    "       */\n",
    "      val newDF = sqlContext.createDataFrame(Seq(\n",
    "        (id, t1, t2, firstValue),\n",
    "        (id, t2, oldET, oldVal)))\n",
    "        .toDF(\"TIMELINE_ID\", \"ST\", \"ET\", \"VAL\")\n",
    "      splicemachineContext.insert(newDF, internalTN)\n",
    "    }\n",
    "  }\n",
    "\n",
    "\n",
    "  /***\n",
    "    * \tsplitAtEnd - Delta overlaps beginning of interval.\n",
    "    *\n",
    "    *         ST------ET\n",
    "    *      t1---t2         ==>  ST---t2 t2----ET\n",
    "    *\n",
    "    * Change the interval to end at the end of the delta then add a split from end of delta to the end of interval\n",
    "    *\n",
    "    * @param id - the id of the timeline to update\n",
    "    * @param t1 - the start of new delta\n",
    "    * @param t2 - the end of the new delta\n",
    "    * @param delta - an integer increment to the timeline\n",
    "    * @param persistence - CHANGE_AT_ST persists delta from t1 onwards\n",
    "    *                      CHANGE_AT_ET persists delta from t2 onwards\n",
    "    *                      CHANGE_BETWEEN_ST_ET persists delta during [t1 t2]\n",
    "    */\n",
    "    \n",
    "     \n",
    "  def splitAtEnd(id: Integer,\n",
    "                 t1: java.sql.Timestamp, t2: java.sql.Timestamp,\n",
    "                 delta: Long,\n",
    "                 persistence: Int,\n",
    "                  internalTN : String =internalTN ,\n",
    "                  internalOptions : Map[String,String] = internalOptions): Unit = {\n",
    "    val df = sqlContext.read.options(internalOptions).splicemachine\n",
    "      .where(s\"\"\"TIMELINE_ID = $id AND ST >= to_utc_timestamp('$t1','GMT') AND ET > to_utc_timestamp('$t2','GMT') AND ST < to_utc_timestamp('$t2','GMT')\"\"\")\n",
    "\n",
    "    if (df.count() > 0) {\n",
    "\n",
    "      /* Save old values */\n",
    "      var oldVal = df.first().getLong(DF_VAL)\n",
    "      var oldST = df.first().getTimestamp(DF_ST)\n",
    "      var oldET = df.first().getTimestamp(DF_ET)\n",
    "      /* Update overlapping interval to be the begin split */\n",
    "\n",
    "      /* calculate persistence */\n",
    "      /* Note - the second new split will have delta added\n",
    "          in the persistAfter method if required\n",
    " */\n",
    "      val firstValue: Long = persistence match {\n",
    "        case CHANGE_AT_ST          => oldVal + delta\n",
    "        case CHANGE_AT_ET          => oldVal\n",
    "        case CHANGE_BETWEEN_ST_ET  => oldVal + delta\n",
    "        case _                     => 0\n",
    "      }\n",
    "\n",
    "      val updatedDF = df\n",
    "\t    .filter(s\"TIMELINE_ID = $id AND ST >= to_utc_timestamp('$t1','GMT') AND ET > to_utc_timestamp('$t2','GMT') AND ST < to_utc_timestamp('$t2','GMT')\")\n",
    "        .select(\"TIMELINE_ID\", \"ST\", \"ET\", \"VAL\")\n",
    "      .withColumn(\"ET\", lit(t2))\n",
    "      .withColumn(\"VAL\", lit(firstValue))\n",
    "      splicemachineContext.update(updatedDF, internalTN)\n",
    "\n",
    "      /* Insert a new split after the delta */\n",
    "      val newDF = sqlContext.createDataFrame(Seq((id, t2, oldET, oldVal))).toDF(\"TIMELINE_ID\", \"ST\", \"ET\", \"VAL\")\n",
    "      splicemachineContext.insert(newDF, internalTN)\n",
    "    }\n",
    "  }\n",
    "\n",
    "  /**\n",
    "    * \tsplitAtStart - Delta overlaps end of interval.\n",
    "    *\n",
    "    *         ST-----ET\n",
    "    *            t1------t2         ==>    ST---t1 t1---ET\n",
    "    *\n",
    "    * Change the interval to end at the start of the delta then add a split from beginning of delta to the end of interval\n",
    "    *\n",
    "    * @param id - the id of the timeline to update\n",
    "    * @param t1 - the start of new delta\n",
    "    * @param t2 - the end of the new delta\n",
    "    * @param delta - an integer increment to the timeline\n",
    "    * @param persistence - CHANGE_AT_ST persists delta from t1 onwards\n",
    "    *                      CHANGE_AT_ET persists delta from t2 onwards\n",
    "    *                      CHANGE_BETWEEN_ST_ET persists delta during [t1 t2]\n",
    "    */\n",
    "   \n",
    "  def splitAtStart(id: Integer,\n",
    "                   t1: java.sql.Timestamp, t2: java.sql.Timestamp,\n",
    "                   delta: Long, persistence: Int,\n",
    "                  internalTN : String =internalTN ,\n",
    "                  internalOptions : Map[String,String] = internalOptions): Unit = {\n",
    "    val df = sqlContext.read.options(internalOptions).splicemachine.where(s\"TIMELINE_ID = $id AND ST < to_utc_timestamp('$t1','GMT') AND \" +\n",
    "                s\"ET < to_utc_timestamp('$t2','GMT') AND ET > to_utc_timestamp('$t1','GMT')\")\n",
    "    if (df.count() > 0) {\n",
    "\n",
    "      /* Save old values */\n",
    "      var oldVal = df.first().getLong(DF_VAL)\n",
    "      var oldST = df.first().getTimestamp(DF_ST)\n",
    "      var oldET = df.first().getTimestamp(DF_ET)\n",
    "\n",
    "      /* calculate persistence */\n",
    "      val newValue: Long = persistence match {\n",
    "        case CHANGE_AT_ST          => oldVal + delta\n",
    "        case CHANGE_AT_ET          => oldVal\n",
    "        case CHANGE_BETWEEN_ST_ET  => oldVal\n",
    "        case _                     => 0\n",
    "      }\n",
    "      /* Update overlapping interval to be the begin split */\n",
    "      val updatedDF = df\n",
    "        .filter(s\"TIMELINE_ID = $id AND ST < to_utc_timestamp('$t1','GMT') AND ET < to_utc_timestamp('$t2','GMT') AND ET > to_utc_timestamp('$t1','GMT')\")\n",
    "        .select(\"TIMELINE_ID\", \"ST\", \"ET\", \"VAL\")\n",
    "      .withColumn(\"ET\", lit(t1))\n",
    "      .withColumn(\"VAL\", lit(newValue))\n",
    "      splicemachineContext.update(updatedDF, internalTN)\n",
    "      \n",
    "      /* Insert a new split */\n",
    "      val newDF = sqlContext.createDataFrame(Seq(\n",
    "        (id, t1, oldET, oldVal)\n",
    "      )).toDF(\"TIMELINE_ID\", \"ST\", \"ET\", \"VAL\")\n",
    "      splicemachineContext.insert(newDF, internalTN)\n",
    "    }\n",
    " }\n",
    "                   \n",
    "    \n",
    "  /***\n",
    "    *   changeNoSplit - Handles all intervals contained by delta\n",
    "    *\n",
    "    *           ST-----ET\n",
    "    *     t1---------------t2\n",
    "    *\n",
    "    *  No splits required since always initialized with infinite time, just need values changed\n",
    "    *\n",
    "    * @param id - the id of the timeline to update\n",
    "    * @param t1 - the start of new delta\n",
    "    * @param t2 - the end of the new delta\n",
    "    * @param delta - an integer increment to the timeline\n",
    "    * @param persistence - CHANGE_AT_ST persists delta from t1 onwards\n",
    "    *                      CHANGE_AT_ET persists delta from t2 onwards\n",
    "    *                      CHANGE_BETWEEN_ST_ET persists delta during [t1 t2]\n",
    "    */\n",
    "  def changeNoSplit(id: Integer,\n",
    "                    t1: java.sql.Timestamp, t2: java.sql.Timestamp,\n",
    "                    delta: Long,\n",
    "                    persistence: Int,\n",
    "                  internalTN : String =internalTN ,\n",
    "                  internalOptions : Map[String,String] = internalOptions): Unit = {\n",
    "           \n",
    "    val df = sqlContext.read.options(internalOptions).splicemachine\n",
    "      .where(s\"TIMELINE_ID = $id AND ST >= to_utc_timestamp('$t1','GMT') AND ET <= to_utc_timestamp('$t2','GMT')\")\n",
    "\n",
    "    /* Calculate persistence */\n",
    "    val increment: Long = persistence match {\n",
    "      case CHANGE_AT_ST          => delta\n",
    "      case CHANGE_AT_ET          => 0\n",
    "      case CHANGE_BETWEEN_ST_ET  => delta\n",
    "      case _                     => 0\n",
    "    }\n",
    "    val updatedDF = df\n",
    "      .filter(s\"TIMELINE_ID = $id AND ST >= to_utc_timestamp('$t1','GMT') AND ET <= to_utc_timestamp('$t2','GMT')\")\n",
    "      .select(\"TIMELINE_ID\", \"ST\", \"ET\", \"VAL\")\n",
    "      .withColumn(\"VAL\", col(\"VAL\") + increment)\n",
    "\n",
    "    splicemachineContext.update(updatedDF, internalTN)\n",
    "  }\n",
    "\n",
    "  /***\n",
    "    *   persistAfter - changes the values for all intervals after delta\n",
    "    *\n",
    "    *     t1---------------t2  ST-----ET\n",
    "    *\n",
    "    *  No splits required since always initialized with infinite time, just need values changed\n",
    "    *\n",
    "    * @param id - the id of the timeline to update\n",
    "    * @param t1 - the start of new delta\n",
    "    * @param t2 - the end of the new delta\n",
    "    * @param delta - an integer increment to the timeline\n",
    "    * @param persistence - CHANGE_AT_ST persists delta from t1 onwards\n",
    "    *                      CHANGE_AT_ET persists delta from t2 onwards\n",
    "    *                      CHANGE_BETWEEN_ST_ET persists delta during [t1 t2]\n",
    "    */\n",
    "  def persistAfter(id: Integer,\n",
    "                   t1: java.sql.Timestamp, t2: java.sql.Timestamp,\n",
    "                   delta: Long,\n",
    "                   persistence: Int,\n",
    "                  internalTN : String =internalTN ,\n",
    "                  internalOptions : Map[String,String] = internalOptions): Unit = {\n",
    "\n",
    "    /* Persist delta after new splits if necesary */\n",
    "    if (persistence != CHANGE_BETWEEN_ST_ET) {\n",
    "      val persistDF = sqlContext.read.options(internalOptions).splicemachine\n",
    "        .filter(s\"TIMELINE_ID = $id AND ST >= to_utc_timestamp('$t2','GMT')\")\n",
    "        .select(\"TIMELINE_ID\", \"ST\", \"ET\", \"VAL\")\n",
    "      .withColumn(\"VAL\", col(\"VAL\") + delta)\n",
    "      splicemachineContext.update(persistDF, internalTN)\n",
    "    }\n",
    "  }\n",
    "\n",
    "  /** *\n",
    "    * update - increases/decreases the value for the interval\n",
    "    * from the start, end or during the interval\n",
    "    *\n",
    "    * @param table       - the name of the timeline table\n",
    "    * @param id          - the id of the timeline to update\n",
    "    * @param t1          - the start of new delta\n",
    "    * @param t2          - the end of the new delta\n",
    "    * @param delta       - an integer increment to the timeline\n",
    "    * @param persistence - CHANGE_AT_ST persists delta from t1 onwards\n",
    "    *                    CHANGE_AT_ET persists delta from t2 onwards\n",
    "    *                    CHANGE_BETWEEN_ST_ET persists delta during [t1 t2]\n",
    "    */\n",
    "\n",
    "  def update(table: String,\n",
    "             id: Integer,\n",
    "             t1: Timestamp, t2: Timestamp,\n",
    "             delta: Long,\n",
    "             persistence: Int): Unit = {\n",
    "                \n",
    "\n",
    "   val intOptions = Map(\n",
    "    org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.JDBC_TABLE_NAME -> table,\n",
    "    org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.JDBC_URL -> defaultJDBCURL\n",
    "  )\n",
    "    changeNoSplit(id, t1, t2, delta, persistence, table,intOptions )\n",
    "    splitAtStart(id, t1, t2, delta, persistence, table,intOptions)\n",
    "    splitMiddle(id, t1, t2, delta, persistence, table,intOptions)\n",
    "    splitAtEnd(id, t1, t2, delta, persistence, table,intOptions)\n",
    "    persistAfter(id, t1, t2, delta, persistence, table,intOptions)\n",
    "  }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application Requirements\n",
    "\n",
    "* Streaming inventory movements - ASN - EDI856\n",
    "* Streaming ancilary data like real-time weather\n",
    "* Needle-in-Haystack OLTP queries and range scans for ATP\n",
    "* ACID Transactions for concurrent inventory changes\n",
    "* OLAP for analysis and feature engineering\n",
    "* OLAP for inventory projections\n",
    "* Machine Learning\n",
    "* UI for Collaboration\n",
    "\n",
    "Splice Machine has all these components pre-integrated and optimized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest Advanced Shipping Notices - Transfer Orders\n",
    "\n",
    "Here you can copy and paste these data ingestion calls.\n",
    "\n",
    "The data was generated by a supply-chain simulator http://localhost:8080/#/notebook/2CKG62TQU.\n",
    "\n",
    "The simulator ticks through time randomly inserting Transfer Orders and also randomly inserts changes to Transfer Orders delivery dates. \n",
    "\n",
    "The generator randomly selects features for the transfer orders. \n",
    "\n",
    "The files below reflect the state of the database after the simulation runs.\n",
    "\n",
    "The orders and change orders are independent files that can be loaded separately.\n",
    "\n",
    "The simulation runs are cumulative meaning demo has the inventory timelines for the test data and the train data. \n",
    "\n",
    "So to use the demo data. Load train and test for orders and change orders and the load the demo inventory timeline file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%scala \n",
    "\n",
    "z.run(\"20170622-063514_1166002275\"); // JDBC URL\n",
    "z.run(\"20170622-231413_1135446195\"); // Timeline Code\n",
    "z.run(\"20170622-222153_977899468\"); // DDL\n",
    "z.run(\"20170623-174025_718327456\"); // Load Data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "create schema TIMELINE;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "\n",
    "drop table IF EXISTS  TIMELINE.TRANSFERORDERS;\n",
    "drop table IF EXISTS  TIMELINE.TO_DELIVERY_CHG_EVENT;\n",
    "drop table IF EXISTS  TIMELINE.TIMELINE_INT;\n",
    "drop table IF EXISTS TIMELINE.STOCKOUTS;\n",
    "\n",
    "\n",
    "\n",
    "create table TIMELINE.TRANSFERORDERS(\n",
    "    TO_ID   BIGINT,\n",
    "    PO_ID   BIGINT,\n",
    "    SHIPFROM BIGINT,\n",
    "    SHIPTO  BIGINT,\n",
    "    SHIPDATE TIMESTAMP,\n",
    "    DELIVERYDATE TIMESTAMP,\n",
    "    MODDELIVERYDATE TIMESTAMP,\n",
    "    SOURCEINVENTORY BIGINT,\n",
    "    DESTINATIONINVENTORY BIGINT,\n",
    "    QTY BIGINT,\n",
    "    SUPPLIER BIGINT,\n",
    "    ASN VARCHAR(100),\n",
    "    CONTAINER VARCHAR(100),\n",
    "    TRANSPORTMODE SMALLINT,\n",
    "    CARRIER BIGINT,\n",
    "    FROMWEATHER SMALLINT,\n",
    "    TOWEATHER SMALLINT,\n",
    "    LATITUDE  DOUBLE,\n",
    "    LONGITUDE DOUBLE,\n",
    "    primary key (TO_ID)\n",
    "    );\n",
    "\n",
    "create index TIMELINE.TOSTIDX on TRANSFERORDERS (\n",
    "    ShipDate,\n",
    "    TO_Id\n",
    " );\n",
    " \n",
    " create index TIMELINE.TOETIDX on TRANSFERORDERS (\n",
    "    Deliverydate,\n",
    "    TO_Id\n",
    " );\n",
    "\n",
    "create table TIMELINE.TO_DELIVERY_CHG_EVENT(\n",
    "    TO_event_Id bigint,\n",
    "    TO_Id bigint ,\n",
    "    ShipFrom bigint,\n",
    "    ShipTo bigint,\n",
    "    OrgDeliveryDate timestamp,\n",
    "    newDeliveryDate timestamp,\n",
    "    Supplier varchar(100) ,\n",
    "    TransportMode smallint ,\n",
    "    Carrier bigint ,\n",
    "    Fromweather smallint,\n",
    "    ToWeather smallint,\n",
    "    primary key (TO_event_Id)\n",
    "    );\n",
    "    \n",
    "create table TIMELINE.TIMELINE_INT(\n",
    "    Timeline_Id BIGINT,\n",
    "    ST          TIMESTAMP,\n",
    "    ET          TIMESTAMP,\n",
    "    VAL         BIGINT,\n",
    "    primary key (Timeline_Id, ST)\n",
    "    );\n",
    "    \n",
    "create table TIMELINE.STOCKOUTS(\n",
    "    TO_ID   BIGINT,\n",
    "    Timeline_Id BIGINT,\n",
    "    ST          TIMESTAMP,\n",
    "    primary key (TO_ID,ST)\n",
    "    );\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "\n",
    "call SYSCS_UTIL.IMPORT_DATA('TIMELINE','TRANSFERORDERS',null, 's3a://splice-demo/supplychain/data_0623/train_orders.csv', null, null, 'yyyy-MM-dd HH:mm:ss.S', null, null, -1, '/tmp', true, null);\n",
    "\n",
    "call SYSCS_UTIL.IMPORT_DATA('TIMELINE','TO_DELIVERY_CHG_EVENT', null, 's3a://splice-demo/supplychain/data_0623/train_events.csv', null, null, 'yyyy-MM-dd HH:mm:ss.S', null, null, -1, '/tmp', true, null);\n",
    "\n",
    "call SYSCS_UTIL.IMPORT_DATA('TIMELINE','TIMELINE_INT', null, 's3a://splice-demo/supplychain/data_0623/train_inv.csv', null, null, 'yyyy-MM-dd HH:mm:ss.S', null, null, -1, '/tmp', true, null);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "\n",
    "call SYSCS_UTIL.IMPORT_DATA('TIMELINE','TRANSFERORDERS',null, 's3a://splice-demo/supplychain/data_0623/test_orders.csv', null, null, 'yyyy-MM-dd HH:mm:ss.S', null, null, -1, '/tmp', true, null);\n",
    "call SYSCS_UTIL.IMPORT_DATA('TIMELINE','TO_DELIVERY_CHG_EVENT', null, 's3a://splice-demo/supplychain/data_0623/test_events.csv', null, null, 'yyyy-MM-dd HH:mm:ss.S', null, null, -1, '/tmp', true, null);\n",
    "call SYSCS_UTIL.IMPORT_DATA('TIMELINE','TIMELINE_INT', null, 's3a://splice-demo/supplychain/data_0623/test_inv.csv', null, null, 'yyyy-MM-dd HH:mm:ss.S', null, null, -1, '/tmp', true, null);\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "\n",
    "\n",
    "call SYSCS_UTIL.IMPORT_DATA('TIMELINE','TRANSFERORDERS',null, 's3a://splice-demo/supplychain/data_0623/demo_orders.csv', null, null, 'yyyy-MM-dd HH:mm:ss.S', null, null, -1, '/tmp', true, null);\n",
    "call SYSCS_UTIL.IMPORT_DATA('TIMELINE','TO_DELIVERY_CHG_EVENT', null, 's3a://splice-demo/supplychain/demo_0623/demo_events.csv', null, null, 'yyyy-MM-dd HH:mm:ss.S', null, null, -1, '/tmp', true, null);\n",
    "call SYSCS_UTIL.IMPORT_DATA('TIMELINE','TIMELINE_INT', null, 's3a://splice-demo/supplychain/data_0623/demo_inv.csv', null, null, 'yyyy-MM-dd HH:mm:ss.S', null, null, -1, '/tmp', true, null);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "select * from timeline.transferorders\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timelines\n",
    "\n",
    "Timelines are a relational representation of temporal data for AI applications \n",
    "\n",
    "Timelines record historical, present and future values.\n",
    "\n",
    "A timeline table contains a collection of timelines, each with a unique id.\n",
    "Every row represents: `TIMELINE_ID = VAL @ [ST ET]` meaning the variable denoted by the id has the value over that time interval\n",
    "\n",
    "Timelines require indexed row-based storage and an OLTP compute engine to quickly look up values associated at times.\n",
    "\n",
    "Timelines require ACID properties because they serve concurrent users changing timelines plus all the timeline updates require atomic changes to timelines.\n",
    "\n",
    "For example, you have to make sure that when you move an order that the decrement to the source inventory changes atomically with the change to the destination inventory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "select * from timeline.timeline_int\n",
    "where TIMELINE_ID = ${inv=200}\n",
    "order by TIMELINE.TIMELINE_INT.ST;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "select * from timeline.timeline_int\n",
    "where timeline_id = ${inv= 100} AND val < 0 \n",
    "order by timeline.timeline_int.st;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "select VAL AS Available from timeline.timeline_int\n",
    "where timeline_id = ${inv= 100} \n",
    "AND ST <= TIMESTAMP('${Time=2017-05-05 00:00:00.0}')  \n",
    "AND ET > TIMESTAMP('${Time=2017-05-05 00:00:00.0}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply the Crystal Ball\n",
    "#### Perform a What-If on any predicted late shipment\n",
    "\n",
    "By moving the shipment to the predicted new delivery date, you can see the new inventory levels and plan around stockouts.\n",
    "\n",
    "Below we create a prediction table that materializes predictions on some set of orders as to whether they are late. Then we initialize the table randomly for demostration purposes only. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "Drop table IF EXISTS TIMELINE.PREDICTIONS;\n",
    "create table TIMELINE.PREDICTIONS(\n",
    "    TO_ID   BIGINT,\n",
    "    LatenessBin1 DOUBLE,\n",
    "    LatenessBin2 DOUBLE,\n",
    "    LatenessBin3 DOUBLE,\n",
    "    LatenessBin4 DOUBLE,\n",
    "    LatenessBin5 DOUBLE,\n",
    "    LatenessBin6 DOUBLE,\n",
    "    LatenessBin7 DOUBLE,\n",
    "    LatenessBin8 DOUBLE,\n",
    "    LatenessBin9 DOUBLE,\n",
    "    LatenessBin10 DOUBLE,\n",
    "    primary key (TO_ID)\n",
    "    );\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "insert into TIMELINE.PREDICTIONS (\n",
    "    TO_ID, \n",
    "    LatenessBin1,\n",
    "    LatenessBin2,\n",
    "    LatenessBin3,\n",
    "    LatenessBin4\n",
    "    )\n",
    "    SELECT TO_ID, RANDOM(), RANDOM(), RANDOM(), RANDOM() \n",
    "    From TIMELINE.TRANSFERORDERS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "SELECT TP.TO_ID, TP.LatenessBin1 as ZERO_DAYLATE, TP.LatenessBin2 as ONE_DAYLATE, TP.LatenessBin3 as FIVE_DAYLATE,  TP.LatenessBin4 as TEN_DAYLATE, timeline.transferorders.*    FROM timeline.predictions TP  LEFT OUTER JOIN timeline.transferorders  ON TP.to_id = timeline.transferorders.to_id\n",
    "where TIMESTAMP('${begin =2017-05-05 00:00:00.0}') >= timeline.transferorders.deliverydate \n",
    "AND TIMESTAMP('${end =2017-05-05 00:00:00.0}') >timeline.transferorders.deliverydate \n",
    "AND TP.LatenessBin3 >= ${threshold = .75}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What-If Simulation \n",
    "#### Perform a What-If for the specified Order with specified delay in shipment\n",
    "\n",
    "Pick an order that may be delayed by number of days to see what orders may result in stock out situation because of the delay.\n",
    "\n",
    "First all the orders that are sourced from the Destination of the Order in consideration, the ones that have stockout are listed, before the delay in delivery date for comparison.\n",
    "Next the delay is simulated, and inventory calculations  are made and the Orders that are sourced from the Destination, are again checked for stockout situation.\n",
    "\n",
    "Since the what-if calculations are done on temporary table and does not impact the actual data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": "auto"
   },
   "outputs": [],
   "source": [
    "%angular\n",
    "\n",
    "<form class=\"form-inline\">\n",
    "  <div class=\"form-group\">\n",
    "   <h5>Simulate Late Order </h5>\n",
    "    <label for=\"orderFieldId\"> Order ID : </label>\n",
    "    <input type=\"text\" class=\"form-control\" id=\"orderFieldId\" placeholder= Order id ...\" ng-model=\"orderId\"></input>\n",
    "    <label for=\"delayFieldId\">Delay in Days: </label>\n",
    "    <input type=\"text\" class=\"form-control\" id=\"delayFieldId\" placeholder= Delay Days ...\" ng-model=\"delayDays\"></input>\n",
    "      <button type=\"submit\" class=\"btn btn-primary\" ng-click=\"z.angularBind('orderId',orderId,'20170625-202310_651912174');z.angularBind('delayDays',delayDays,'20170625-202310_651912174'); z.runParagraph('20170625-202310_651912174')\"> Run What-If</button>\n",
    "  </div>\n",
    "\n",
    "</form>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%scala \n",
    "\n",
    "val splicemachineContext = new SplicemachineContext(defaultJDBCURL)\n",
    "val InventoryTable = \"timeline.timeline_int\"\n",
    "val TOTable = \"timeline.transferorders\"\n",
    "val stockoutTable = \"timeline.STOCKOUTS\"\n",
    "val CHANGE_AT_ST = 0\n",
    "val CHANGE_AT_ET = 1\n",
    "val tempTableColsWithPKey : String  = \"(Timeline_Id bigint, \" + \"ST timestamp, \" + \"ET timestamp, \" + \"Val bigint, \" + \"primary key (Timeline_ID, ST)\" +\")\"\n",
    "\n",
    "def createTempInvTable (smContext :SplicemachineContext, source : Long, dest : Long): String = {\n",
    "    \n",
    "    var tempTable =\"Timeline.\"+  \"TEMP_INV_\" + org.apache.commons.lang3.RandomStringUtils.randomAlphabetic(6).toUpperCase();\n",
    "            while(smContext.tableExists( tempTable))\n",
    "                tempTable =\"Timeline.\"+  \"TEMP_INV_\" +org.apache.commons.lang3.RandomStringUtils.randomAlphabetic(6).toUpperCase();\n",
    "    \n",
    "    \n",
    "    \n",
    "    val tempOptions = Map(\n",
    "         org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.JDBC_TABLE_NAME -> tempTable,\n",
    "        org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions.JDBC_URL -> defaultJDBCURL\n",
    "    )\n",
    "\n",
    "    val tempJDBCOptions = new org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions(tempOptions)\n",
    "\n",
    "    \n",
    "    val conn = JdbcUtils.createConnectionFactory(tempJDBCOptions)()\n",
    "    try {\n",
    "    conn.createStatement().execute(\"create table \" + tempTable + tempTableColsWithPKey)\n",
    "    } finally {\n",
    "      conn.close()\n",
    "    }\n",
    "    \n",
    "    //insert \n",
    "    val stmt = \"select *  FROM timeline.timeline_int  WHERE Timeline_Id  in ( \" + source + \", \"+ dest +\")\"\n",
    "    val timesDf = splicemachineContext.df(stmt)\n",
    "    smContext.insert(timesDf,tempTable )\n",
    "   tempTable\n",
    " \n",
    "    \n",
    "}\n",
    "\n",
    "def whatif(smContext :SplicemachineContext,\n",
    "            tempInvTable: String,\n",
    "           source: Integer,\n",
    "           destination: Integer,\n",
    "           shippingDate: Timestamp,\n",
    "           deliveryDate: Timestamp,\n",
    "           newDeliveryDate : Timestamp,\n",
    "           qty: Long,\n",
    "           retryCount: Integer = 0,\n",
    "           revertFlag: Boolean): Unit = {\n",
    "\n",
    "        val conn: Connection = smContext.getConnection()\n",
    "         try {\n",
    "          conn.setAutoCommit(false) //TBD - Need to set to false when DBAAS-570 is resolved\n",
    "          update(tempInvTable, source, shippingDate, deliveryDate,  qty, CHANGE_AT_ST)\n",
    "          update(tempInvTable, destination, shippingDate, deliveryDate,  -qty, CHANGE_AT_ET)\n",
    "          update(tempInvTable, source, shippingDate, newDeliveryDate,  -qty, CHANGE_AT_ST)\n",
    "          update(tempInvTable, destination, shippingDate, newDeliveryDate,  qty, CHANGE_AT_ET)\n",
    "          conn.commit()\n",
    "          \n",
    "        }\n",
    "        catch {\n",
    "          case exp: WriteConflict => {\n",
    "            conn.rollback()\n",
    "            conn.setAutoCommit(true)\n",
    "            if (retryCount < MAX_RETRIES) {\n",
    "              println(\"Retrying create TO\" + source + \" \" + destination + \" \" + shippingDate + \" \" + deliveryDate + \" \" + qty + \" \" + retryCount + 1)\n",
    "              whatif(smContext, tempInvTable,source, destination, shippingDate, deliveryDate, newDeliveryDate, qty, retryCount + 1, revertFlag)\n",
    "            }\n",
    "            else {\n",
    "              // put code here to handle too many retries\n",
    "            }\n",
    "          }\n",
    "          case e: Throwable => println(s\"Got some other kind of exception: $e\")\n",
    "        }\n",
    "        finally {\n",
    "          conn.setAutoCommit(true)\n",
    "        }\n",
    "      }\n",
    "      \n",
    "      \n",
    "  // Will need to copy the inventory table so that what-if is not visible to others\n",
    "\n",
    "  val transferOrdersTable = Map(\n",
    "    JDBCOptions.JDBC_TABLE_NAME -> \"Timeline.TransferOrders\",\n",
    "    JDBCOptions.JDBC_URL -> defaultJDBCURL\n",
    "    ) \n",
    "  val orderid = z.angular(\"orderId\").toString.toLong\n",
    "  val q = s\"select *  FROM timeline.transferorders WHERE to_id = $orderid\"\n",
    "  val order = splicemachineContext.df(q)\n",
    "  \n",
    "   val days = z.angular(\"delayDays\").toString.toInt\n",
    " \n",
    "  if(order.count > 0 && days > 0) {\n",
    "    //  val days: Int = 5\n",
    "      val source = order.first().getAs(\"SOURCEINVENTORY\").asInstanceOf[Long]\n",
    "      val dest = order.first().getAs(\"DESTINATIONINVENTORY\").asInstanceOf[Long]\n",
    "      val ship = order.first().getAs(\"SHIPDATE\").asInstanceOf[Timestamp]\n",
    "      val delivery = order.first().getAs(\"DELIVERYDATE\").asInstanceOf[Timestamp]\n",
    "      val qty = order.first().getAs(\"QTY\").asInstanceOf[Long]\n",
    "      val newDelivery =new Timestamp( new org.joda.time.DateTime (delivery).plusDays(days).getMillis())\n",
    "      \n",
    "      //Populate stockouts before What If\n",
    "      \n",
    "     val queryBefore = s\"\"\"SELECT t.to_id, i.ST, i.timeline_id FROM $InventoryTable i  , $TOTable t\n",
    "        WHERE i.timeline_id = $dest\n",
    "        AND t.sourceinventory = i.timeline_id\n",
    "        AND val < 0\n",
    "        AND i.ST <=  t.shipdate\n",
    "        AND  t.shipdate < i.ET\n",
    "        ORDER BY i.ST\"\"\"\n",
    "    \n",
    "       println(s\"q=$queryBefore\")\n",
    "      val stockOutsBefore = splicemachineContext.df(queryBefore)\n",
    "  \n",
    "      splicemachineContext.insert(stockOutsBefore, stockoutTable)\n",
    "      z.run(\"20170621-055239_1661420434\")\n",
    "      z.run(\"20170628-152508_1831563439\")\n",
    "      \n",
    "      val tempInventoryTable = createTempInvTable(splicemachineContext,source,dest)\n",
    "      \n",
    "      whatif(splicemachineContext,tempInventoryTable,source.toInt, dest.toInt, ship, delivery, newDelivery, qty, days, false)\n",
    "      \n",
    "      val destInvCol = TOTable + \".destinationinventory \"\n",
    "      val timelineIdCol = tempInventoryTable + \".timeline_id \"\n",
    "      val toIdCol = TOTable + \".to_id \"\n",
    "      val stCol = tempInventoryTable + \".ST \"\n",
    "      val etCol = tempInventoryTable + \".ET \"\n",
    "      val delDateCol = TOTable + \".deliverydate \"\n",
    "      val shipDateCol =  TOTable + \".shipdate \"\n",
    "      val sourceInvCol = TOTable + \".sourceinventory \"\n",
    "      val latCol = TOTable + \".latitude \"\n",
    "      val longCol = TOTable + \".longitude \"\n",
    "      val srcWeatherCol = TOTable + \".fromweather \"\n",
    "      val destWeatherCol = TOTable + \".toweather \"\n",
    "      /*\n",
    "      val query = s\"\"\"SELECT $toIdCol, $stCol, $timelineIdCol FROM $tempInventoryTable , $TOTable\n",
    "                        WHERE $timelineIdCol = $dest\n",
    "                        AND $destInvCol = $timelineIdCol\n",
    "                        AND val < 0\n",
    "                        AND $stCol >= $delDateCol\n",
    "                        ORDER BY $stCol\"\"\"\n",
    "                        println(s\"q=$query\")\n",
    "                        */\n",
    "   \n",
    "     val query = s\"\"\"SELECT $toIdCol, $stCol, $timelineIdCol FROM $tempInventoryTable , $TOTable\n",
    "        WHERE $timelineIdCol = $dest\n",
    "        AND $sourceInvCol = $timelineIdCol\n",
    "        AND val < 0\n",
    "        AND $stCol <=  $shipDateCol\n",
    "        AND  $shipDateCol < $etCol\n",
    "        ORDER BY $stCol\"\"\"\n",
    "    \n",
    "    println(s\"q=$query\")\n",
    "      val stockOuts = splicemachineContext.df(query)\n",
    "  \n",
    "     // whatif(source.toInt, dest.toInt, ship, delivery, newDelivery, qty, 5, true) // undo what-if \n",
    "      splicemachineContext.insert(stockOuts, stockoutTable)\n",
    "      z.run(\"20170716-165322_7991113\")\n",
    "      z.run(\"20170628-152508_1831563439\")\n",
    "     // splicemachineContext.dropTable(tempInventoryTable)\n",
    "   } else {\n",
    "    println(\" NO ORDERS FOUND FOR ORDER ID \" + orderid)\n",
    "   }\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "select \n",
    "    timeline.stockouts.st,\n",
    "    timeline.timeline_int.et,\n",
    "    val,\n",
    "    timeline.stockouts.to_id,\n",
    "    shipfrom,\n",
    "    shipto,\n",
    "    qty,\n",
    "    DESTINATIONINVENTORY,\n",
    "    SOURCEINVENTORY,\n",
    "    LATITUDE,\n",
    "    LONGITUDE,\n",
    "    FROMWEATHER,\n",
    "    TOWEATHER,     \n",
    "    SUPPLIER,\n",
    "    CARRIER,\n",
    "    TRANSPORTMODE\n",
    "    from Timeline.Stockouts, Timeline.TransferOrders, timeline.Timeline_int \n",
    "    Where Timeline.Stockouts.to_id = Timeline.TransferOrders.to_id \n",
    "    AND Timeline.Stockouts.timeline_id =  timeline.Timeline_int.timeline_id\n",
    "    AND timeline.Timeline_int.timeline_id =  Timeline.TransferOrders.sourceinventory\n",
    "    AND  Timeline.Stockouts.ST =  timeline.Timeline_int.ST\n",
    "order by ST;\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "select \n",
    "    timeline.stockouts.st,\n",
    "    timeline.timeline_int.et,\n",
    "    val,\n",
    "    timeline.stockouts.to_id,\n",
    "    shipfrom,\n",
    "    shipto,\n",
    "    qty,\n",
    "    DESTINATIONINVENTORY,\n",
    "    SOURCEINVENTORY,\n",
    "    LATITUDE,\n",
    "    LONGITUDE,\n",
    "    FROMWEATHER,\n",
    "    TOWEATHER,     \n",
    "    SUPPLIER,\n",
    "    CARRIER,\n",
    "    TRANSPORTMODE\n",
    "    from Timeline.Stockouts, Timeline.TransferOrders, timeline.Timeline_int \n",
    "    Where Timeline.Stockouts.to_id = Timeline.TransferOrders.to_id \n",
    "    AND Timeline.Stockouts.timeline_id =  timeline.Timeline_int.timeline_id\n",
    "    AND timeline.Timeline_int.timeline_id =  Timeline.TransferOrders.sourceinventory\n",
    "    AND  Timeline.Stockouts.ST =  timeline.Timeline_int.ST\n",
    "order by ST;\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "delete from timeline.stockouts where to_id >0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "select * from timeline.TO_DELIVERY_CHG_EVENT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "select * from timeline.TO_DELIVERY_CHG_EVENT\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "select * from timeline.TO_DELIVERY_CHG_EVENT, timeline.transferorders \n",
    "where timeline.transferorders.to_id = timeline.TO_DELIVERY_CHG_EVENT.to_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning \n",
    "### Learning to Predict Late Shipments\n",
    "\n",
    "Here we use a logistic regression ML model to classify late orders. Our inventory system tracks events such as the delivery date on an order changing or the qty delivered being different than expected.\n",
    "\n",
    "The model considers attributes of the orders such as:\n",
    "- Mode of Transport\n",
    "- Carrier\n",
    "- Latitude \n",
    "- Longitude\n",
    "- Source City\n",
    "- Destination City\n",
    "- Part.\n",
    "\n",
    "The model also considers exogenous data to enrich the inventory data such as weather:\n",
    "- weather at source\n",
    "- weather at destination.\n",
    "\n",
    "The machine learning algorithm outputs a model that can predict whether a shipment is late. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ease of ML - NO ETL \n",
    "#### Just Transformations\n",
    "\n",
    "Splice Machine is HTAP so you can perform both transactional and analytical queries.\n",
    "\n",
    "Therefore we do not need to extract and load data - we only transform it.\n",
    "\n",
    "Below we transform the transfer orders and the changes to transfer orders into a view, computing how late the order is and binning the lateness into 0,1,5,and 10 day late bins.\n",
    "\n",
    "The first step is a classic transformation step of merging a master table of data with a table of changes and labeling the rows with classes and enriching it with outside data like weather in this case.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "\n",
    "drop table IF EXISTS TIMELINE.FEATURES;\n",
    "\n",
    "CREATE table TIMELINE.FEATURES AS\n",
    "SELECT \n",
    "    TimeLine.TO_DELIVERY_CHG_EVENT.orgdeliverydate,  \n",
    "    TimeLine.TO_DELIVERY_CHG_EVENT.newdeliverydate, \n",
    "    CASE WHEN TimeLine.TO_DELIVERY_CHG_EVENT.TO_EVENT_ID is Null \n",
    "        THEN TimeLine.TransferOrders.fromweather \n",
    "        ELSE TimeLine.TO_DELIVERY_CHG_EVENT.fromweather end as currentweather,\n",
    "    TimeLine.TransferOrders.*, \n",
    "    CASE WHEN TimeLine.TO_DELIVERY_CHG_EVENT.TO_EVENT_ID is Null \n",
    "        THEN 0 \n",
    "        ELSE TimeLine.TO_DELIVERY_CHG_EVENT.newdeliverydate - TimeLine.TO_DELIVERY_CHG_EVENT.orgdeliverydate end as Lateness,\n",
    "    CASE\n",
    "    WHEN  TimeLine.TO_DELIVERY_CHG_EVENT.newdeliverydate - TimeLine.TO_DELIVERY_CHG_EVENT.orgdeliverydate > 0 \n",
    "    THEN\n",
    "        CASE\n",
    "            WHEN  TimeLine.TO_DELIVERY_CHG_EVENT.newdeliverydate - TimeLine.TO_DELIVERY_CHG_EVENT.orgdeliverydate > 5 \n",
    "            THEN\n",
    "                CASE \n",
    "                    WHEN  TimeLine.TO_DELIVERY_CHG_EVENT.newdeliverydate - TimeLine.TO_DELIVERY_CHG_EVENT.orgdeliverydate > 10\n",
    "                    THEN 3\n",
    "                    ELSE 2\n",
    "                END\n",
    "            ELSE 1\n",
    "\n",
    "        END\n",
    "    ELSE 0\n",
    "    END AS Label\n",
    " from TimeLine.TransferOrders Left Outer Join TimeLine.TO_DELIVERY_CHG_EVENT\n",
    " on TimeLine.TransferOrders.TO_ID = TimeLine.TO_DELIVERY_CHG_EVENT.TO_ID\n",
    " WHERE  TIMESTAMP('${begin = 2017-05-05 00:00:00.0}') >= timeline.transferorders.deliverydate \n",
    "AND TIMESTAMP('${end =2017-05-05 00:00:00.0}') > timeline.transferorders.deliverydate; \n",
    " select * from timeline.features\n",
    " WHERE TIMESTAMP('${begin = 2017-05-05 00:00:00.0}') >= timeline.features.orgdeliverydate \n",
    "    AND TIMESTAMP('${end =2017-05-05 00:00:00.0}') > timeline.features.orgdeliverydate ;\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLlib\n",
    "\n",
    "MLlib is a rich repository of Transformers and Models. \n",
    "\n",
    "https://spark.apache.org/docs/latest/ml-guide.html\n",
    "\n",
    "In this use case we will use Logistic Regression to classify late orders into four classes: 0 days late, 1-5 days late, 5-10 days late and  10 or over days late.\n",
    "\n",
    "The Logistic Regression Model expects a dataframe with two elements: feature Vector and label.\n",
    "\n",
    "Therefore we have to extract the columns from the above table into this form.\n",
    "\n",
    "Luckily MLlib has such a transformer called a Vector Assembler.\n",
    "\n",
    "Below we create a Vector Assembler, extract some columns from or feature table and then feed this to the model.\n",
    "\n",
    "Then we can deploy the model to create the prediction table we used above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%scala \n",
    "    \n",
    "    import org.apache.spark.ml.feature.VectorAssembler\n",
    "    import java.sql.{Connection,Timestamp}\n",
    "    import com.splicemachine.spark.splicemachine._\n",
    "    import org.apache.spark.sql.execution.datasources.jdbc.{JDBCOptions, JdbcUtils}\n",
    "    import org.apache.spark.ml.classification.LogisticRegression\n",
    "    import spark.implicits._\n",
    "\n",
    "    \n",
    "    val optionMap = Map(\n",
    "      JDBCOptions.JDBC_TABLE_NAME -> \"Timeline.Features\",\n",
    "      JDBCOptions.JDBC_URL -> defaultJDBCURL\n",
    "    )\n",
    "    val dfUpper = sqlContext.read.options(optionMap).splicemachine\n",
    "    val newNames = Seq(\"orgdeliverydate\",\"newdeliverydate\",\"currentweather\",\"to_id\",\n",
    "      \"po_id\",\"shipfrom\",\"shipto\",\"shipdate\",\"deliverydate\",\"moddeliverydate\",\"sourceinventory\",\n",
    "      \"destinationinventory\",\"qty\",\"supplier\",\"asn\",\"container\",\"transportmode\",\"carrier\",\n",
    "      \"fromweather\",\"toweather\",\"latitude\",\"longitude\",\"lateness\",\"label\")\n",
    "    val df = dfUpper.toDF(newNames: _*)\n",
    "    \n",
    "    \n",
    "    //assemble feature vector from dataframe\n",
    "    val assembler = new VectorAssembler()\n",
    "      .setInputCols(Array(\"shipfrom\", \"shipto\", \"sourceinventory\", \"destinationinventory\", \"supplier\", \"transportmode\", \"carrier\", \"fromweather\", \"toweather\"))\n",
    "      .setOutputCol(\"features\")\n",
    "    \n",
    "    val output = assembler.transform(df)\n",
    "    println(\"Assembled columns ShipFrom, ShipTo, SourceInventory, DestinationInventory, Supplier, TransportMode, Carrier, FromWeather, ToWeather to vector column 'features'\")\n",
    "    output.select(\"features\", \"label\").show(true)\n",
    "    \n",
    "    // Set parameters for the algorithm.\n",
    "    // Here, we limit the number of iterations to 10.\n",
    "    val lr = new LogisticRegression()\n",
    "        .setMaxIter(10)\n",
    "\n",
    "        \n",
    "    \n",
    "    // Fit the model to the data.\n",
    "    val model = lr.fit(output)\n",
    "    \n",
    "   //Get the number of classes in Label\n",
    "     val numClasses = model.numClasses\n",
    "    // Given a dataset, predict each point's label, and show the results.\n",
    "    val newdf = model.transform(output)\n",
    "    \n",
    "    // Print the coefficients and intercept for multinomial logistic regression\n",
    "    println(s\"Coefficients: \\n${model.coefficientMatrix}\")\n",
    "    println(s\"Intercepts: ${model.interceptVector}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%scala \n",
    "import org.apache.spark.ml.linalg.{Vector, Vectors}\n",
    "import org.apache.spark.sql.types.{StructType,StructField,DoubleType, LongType}\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.sql.catalyst.encoders.RowEncoder\n",
    "\n",
    "val predictionTable = \"timeline.predictions\"\n",
    "\n",
    "var labelCnt = numClasses\n",
    "//Only allow max of 10 lables\n",
    "if(labelCnt > 10)\n",
    "    labelCnt =10\n",
    "\n",
    "    \n",
    " newdf.printSchema()\n",
    " \n",
    "    \n",
    "    var schema = StructType(\n",
    "    StructField(\"TO_ID\", LongType, false) :: Nil)\n",
    "   \n",
    "    \n",
    "    var i=0;\n",
    "    for (i <- 1 to labelCnt) {\n",
    "        schema = schema.add( StructField(\"LATENESSBIN\"+i, DoubleType, false) )\n",
    "    }\n",
    "           \n",
    "    \n",
    "val encoder = RowEncoder(schema)\n",
    " \n",
    " val pred = newdf\n",
    "  .select( \"features\", \"label\", \"probability\", \"prediction\", \"to_id\")\n",
    "  .map { case Row( features: Vector, label: Integer, prob: Vector, prediction: Double, idd:Long) => \n",
    "  \n",
    "    var seq1 : Seq[Any] = Seq(idd.asInstanceOf[Number].longValue())\n",
    "    var j=0;\n",
    "    for (j <- 1 to labelCnt) {\n",
    "        seq1 = seq1:+ ( prob(j-1).asInstanceOf[Number].doubleValue())\n",
    "    }\n",
    "    println(seq1)\n",
    "     Row.fromSeq(seq1)   \n",
    "    }(encoder)\n",
    "    \n",
    "    \n",
    "  splicemachineContext.update(pred, predictionTable) \n",
    "  pred.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%scala\n",
    "%lsmagic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lsmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
