{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration in Splice Machine\n",
    "#### Let's start our Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "from pyspark.sql import SparkSession\n",
    "from splicemachine.spark import PySpliceContext\n",
    "from splicemachine.mlflow_support.utilities import get_user\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "splice = PySpliceContext(spark)\n",
    "schema = get_user()\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingesting Data\n",
    "<blockquote><p class='quotation'><span style='font-size:15px'> There are a variety of ways to ingest data into splice machineâ€” here we'll use an import from s3 directly into a splice machine table. From there we'll be able to investigate our features. <footer>Splice Machine</footer>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "drop table if exists cc_fraud_data; \n",
    "create table cc_fraud_data (\n",
    "    time_offset integer,\n",
    "    expected_weekly_trans_cnt double,\n",
    "    expected_weekly_trans_amnt double,\n",
    "    expected_daily_trans_cnt double,\n",
    "    expected_daily_trans_amnt double,\n",
    "    weekly_trans_cnt double,\n",
    "    weekly_trans_amnt double,\n",
    "    daily_trans_cnt double,\n",
    "    daily_trans_amnt double,\n",
    "    rolling_avg_weekly_trans_cnt double,\n",
    "    rolling_avg_weekly_trans_amnt double,\n",
    "    rolling_avg_daily_trans_cnt double,\n",
    "    rolling_avg_daily_trans_amnt double,\n",
    "    MACD_trans_amnt double,\n",
    "    MACD_trans_cnt double,\n",
    "    RSI_trans_amnt double,\n",
    "    RSI_trans_cnt double,\n",
    "    Aroon_trans_amnt double,\n",
    "    Aroon_trans_cnt double,\n",
    "    ADX_trans_amnt double,\n",
    "    ADX_trans_cnt double,\n",
    "    current_balance double,\n",
    "    rolling_avg_balance double,\n",
    "    MACD_balance double,\n",
    "    Aroon_balance double,\n",
    "    RSI_balance double,\n",
    "    ADX_balance double,\n",
    "    credit_score double,\n",
    "    credit_limit double,\n",
    "    amount decimal(10,2),\n",
    "    class_result int\n",
    ");\n",
    "\n",
    "call SYSCS_UTIL.IMPORT_DATA (\n",
    "     null,\n",
    "     'cc_fraud_data',\n",
    "     null,\n",
    "     's3a://splice-demo/kaggle-fraud-data/creditcard.csv',\n",
    "     ',',\n",
    "     null,\n",
    "     null,\n",
    "     null,\n",
    "     null,\n",
    "     -1,\n",
    "     's3a://splice-demo/kaggle-fraud-data/bad',\n",
    "     null, \n",
    "     null);\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can use `SQL` natively to investigate our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "select class_result, count(*) as NUM_OCurrences \n",
    "from cc_fraud_data \n",
    "group by class_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "SELECT class_result, AVG(expected_weekly_trans_cnt) as avg_expected_weekly_trans_cnt, \n",
    "AVG(MACD_trans_amnt) as avg_MACD_trans_amnt, \n",
    "AVG(RSI_trans_amnt) as avg_RSI_trans_amnt\n",
    "from cc_fraud_data\n",
    "group by class_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Native Spark Data Source to get our data in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = splice.df(f\"SELECT * FROM {schema}.cc_fraud_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating the correlation among our features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "pdf = df.filter(df.CLASS_RESULT == 0).limit(900).toPandas()\\\n",
    "        .append(df.filter(df.CLASS_RESULT == 1).limit(100).toPandas())\n",
    "pdf = pdf.apply(pd.to_numeric)\n",
    "corr = pdf.corr()\n",
    "\n",
    "ticks = [i for i in range(len(corr.columns))]\n",
    "mask = np.triu(np.ones_like(corr, dtype=np.bool))\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "cmap = \"coolwarm\"\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "\n",
    "plt.xticks(ticks, corr.columns)\n",
    "plt.yticks(ticks, corr.columns)\n",
    "plt.title('Credit Card Fraud Data Feature Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating the distribution of our features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(pdf.columns)\n",
    "features.remove(\"CLASS_RESULT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=5, ncols=6, figsize=(30,10))\n",
    "\n",
    "for i in range(len(features)):\n",
    "    row = int(i/6)\n",
    "    col = i%6\n",
    "    \n",
    "    axes[row,col] = sns.distplot(pdf[features[i]], ax = axes[row,col])\n",
    "    axes[row,col].set_xticks([])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigating our features correlation to our target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from beakerx.object import beakerx\n",
    "beakerx.pandas_display_table()\n",
    "most_correlated = corr.abs()['CLASS_RESULT'].sort_values(ascending=False).reset_index()\n",
    "most_correlated = most_correlated.iloc[1:].rename({\"index\":\"feature\",\"CLASS_RESULT\":\"correlation_to_target\"}, axis = 1)\n",
    "print(most_correlated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fantastic!\n",
    "<blockquote> \n",
    "We've just demonstrated our platform's ability to execute simple feature investigation routines <br>\n",
    "    Next Up: <a href='./7.4 Feature Engineering.ipynb'>Using MLManager to Engineer Features</a>\n",
    "<footer>Splice Machine</footer>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
