{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering in Splice Machine\n",
    "#### Let's start our Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "from pyspark.sql import SparkSession\n",
    "from splicemachine.spark import PySpliceContext\n",
    "from splicemachine.mlflow_support.utilities import get_user\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "splice = PySpliceContext(spark)\n",
    "schema = get_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingesting Data\n",
    "<blockquote><p class='quotation'><span style='font-size:15px'> Using the table created in <a href='./7.3 Data Exploration.ipynb'>7.3 Data Exploration</a>, we will create features first with <code>SQL</code> and subsequently ingest into <code>PySpark</code> for further analysis. <footer>Splice Machine</footer>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Within the same platform, we're able to easily engineer features in a number of ways \n",
    "\n",
    "With native access to the Splice Database, we can engineer features using SQL. But we can also use PySpark, or external libraries, like Koalas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we'll calculate a couple simple features in SQL \n",
    "We're computing a couple quick scaled differenced features for our use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT     \n",
    "    time_offset,\n",
    "    expected_weekly_trans_cnt,\n",
    "    expected_weekly_trans_amnt,\n",
    "    expected_daily_trans_cnt,\n",
    "    expected_daily_trans_amnt,\n",
    "    weekly_trans_cnt,\n",
    "    weekly_trans_amnt,\n",
    "    daily_trans_cnt,\n",
    "    daily_trans_amnt,\n",
    "    rolling_avg_weekly_trans_cnt,\n",
    "    rolling_avg_weekly_trans_amnt,\n",
    "    rolling_avg_daily_trans_cnt,\n",
    "    rolling_avg_daily_trans_amnt,\n",
    "    MACD_trans_amnt,\n",
    "    MACD_trans_cnt,\n",
    "    RSI_trans_amnt,\n",
    "    RSI_trans_cnt,\n",
    "    Aroon_trans_amnt,\n",
    "    Aroon_trans_cnt,\n",
    "    ADX_trans_amnt,\n",
    "    ADX_trans_cnt,\n",
    "    current_balance,\n",
    "    rolling_avg_balance,\n",
    "    MACD_balance,\n",
    "    Aroon_balance,\n",
    "    RSI_balance,\n",
    "    ADX_balance,\n",
    "    credit_score,\n",
    "    credit_limit,\n",
    "    amount,\n",
    "    (weekly_trans_cnt - expected_weekly_trans_cnt)/expected_weekly_trans_cnt AS weekly_trans_cnt_DIFF,\n",
    "    (weekly_trans_amnt - expected_weekly_trans_amnt)/expected_weekly_trans_amnt AS weekly_trans_amnt_DIFF,\n",
    "    (daily_trans_cnt - expected_daily_trans_cnt)/expected_daily_trans_cnt AS daily_trans_cnt_DIFF,\n",
    "    (daily_trans_amnt - expected_daily_trans_amnt)/expected_daily_trans_amnt AS daily_trans_amnt_DIFF\n",
    "FROM CC_FRAUD_DATA\n",
    "{LIMIT 10}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingesting the data with these new features into splice machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = splice.df(f\"\"\"\n",
    "SELECT     \n",
    "    time_offset,\n",
    "    expected_weekly_trans_cnt,\n",
    "    expected_weekly_trans_amnt,\n",
    "    expected_daily_trans_cnt,\n",
    "    expected_daily_trans_amnt,\n",
    "    weekly_trans_cnt,\n",
    "    weekly_trans_amnt,\n",
    "    daily_trans_cnt,\n",
    "    daily_trans_amnt,\n",
    "    rolling_avg_weekly_trans_cnt,\n",
    "    rolling_avg_weekly_trans_amnt,\n",
    "    rolling_avg_daily_trans_cnt,\n",
    "    rolling_avg_daily_trans_amnt,\n",
    "    MACD_trans_amnt,\n",
    "    MACD_trans_cnt,\n",
    "    RSI_trans_amnt,\n",
    "    RSI_trans_cnt,\n",
    "    Aroon_trans_amnt,\n",
    "    Aroon_trans_cnt,\n",
    "    ADX_trans_amnt,\n",
    "    ADX_trans_cnt,\n",
    "    current_balance,\n",
    "    rolling_avg_balance,\n",
    "    MACD_balance,\n",
    "    Aroon_balance,\n",
    "    RSI_balance,\n",
    "    ADX_balance,\n",
    "    credit_score,\n",
    "    credit_limit,\n",
    "    amount,\n",
    "    (weekly_trans_cnt - expected_weekly_trans_cnt) AS weekly_trans_cnt_DIFF,\n",
    "    (weekly_trans_amnt - expected_weekly_trans_amnt) AS weekly_trans_amnt_DIFF,\n",
    "    (daily_trans_cnt - expected_daily_trans_cnt) AS daily_trans_cnt_DIFF,\n",
    "    (daily_trans_amnt - expected_daily_trans_amnt) AS daily_trans_amnt_DIFF,\n",
    "    CLASS_RESULT\n",
    "FROM {schema}.CC_FRAUD_DATA\"\"\")\n",
    "sdf.select(sdf.columns[-8:]).describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now engineering features with PySpark\n",
    "Here, we calculate the z-score for the AMOUNT feature based on class result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from beakerx.object import beakerx\n",
    "beakerx.pandas_display_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_zscore = sdf.join(F.broadcast(sdf.groupBy(\"CLASS_RESULT\").agg(\n",
    "                        F.stddev_pop(\"AMOUNT\").alias(\"AMOUNT_sd\"), \n",
    "                        F.avg(\"AMOUNT\").alias(\"AMOUNT_avg\"))),\n",
    "             \"CLASS_RESULT\")\\\n",
    "        .withColumn(\"AMOUNT_Z_SCORE\", (F.col(\"AMOUNT\") - F.col(\"AMOUNT_avg\")) / F.col(\"AMOUNT_sd\")).limit(150)\n",
    "\n",
    "df_zscore.select('AMOUNT','WEEKLY_TRANS_CNT_DIFF','WEEKLY_TRANS_AMNT_DIFF','DAILY_TRANS_CNT_DIFF','DAILY_TRANS_AMNT_DIFF', 'AMOUNT_Z_SCORE').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can do the same thing with Koalas if that is preferred\n",
    "\n",
    "Now calculating the z score normalization by class group for `current_balance` manually with Koalas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import zscore\n",
    "import databricks.koalas as ks\n",
    "ks.set_option('compute.ops_on_diff_frames', True)\n",
    "kdf = df_zscore.to_koalas()\n",
    "kdf['CURRENT_BALANCE_Z_SCORE'] = kdf.groupby(\"CLASS_RESULT\").CURRENT_BALANCE.transform(lambda x: zscore(x))\n",
    "    \n",
    "kdf[['AMOUNT','WEEKLY_TRANS_CNT_DIFF','WEEKLY_TRANS_AMNT_DIFF','DAILY_TRANS_CNT_DIFF','DAILY_TRANS_AMNT_DIFF', 'AMOUNT_Z_SCORE','CURRENT_BALANCE_Z_SCORE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fantastic!\n",
    "<blockquote> \n",
    "Now you can start building basic and advanced feature engineering tasks in both SQL and PySpark! <br>\n",
    "    Next Up: <a href='./7.5 Model Creation.ipynb'>Using MLManager to create basic machine learning models.</a>\n",
    "<footer>Splice Machine</footer>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "268px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
