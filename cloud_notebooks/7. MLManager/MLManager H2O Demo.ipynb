{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "#s {\n",
    "}\n",
    "h1, h2, h3, h4, h5, h6, table, button, a, p, blockquote {\n",
    "font-family:Geneva;\n",
    "}\n",
    "\n",
    ".log {\n",
    "transition: all .2s ease-in-out;\n",
    "}\n",
    "\n",
    ".log:hover {a\n",
    "transform: scale(1.05);\n",
    "}\n",
    "</style>\n",
    "<div id='s' style='width:90%'>\n",
    "<center><img class='log' src='https://splicemachine.com/wp-content/uploads/splice-logo-1.png' width='20%' style='z-index:5'></center>\n",
    "<center><h1 class='log' style='font-size:40px; color:black;'>Welcome to Splice Machine MLManager</h1></center>\n",
    "<center><h2 class = 'log' style='font-size:25px; color:grey;'>The data platform for intelligent applications</center>\n",
    "<center><img class='log' src='https://splice-demo.s3.amazonaws.com/splice-machine-data-science-process-h2o.png' width='40%' style='z-index:5'></center>\n",
    "</div>\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this notebook, we're going to take a look at using MLManager with [H2O](https://www.h2o.ai/) + [Spark](https://spark.apache.org/)\n",
    "<h2 style='font-size:25px;  font-weight:bold'>What is <a href=http://docs.h2o.ai/sparkling-water/2.1/latest-stable/doc/pysparkling.html>PySparkling Water?</a> What is <a href=https://splicemachine.com/product/ml-manager/>MLManager?</a></h2>\n",
    "<style>\n",
    "blockquote{\n",
    "  font-size: 15px;\n",
    "  background: #f9f9f9;\n",
    "  border-left: 10px solid #ccc;\n",
    "  margin: .5em 10px;\n",
    "  padding: 30em, 10px;\n",
    "  quotes: \"\\201C\"\"\\201D\"\"\\2018\"\"\\2019\";\n",
    "  padding: 10px 20px;\n",
    "  line-height: 1.4;\n",
    "}\n",
    "\n",
    "blockquote:before {\n",
    "  content: open-quote;\n",
    "  display: inline;\n",
    "  height: 0;\n",
    "  line-height: 0;\n",
    "  left: -10px;\n",
    "  position: relative;\n",
    "  top: 30px;\n",
    "  bottom:30px;\n",
    "  color: #ccc;\n",
    "  font-size: 3em;\n",
    "    display:none;\n",
    "\n",
    "}\n",
    "\n",
    "p{\n",
    "  margin: 0;\n",
    "}\n",
    "\n",
    "footer{\n",
    "  margin:0;\n",
    "  text-align: right;\n",
    "  font-size: 1em;\n",
    "  font-style: italic;\n",
    "}\n",
    "</style>\n",
    "<blockquote><p class='quotation'><b><br><span style='font-size:25px'>PySparkling</span></b> <br><br>PySparkling Water is an awesome H2O extension that allows you to run H2O clusters on top of existing Spark clusters. With Splice Machine, this integration is taken care of for you, so it's simple to start modeling with your new favorite library</i></br><footer>Splice Machine</footer></blockquote><br>\n",
    "<blockquote><p class='quotation'><b><br><span style='font-size:25px'>MLManager (+MLFlow)</span></b><br><br>As a data scientist constantly creating new models and testing new features, it is necessary to effectively track and manage those different ML runs. MLManager + MLFlow allows you to track entire <code>experiments</code> and individual <code>run</code> parameters and metrics. The way you organize your flow is unique to you, and the intuitive Python API allows you to organize your development process and run with it.<br>\n",
    "     <center><img class='log' src='https://s3.amazonaws.com/splice-demo/mlflow+ui.png' width='40%' style='z-index:5'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's get started\n",
    "## In this notebook, we will see how to use Spark, H2O and MLManager to predict sentiment analysis of Amazon reviews, tracking everything in the [MLFlow UI](/mlflow) and deploy our models to production\n",
    "This is an adaptation of the original [H2O Demo](http://docs.h2o.ai/h2o-tutorials/latest-stable/h2o-world-2017/nlp/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important imports and setup\n",
    "* Create our Spark Session\n",
    "* Create our Native Spark Data Source\n",
    "* Create our PySparkling Water cluster\n",
    "* Import our MLManager functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://www.github.com/splicemachine/pysplice@DBAAS-3990\n",
      "  Cloning https://www.github.com/splicemachine/pysplice (to revision DBAAS-3990) to /tmp/pip-req-build-tvecgxkt\n",
      "  Running command git clone -q https://www.github.com/splicemachine/pysplice /tmp/pip-req-build-tvecgxkt\n",
      "  Running command git checkout -b DBAAS-3990 --track origin/DBAAS-3990\n",
      "  Switched to a new branch 'DBAAS-3990'\n",
      "  Branch 'DBAAS-3990' set up to track remote branch 'DBAAS-3990' from 'origin'.\n",
      "Requirement already satisfied, skipping upgrade: py4j==0.10.8.1 in /opt/conda/lib/python3.7/site-packages (from splicemachine==2.0.0) (0.10.8.1)\n",
      "Collecting pytest==5.1.3\n",
      "  Downloading pytest-5.1.3-py3-none-any.whl (224 kB)\n",
      "     |████████████████████████████████| 224 kB 17.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: mlflow==1.6.0 in /opt/conda/lib/python3.7/site-packages (from splicemachine==2.0.0) (1.6.0)\n",
      "Requirement already satisfied, skipping upgrade: mleap==0.15.0 in /opt/conda/lib/python3.7/site-packages (from splicemachine==2.0.0) (0.15.0)\n",
      "Requirement already satisfied, skipping upgrade: graphviz==0.13 in /opt/conda/lib/python3.7/site-packages (from splicemachine==2.0.0) (0.13)\n",
      "Requirement already satisfied, skipping upgrade: requests in /opt/conda/lib/python3.7/site-packages (from splicemachine==2.0.0) (2.23.0)\n",
      "Requirement already satisfied, skipping upgrade: gorilla==0.3.0 in /opt/conda/lib/python3.7/site-packages (from splicemachine==2.0.0) (0.3.0)\n",
      "Requirement already satisfied, skipping upgrade: tqdm==4.43.0 in /opt/conda/lib/python3.7/site-packages (from splicemachine==2.0.0) (4.43.0)\n",
      "Requirement already satisfied, skipping upgrade: pyspark-dist-explore==0.1.8 in /opt/conda/lib/python3.7/site-packages (from splicemachine==2.0.0) (0.1.8)\n",
      "Requirement already satisfied, skipping upgrade: numpy==1.18.2 in /opt/conda/lib/python3.7/site-packages (from splicemachine==2.0.0) (1.18.2)\n",
      "Requirement already satisfied, skipping upgrade: pandas==1.0.3 in /opt/conda/lib/python3.7/site-packages (from splicemachine==2.0.0) (1.0.3)\n",
      "Requirement already satisfied, skipping upgrade: scipy==1.4.1 in /opt/conda/lib/python3.7/site-packages (from splicemachine==2.0.0) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from pytest==5.1.3->splicemachine==2.0.0) (19.3.0)\n",
      "Requirement already satisfied, skipping upgrade: packaging in /opt/conda/lib/python3.7/site-packages (from pytest==5.1.3->splicemachine==2.0.0) (20.4)\n",
      "Requirement already satisfied, skipping upgrade: wcwidth in /opt/conda/lib/python3.7/site-packages (from pytest==5.1.3->splicemachine==2.0.0) (0.1.9)\n",
      "Requirement already satisfied, skipping upgrade: atomicwrites>=1.0 in /opt/conda/lib/python3.7/site-packages (from pytest==5.1.3->splicemachine==2.0.0) (1.4.0)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.12; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from pytest==5.1.3->splicemachine==2.0.0) (1.6.0)\n",
      "Requirement already satisfied, skipping upgrade: py>=1.5.0 in /opt/conda/lib/python3.7/site-packages (from pytest==5.1.3->splicemachine==2.0.0) (1.8.1)\n",
      "Requirement already satisfied, skipping upgrade: pluggy<1.0,>=0.12 in /opt/conda/lib/python3.7/site-packages (from pytest==5.1.3->splicemachine==2.0.0) (0.13.1)\n",
      "Requirement already satisfied, skipping upgrade: more-itertools>=4.0.0 in /opt/conda/lib/python3.7/site-packages (from pytest==5.1.3->splicemachine==2.0.0) (8.3.0)\n",
      "Requirement already satisfied, skipping upgrade: prometheus-flask-exporter in /opt/conda/lib/python3.7/site-packages (from mlflow==1.6.0->splicemachine==2.0.0) (0.13.0)\n",
      "Requirement already satisfied, skipping upgrade: gunicorn; platform_system != \"Windows\" in /opt/conda/lib/python3.7/site-packages (from mlflow==1.6.0->splicemachine==2.0.0) (20.0.4)\n",
      "Requirement already satisfied, skipping upgrade: click>=7.0 in /opt/conda/lib/python3.7/site-packages (from mlflow==1.6.0->splicemachine==2.0.0) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: gitpython>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from mlflow==1.6.0->splicemachine==2.0.0) (3.1.2)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.6.0 in /opt/conda/lib/python3.7/site-packages (from mlflow==1.6.0->splicemachine==2.0.0) (3.9.1)\n",
      "Requirement already satisfied, skipping upgrade: alembic in /opt/conda/lib/python3.7/site-packages (from mlflow==1.6.0->splicemachine==2.0.0) (1.4.2)\n",
      "Requirement already satisfied, skipping upgrade: entrypoints in /opt/conda/lib/python3.7/site-packages (from mlflow==1.6.0->splicemachine==2.0.0) (0.3)\n",
      "Requirement already satisfied, skipping upgrade: sqlparse in /opt/conda/lib/python3.7/site-packages (from mlflow==1.6.0->splicemachine==2.0.0) (0.3.1)\n",
      "Requirement already satisfied, skipping upgrade: querystring-parser in /opt/conda/lib/python3.7/site-packages (from mlflow==1.6.0->splicemachine==2.0.0) (1.2.4)\n",
      "Requirement already satisfied, skipping upgrade: sqlalchemy in /opt/conda/lib/python3.7/site-packages (from mlflow==1.6.0->splicemachine==2.0.0) (1.3.8)\n",
      "Requirement already satisfied, skipping upgrade: simplejson in /opt/conda/lib/python3.7/site-packages (from mlflow==1.6.0->splicemachine==2.0.0) (3.17.0)\n",
      "Requirement already satisfied, skipping upgrade: databricks-cli>=0.8.7 in /opt/conda/lib/python3.7/site-packages (from mlflow==1.6.0->splicemachine==2.0.0) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: docker>=4.0.0 in /opt/conda/lib/python3.7/site-packages (from mlflow==1.6.0->splicemachine==2.0.0) (4.2.0)\n",
      "Requirement already satisfied, skipping upgrade: Flask in /opt/conda/lib/python3.7/site-packages (from mlflow==1.6.0->splicemachine==2.0.0) (1.1.2)\n",
      "Requirement already satisfied, skipping upgrade: cloudpickle in /opt/conda/lib/python3.7/site-packages (from mlflow==1.6.0->splicemachine==2.0.0) (1.2.2)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil in /opt/conda/lib/python3.7/site-packages (from mlflow==1.6.0->splicemachine==2.0.0) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml in /opt/conda/lib/python3.7/site-packages (from mlflow==1.6.0->splicemachine==2.0.0) (5.3.1)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from mlflow==1.6.0->splicemachine==2.0.0) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn>=0.18.dev0 in /opt/conda/lib/python3.7/site-packages (from mleap==0.15.0->splicemachine==2.0.0) (0.21.3)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->splicemachine==2.0.0) (2020.4.5.1)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->splicemachine==2.0.0) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->splicemachine==2.0.0) (2.9)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->splicemachine==2.0.0) (1.25.9)\n",
      "Requirement already satisfied, skipping upgrade: matplotlib in /opt/conda/lib/python3.7/site-packages (from pyspark-dist-explore==0.1.8->splicemachine==2.0.0) (3.1.1)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas==1.0.3->splicemachine==2.0.0) (2020.1)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->pytest==5.1.3->splicemachine==2.0.0) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest==5.1.3->splicemachine==2.0.0) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: prometheus-client in /opt/conda/lib/python3.7/site-packages (from prometheus-flask-exporter->mlflow==1.6.0->splicemachine==2.0.0) (0.7.1)\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=3.0 in /opt/conda/lib/python3.7/site-packages (from gunicorn; platform_system != \"Windows\"->mlflow==1.6.0->splicemachine==2.0.0) (46.4.0.post20200518)\n",
      "Requirement already satisfied, skipping upgrade: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.7/site-packages (from gitpython>=2.1.0->mlflow==1.6.0->splicemachine==2.0.0) (4.0.5)\n",
      "Requirement already satisfied, skipping upgrade: python-editor>=0.3 in /opt/conda/lib/python3.7/site-packages (from alembic->mlflow==1.6.0->splicemachine==2.0.0) (1.0.4)\n",
      "Requirement already satisfied, skipping upgrade: Mako in /opt/conda/lib/python3.7/site-packages (from alembic->mlflow==1.6.0->splicemachine==2.0.0) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: tabulate>=0.7.7 in /opt/conda/lib/python3.7/site-packages (from databricks-cli>=0.8.7->mlflow==1.6.0->splicemachine==2.0.0) (0.8.7)\n",
      "Requirement already satisfied, skipping upgrade: configparser>=0.3.5 in /opt/conda/lib/python3.7/site-packages (from databricks-cli>=0.8.7->mlflow==1.6.0->splicemachine==2.0.0) (5.0.0)\n",
      "Requirement already satisfied, skipping upgrade: websocket-client>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from docker>=4.0.0->mlflow==1.6.0->splicemachine==2.0.0) (0.57.0)\n",
      "Requirement already satisfied, skipping upgrade: Jinja2>=2.10.1 in /opt/conda/lib/python3.7/site-packages (from Flask->mlflow==1.6.0->splicemachine==2.0.0) (2.11.2)\n",
      "Requirement already satisfied, skipping upgrade: itsdangerous>=0.24 in /opt/conda/lib/python3.7/site-packages (from Flask->mlflow==1.6.0->splicemachine==2.0.0) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: Werkzeug>=0.15 in /opt/conda/lib/python3.7/site-packages (from Flask->mlflow==1.6.0->splicemachine==2.0.0) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.18.dev0->mleap==0.15.0->splicemachine==2.0.0) (0.15.1)\n",
      "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->pyspark-dist-explore==0.1.8->splicemachine==2.0.0) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->pyspark-dist-explore==0.1.8->splicemachine==2.0.0) (1.2.0)\n",
      "Requirement already satisfied, skipping upgrade: smmap<4,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->gitpython>=2.1.0->mlflow==1.6.0->splicemachine==2.0.0) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.7/site-packages (from Mako->alembic->mlflow==1.6.0->splicemachine==2.0.0) (1.1.1)\n",
      "Building wheels for collected packages: splicemachine\n",
      "  Building wheel for splicemachine (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for splicemachine: filename=splicemachine-2.0.0-py3-none-any.whl size=54452 sha256=52b264eebf01c0a06154c7f5afe21982440aab1f7b0e37a04956721096c26a9e\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-e1e5ssi8/wheels/0c/58/ec/d4dd5d3e14310dcd82c132198196b6973a4139393dee4ddd79\n",
      "Successfully built splicemachine\n",
      "Installing collected packages: pytest, splicemachine\n",
      "  Attempting uninstall: pytest\n",
      "    Found existing installation: pytest 5.4.2\n",
      "    Uninstalling pytest-5.4.2:\n",
      "      Successfully uninstalled pytest-5.4.2\n",
      "  Attempting uninstall: splicemachine\n",
      "    Found existing installation: splicemachine 2.0.0\n",
      "    Uninstalling splicemachine-2.0.0:\n",
      "      Successfully uninstalled splicemachine-2.0.0\n",
      "Successfully installed pytest-5.1.3 splicemachine-2.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade git+https://www.github.com/splicemachine/pysplice@DBAAS-3990"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to H2O server at http://10.128.29.75:54321 ... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>12 secs</td></tr>\n",
       "<tr><td>H2O cluster timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O data parsing timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.28.1.2</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>2 months and 15 days </td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>sparkling-water-jovyan_spark-application-1591024739826</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>2</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>9.58 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>32</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>10</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://10.128.29.75:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>XGBoost, Algos, Amazon S3, Sparkling Water REST API Extensions, AutoML, Core V3, TargetEncoder, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.7.6 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  -------------------------------------------------------------------------------------------------------\n",
       "H2O cluster uptime:         12 secs\n",
       "H2O cluster timezone:       UTC\n",
       "H2O data parsing timezone:  UTC\n",
       "H2O cluster version:        3.28.1.2\n",
       "H2O cluster version age:    2 months and 15 days\n",
       "H2O cluster name:           sparkling-water-jovyan_spark-application-1591024739826\n",
       "H2O cluster total nodes:    2\n",
       "H2O cluster free memory:    9.58 Gb\n",
       "H2O cluster total cores:    32\n",
       "H2O cluster allowed cores:  10\n",
       "H2O cluster status:         locked, healthy\n",
       "H2O connection url:         http://10.128.29.75:54321\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         XGBoost, Algos, Amazon S3, Sparkling Water REST API Extensions, AutoML, Core V3, TargetEncoder, Core V4\n",
       "Python version:             3.7.6 final\n",
       "--------------------------  -------------------------------------------------------------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sparkling Water Context:\n",
      " * Sparkling Water Version: 3.28.1.2-1-2.4\n",
      " * H2O name: sparkling-water-jovyan_spark-application-1591024739826\n",
      " * cluster size: 2\n",
      " * list of used nodes:\n",
      "  (executorId, host, port)\n",
      "  ------------------------\n",
      "  (2,10.128.29.196,54321)\n",
      "  (1,10.128.24.122,54321)\n",
      "  ------------------------\n",
      "\n",
      "  Open H2O Flow in browser: http://10.128.29.75:54321 (CMD + click in Mac OSX)\n",
      "\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from splicemachine.spark import PySpliceContext\n",
    "from splicemachine.mlflow_support import *\n",
    "from pysparkling import *\n",
    "import h2o\n",
    "\n",
    "# Spark Session\n",
    "spark = SparkSession.builder.config('spark.executor.memoryOverhead',1000).config('spark.executor.memory','5g').getOrCreate()\n",
    "# Native Spark Data Source\n",
    "splice = PySpliceContext(spark)\n",
    "# Register Splice so we can access database functions\n",
    "mlflow.register_splice_context(splice)\n",
    "# Create H2O Cluster\n",
    "conf = H2OConf().setInternalClusterMode()\n",
    "hc = H2OContext.getOrCreate(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Great! Now let's import our data\n",
    "<style>\n",
    "blockquote{\n",
    "  font-size: 15px;\n",
    "  background: #f9f9f9;\n",
    "  border-left: 10px solid #ccc;\n",
    "  margin: .5em 10px;\n",
    "  padding: 30em, 10px;\n",
    "  quotes: \"\\201C\"\"\\201D\"\"\\2018\"\"\\2019\";\n",
    "  padding: 10px 20px;\n",
    "  line-height: 1.4;\n",
    "}\n",
    "\n",
    "blockquote:before {\n",
    "  content: open-quote;\n",
    "  display: inline;\n",
    "  height: 0;\n",
    "  line-height: 0;\n",
    "  left: -10px;\n",
    "  position: relative;\n",
    "  top: 30px;\n",
    "  bottom:30px;\n",
    "  color: #ccc;\n",
    "  font-size: 3em;\n",
    "    display:none;\n",
    "\n",
    "}\n",
    "\n",
    "p{\n",
    "  margin: 0;\n",
    "}\n",
    "\n",
    "footer{\n",
    "  margin:0;\n",
    "  text-align: right;\n",
    "  font-size: 1em;\n",
    "  font-style: italic;\n",
    "}\n",
    "</style>\n",
    "<blockquote><p class='quotation'><b><br><span style='font-size:25px'>Importing Data</span></b> <br><br>There are a few easy ways to get data into Splice Machine, and we'll demonstrate 2 of them here. You can use the built-in <code>%%sql</code> magic to import data directly from external sources, such as S3, or you can use H2O to directly read the data from S3, create a table from that dataframe, and insert the data directly using the <code>PySpliceContext</code> you created in the cell above. </i></br><footer>Splice Machine</footer></blockquote><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Direct Import from SQL\n",
    "<style>\n",
    "blockquote{\n",
    "  font-size: 15px;\n",
    "  background: #f9f9f9;\n",
    "  border-left: 10px solid #ccc;\n",
    "  margin: .5em 10px;\n",
    "  padding: 30em, 10px;\n",
    "  quotes: \"\\201C\"\"\\201D\"\"\\2018\"\"\\2019\";\n",
    "  padding: 10px 20px;\n",
    "  line-height: 1.4;\n",
    "}\n",
    "\n",
    "blockquote:before {\n",
    "  content: open-quote;\n",
    "  display: inline;\n",
    "  height: 0;\n",
    "  line-height: 0;\n",
    "  left: -10px;\n",
    "  position: relative;\n",
    "  top: 30px;\n",
    "  bottom:30px;\n",
    "  color: #ccc;\n",
    "  font-size: 3em;\n",
    "    display:none;\n",
    "\n",
    "}\n",
    "\n",
    "p{\n",
    "  margin: 0;\n",
    "}\n",
    "\n",
    "footer{\n",
    "  margin:0;\n",
    "  text-align: right;\n",
    "  font-size: 1em;\n",
    "  font-style: italic;\n",
    "}\n",
    "</style>\n",
    "<blockquote><p class='quotation'><b><br><span style='font-size:25px'>SQL Import</span></b> <br><br>This method is simple: Create your table, point it to a an S3 location, and run the import command</i></br><footer>Splice Machine</footer></blockquote><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19fa64d3-688a-4173-a492-05c204685bc8",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa097b22-8244-4604-988f-2054c4092360",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "922ad222-aa73-416b-aa94-4153fa231847",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%sql\n",
    "DROP TABLE IF EXISTS AMAZON_REVIEWS;\n",
    "CREATE TABLE AMAZON_REVIEWS(\n",
    "    PRODUCTID VARCHAR(250),\n",
    "    USERID VARCHAR(250),\n",
    "    SUMMARY VARCHAR(500),\n",
    "    SCORE INT,\n",
    "    HELPFULNESSDENOMINATOR BIGINT,\n",
    "    ID INT,\n",
    "    PROFILENAME VARCHAR(500),\n",
    "    HELPFULNESSNUMERATOR BIGINT,\n",
    "    REVIEW_TIME BIGINT,\n",
    "    REVIEW VARCHAR(15000)\n",
    ");\n",
    "\n",
    "\n",
    "-- Import the data\n",
    "call SYSCS_UTIL.IMPORT_DATA (\n",
    "     null,\n",
    "     'AMAZON_REVIEWS',\n",
    "     null,\n",
    "     's3a://splice-demo/AmazonReviews.csv',\n",
    "     ',',\n",
    "     null,\n",
    "     null,\n",
    "     null,\n",
    "     null,\n",
    "     -1,\n",
    "     's3a://splice-demo/bad',\n",
    "     null, \n",
    "     null);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "718a61f5-1d7a-4155-8a2b-0feb8edce079",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%sql\n",
    "select top 10 * from AMAZON_REVIEWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows:99999\n",
      "Cols:10\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>       </th><th>PRODUCTID  </th><th>USERID        </th><th>SUMMARY                                             </th><th>SCORE             </th><th>HELPFULNESSDENOMINATOR  </th><th>ID               </th><th>PROFILENAME              </th><th>HELPFULNESSNUMERATOR  </th><th>REVIEW_TIME       </th><th>REVIEW                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>type   </td><td>string     </td><td>string        </td><td>string                                              </td><td>int               </td><td>int                     </td><td>int              </td><td>string                   </td><td>int                   </td><td>int               </td><td>string                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                </td></tr>\n",
       "<tr><td>mins   </td><td>NaN        </td><td>NaN           </td><td>NaN                                                 </td><td>1.0               </td><td>0.0                     </td><td>3.0              </td><td>NaN                      </td><td>0.0                   </td><td>940809600.0       </td><td>NaN                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   </td></tr>\n",
       "<tr><td>mean   </td><td>NaN        </td><td>NaN           </td><td>NaN                                                 </td><td>4.186101861018614 </td><td>2.236472364723653       </td><td>284618.2923129232</td><td>NaN                      </td><td>1.745467454674545     </td><td>1296171870.3267026</td><td>NaN                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   </td></tr>\n",
       "<tr><td>maxs   </td><td>NaN        </td><td>NaN           </td><td>NaN                                                 </td><td>5.0               </td><td>878.0                   </td><td>568436.0         </td><td>NaN                      </td><td>866.0                 </td><td>1351209600.0      </td><td>NaN                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   </td></tr>\n",
       "<tr><td>sigma  </td><td>NaN        </td><td>NaN           </td><td>NaN                                                 </td><td>1.3095421878195728</td><td>8.805400733729007       </td><td>164159.3591659328</td><td>NaN                      </td><td>8.17145025581287      </td><td>48107386.40835985 </td><td>NaN                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   </td></tr>\n",
       "<tr><td>zeros  </td><td>0          </td><td>0             </td><td>0                                                   </td><td>0                 </td><td>47593                   </td><td>0                </td><td>0                        </td><td>53553                 </td><td>0                 </td><td>0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     </td></tr>\n",
       "<tr><td>missing</td><td>0          </td><td>0             </td><td>0                                                   </td><td>0                 </td><td>0                       </td><td>0                </td><td>0                        </td><td>0                     </td><td>0                 </td><td>0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     </td></tr>\n",
       "<tr><td>0      </td><td>B002915VF2 </td><td>A70T7QXN23ZXY </td><td>Fabulous healthy treat that tastes pretty darn good!</td><td>5.0               </td><td>3.0                     </td><td>164348.0         </td><td>Rachel Zaitz             </td><td>2.0                   </td><td>1248825600.0      </td><td>I was a little skeptical about this product before I purchased it. However, I decided to try it out due to the positive reviews.  I&#x27;m glad I did.  For the unbelievably low calorie count for 1 pint of a frosty treat, this is a must buy!  Is it just like ice cream? No.  Is it a good substitute for something cold, relatively creamy and pleasant tasting? YES.  I still don&#x27;t know how they pack only 128 calories into this magical treat, but I guess I won&#x27;t ask too many questions and instead just enjoy it while I have it.  It is pricey and I would love to see the price drop a bit for repeat customers of this product perhaps, but overall I am very impressed.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </td></tr>\n",
       "<tr><td>1      </td><td>B0038ZS6PU </td><td>A2UMUPYH9RTZKL</td><td>Love,Love,Love This Flour                           </td><td>5.0               </td><td>1.0                     </td><td>546392.0         </td><td>PiperL                   </td><td>1.0                   </td><td>1325808000.0      </td><td>I made no knead bread with this flour. It turned out fabulous. The recipe was a wet dough and I found the dough easy to handle and the taste developed over a 24 hr. fermentation period which is pretty fast. This flour is my new best friend.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      </td></tr>\n",
       "<tr><td>2      </td><td>B0039LVLS2 </td><td>A1IX2EV3DOHTLG</td><td>Yummy snack                                         </td><td>5.0               </td><td>1.0                     </td><td>213810.0         </td><td>Nancy Czajkowski         </td><td>1.0                   </td><td>1337126400.0      </td><td>Very satisfying snack for very little calories.  I originally tried the larger bag, until I found this size,perfect!  Just grab and go and you know you have a single serving.  Really fast delivery!! Give it a try.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 </td></tr>\n",
       "<tr><td>3      </td><td>B000LQORIY </td><td>A3VNSULAUIV2JS</td><td>Not bad                                             </td><td>4.0               </td><td>0.0                     </td><td>144312.0         </td><td>Jenny :)                 </td><td>0.0                   </td><td>1289347200.0      </td><td>I bought this because I was curious about what a lobster flavored noodle would actually taste like. I figured it&#x27;d probably taste like spicy seafood, which is pretty good, so I gave it a go.&lt;br /&gt;&lt;br /&gt;It&#x27;s not very spicy, especially compared to all the other spicy flavors Nong Shim offers. The original flavor is super spicy compared to this. I also really like the &quot;hot and spicy&quot; flavor, which is one of my staples (and favorites). The &quot;kimchi&quot; flavor is better than this (and it&#x27;s also slightly spicier). I also have the &quot;neoguri spicy seafood&quot; flavor which is another one of my favorites and much better than this as well. Even the regular Nong Shim &quot;seafood&quot; flavor (that&#x27;s not labeled spicy) is spicier than this for some reason. I was a college student once and ate lots of Nong Shim noodles (I still do, but not as much)....&lt;br /&gt;&lt;br /&gt;Basically, this was kind of a disappointment based on my expectations from the name of the product. It&#x27;s not bad, but it&#x27;s one of the less tasty flavors. It&#x27;s nothing special or distinctive. The ingredients actually listed crab flavoring but not lobster flavoring, which I don&#x27;t really care about as long as it tastes good. I still give this 4 stars because it&#x27;s better than many many instant noodles from other companies. Nong Shim tends to get it right and this noodle pales only in comparison to other Nong Shim flavors.                                                                                                                                                                                                                                                                                                                                                                                                                                             </td></tr>\n",
       "<tr><td>4      </td><td>B002FYJW8U </td><td>AMZ6GGXWARAET </td><td>Leaking containers                                  </td><td>3.0               </td><td>2.0                     </td><td>118402.0         </td><td>Erin Barista             </td><td>2.0                   </td><td>1308528000.0      </td><td>While I love this product, this is the second time we have received a leaking box of soy milk in the case of 12 boxes.  So 1 of the 12 containers was no good from the beginning.  In addition, the leaking containers caused other containers to get soft on the bottom and pour everywhere.  It was a mess and all that soy milk was ruined.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </td></tr>\n",
       "<tr><td>5      </td><td>B0009YF43K </td><td>AYKG9PJK91VG1 </td><td>This is my Golden&#x27;s favorite everlasting flavor     </td><td>5.0               </td><td>0.0                     </td><td>243196.0         </td><td>D. Taylor &quot;coffee nut&quot;   </td><td>0.0                   </td><td>1231200000.0      </td><td>They quickly pop it out of the large everlasting treat ball, and still remain entertained by it for some time.  I&#x27;ve never had any abdominal problems from this product, like you can get from rawhides, bones, and especially the Greenies (which dogs cannot dissolve and so can create abdominal obstructions).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </td></tr>\n",
       "<tr><td>6      </td><td>B000E283KS </td><td>A109WE4H6C9O2X</td><td>Just like when I was a kid                          </td><td>5.0               </td><td>1.0                     </td><td>521267.0         </td><td>R. Workman &quot;copter1&quot;     </td><td>1.0                   </td><td>1303689600.0      </td><td>Deee-licious.  Hard to find the unfrosted Pop-Tarts in stores these days, so this is a nice treat.  And they stay fresh, too.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         </td></tr>\n",
       "<tr><td>7      </td><td>B002ANDYAS </td><td>ADO2DIL3MAS5W </td><td>Nutritious for my Pregnant Doggie :}                </td><td>5.0               </td><td>3.0                     </td><td>246236.0         </td><td>Jordan &quot;Believe The Hype&quot;</td><td>3.0                   </td><td>1301788800.0      </td><td>I found this at a new pet store in Encino CA, after eating at Tony Roma&#x27;s. I was looking for wet food that was very nutritious, made of lamb, beef or liver (I heard a lot of dogs and cats are allergic to chicken by products - but people don&#x27;t really know about it.&lt;br /&gt;I was looking for &#x27;puppy&#x27; food for my pregnant dog.&lt;br /&gt;I found this Lamb wet food. It said it was complete natural dog food for both puppies and adult dogs. So I tried it.&lt;br /&gt;&lt;br /&gt;I checked the ingredients for the Lamb formula, and it has NO;&lt;br /&gt;* Meat by-products (ground/rendered/cleaned parts of necks/feet/undeveloped eggs/intestines - parts that are generally less nutritious and are of lesser quality)&lt;br /&gt;* Chemicals&lt;br /&gt;* Fillers&lt;br /&gt;* CORN/Corn-meal (all dog/cat foods seem to be made of) Who wants to feed their animal Corn, to get them full?&lt;br /&gt;* Meat substitutes&lt;br /&gt;* CHICKEN MEAL (consists of chicken-flesh and skin without bone, feathers, organs, etc). Would you buy a chicken and strip the skin off it to feed to your pet?&lt;br /&gt;* Artificial preservatives(BHA/BHT, ethoxyquin, propyl gallate) and MANY more!&lt;br /&gt;* No Artificial Preservatives, Flavors, Colors or Bleached Ingredients.&lt;br /&gt;&lt;br /&gt;It does have Lamb Meal, which isn&#x27;t something I like.&lt;br /&gt;Although all around nutritious, I&#x27;m going to try &#x27;Wellness Canned Dog Food for Puppy, 12pk of 12.5oz Cans.&#x27;&lt;br /&gt;- Wellness can seems to have none of the above, and no chicken or lamb meal. It&#x27;s cheaper too. A plus for me!&lt;br /&gt;&lt;br /&gt;ALWAYS read ingredients before purchasing cat or dog food. Even if the first ingredient is &#x27;meat by product&#x27;, or &#x27;chicken meal&#x27;, doesn&#x27;t mean it&#x27;s actual meat and nutritious for your pet. (It&#x27;s the equivalent of a human eating imitation powdered mashed potato&#x27;s instead of the real thing).&lt;br /&gt;&lt;br /&gt;Good luck!</td></tr>\n",
       "<tr><td>8      </td><td>B000EGR85A </td><td>A19HJW4VFHB02C</td><td>Yummy Yummy Cookies!                                </td><td>4.0               </td><td>1.0                     </td><td>350415.0         </td><td>A. Clinton &quot;lucywench&quot;   </td><td>1.0                   </td><td>1320710400.0      </td><td>My mother suffers from digestion issues and these cookies are one of the few things she can eat to settle her stomach.  They have been a Godsend for her.  Highly recommend this product!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             </td></tr>\n",
       "<tr><td>9      </td><td>B005A1LJ04 </td><td>A3A90CELVLOUP6</td><td>Didn&#x27;t like this at all                             </td><td>2.0               </td><td>0.0                     </td><td>339426.0         </td><td>Nanciejeanne &quot;Enjay&quot;     </td><td>0.0                   </td><td>1336003200.0      </td><td>I really love margaritas, so I was thrilled when there was a nearly-zero calorie version of the flavor. Except, at least to me, it tasted really artificial. It smelled artificial. It even looked artificial. Almost neon green. I was hoping I didn&#x27;t turn into the hulk or a mutant ninja turtle. Now, I like crystal light (pink Lemonade is my favorite), and drink &quot;diet&quot; sodas, teas, etc. so I don&#x27;t mean to trash all low-calorie drinks. This was just odd. My daughter (who is 12) also like margarita MIX, she always orders a virgin one when we go to Mexican restaurants. And she wouldn&#x27;t drink the container of this either. We both agreed that it wasn&#x27;t terrible or anything, just not good.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get data from table into Spark Dataframe\n",
    "df2 = splice.df('select * from ben.amazon_reviews')\n",
    "hdf = hc.asH2OFrame(df2)\n",
    "hdf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: Import from H2O\n",
    "<style>\n",
    "blockquote{\n",
    "  font-size: 15px;\n",
    "  background: #f9f9f9;\n",
    "  border-left: 10px solid #ccc;\n",
    "  margin: .5em 10px;\n",
    "  padding: 30em, 10px;\n",
    "  quotes: \"\\201C\"\"\\201D\"\"\\2018\"\"\\2019\";\n",
    "  padding: 10px 20px;\n",
    "  line-height: 1.4;\n",
    "}\n",
    "\n",
    "blockquote:before {\n",
    "  content: open-quote;\n",
    "  display: inline;\n",
    "  height: 0;\n",
    "  line-height: 0;\n",
    "  left: -10px;\n",
    "  position: relative;\n",
    "  top: 30px;\n",
    "  bottom:30px;\n",
    "  color: #ccc;\n",
    "  font-size: 3em;\n",
    "    display:none;\n",
    "\n",
    "}\n",
    "\n",
    "p{\n",
    "  margin: 0;\n",
    "}\n",
    "\n",
    "footer{\n",
    "  margin:0;\n",
    "  text-align: right;\n",
    "  font-size: 1em;\n",
    "  font-style: italic;\n",
    "}\n",
    "</style>\n",
    "<blockquote><p class='quotation'><b><br><span style='font-size:25px'>H2O Import</span></b> <br><br>This method is also straightforward, and may be preferable to Data Scientists: Import your data using H2O, and then use the <code>PySpliceContext</code> to create the table from the dataframe and insert the data directly</i></br><footer>Splice Machine</footer></blockquote><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>ProductId  </th><th>UserId        </th><th>Summary                               </th><th style=\"text-align: right;\">  Score</th><th style=\"text-align: right;\">  HelpfulnessDenominator</th><th style=\"text-align: right;\">    Id</th><th>ProfileName                                     </th><th style=\"text-align: right;\">  HelpfulnessNumerator</th><th style=\"text-align: right;\">       Time</th><th>Text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>B00141QYSQ </td><td>A1YS02UZZGRDCT</td><td>Do Not Buy                            </td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">                       2</td><td style=\"text-align: right;\"> 41471</td><td>Evan Eberhardt                                  </td><td style=\"text-align: right;\">                     2</td><td style=\"text-align: right;\">1.34836e+09</td><td>These are made in China (do not buy ANY pet food from China).  Dogswell has been using propylene glycol to soften their treats (what are they thinkng?).  Do not purchase or support this company in any way until they clean up their act.  And for whatever reason Amazon doesn&#x27;t allow returns of this item, so I had to toss mine out.  Bad business all around on this one.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            </td></tr>\n",
       "<tr><td>B0089SPEO2 </td><td>A3JOYNYL458QHP</td><td>Less lemon and less zing              </td><td style=\"text-align: right;\">      3</td><td style=\"text-align: right;\">                       0</td><td style=\"text-align: right;\"> 28582</td><td>coleridge                                       </td><td style=\"text-align: right;\">                     0</td><td style=\"text-align: right;\">1.32391e+09</td><td>Everything is ok, except it just isn&#x27;t as good as it is in the bags.  Just considerably more bland -- less lemon and less zing.  Boring.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </td></tr>\n",
       "<tr><td>B001PMCDK2 </td><td>A14TTMM0Z03Y2W</td><td>my cat goes crazy for these!          </td><td style=\"text-align: right;\">      5</td><td style=\"text-align: right;\">                       0</td><td style=\"text-align: right;\">389965</td><td>Lindsay S. Bradford                             </td><td style=\"text-align: right;\">                     0</td><td style=\"text-align: right;\">1.3106e+09 </td><td>Best cat treat ever. There isn&#x27;t anything comparable to the love my cat has for these treats, he snubs away any other kind now.&lt;br /&gt;I know he likes to manipulate me with his cattiness but these treats are my way of manipulating him to come sit on my lap and have some chill time. :)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 </td></tr>\n",
       "<tr><td>B002Q8JOSI </td><td>A17UQD2RSSQH5X</td><td>My dogs tell me these treats are YUMMY</td><td style=\"text-align: right;\">      5</td><td style=\"text-align: right;\">                       1</td><td style=\"text-align: right;\">212536</td><td>in the dark                                     </td><td style=\"text-align: right;\">                     1</td><td style=\"text-align: right;\">1.31613e+09</td><td>My two Corgis were thoroughly spoiled by my late husband (I spent a year and a half dieting them down a combined total of 25 pounds!)&lt;br /&gt;&lt;br /&gt;They are accustomed to the finest of fare, and they absolutely love the Wellness brand of treats.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          </td></tr>\n",
       "<tr><td>B00176G870 </td><td>A2F2MZW8EOGH5J</td><td>Yummy to the tummy                    </td><td style=\"text-align: right;\">      5</td><td style=\"text-align: right;\">                       0</td><td style=\"text-align: right;\">115971</td><td>daemoncycler &quot;When you arrive at a fork in th...</td><td style=\"text-align: right;\">                     0</td><td style=\"text-align: right;\">1.33479e+09</td><td>We used to have drive down to the specialty pet food store for this product.  So glad we discovered Amazon.  As far as I can tell it is no more expensive &amp; in some cases less - Prime membership is awesome.  Loving Pets treats are some of the best according to my dog.  They do not develop that nasty smell like some dog treats do.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  </td></tr>\n",
       "<tr><td>B001CHFUGY </td><td>A2M8VROSDPU4JT</td><td>Very good coffee                      </td><td style=\"text-align: right;\">      5</td><td style=\"text-align: right;\">                       1</td><td style=\"text-align: right;\">434484</td><td>Officefan &quot;Officefankt&quot;                         </td><td style=\"text-align: right;\">                     1</td><td style=\"text-align: right;\">1.27725e+09</td><td>I really liked this coffee, it was just as good as everyone claimed it was.  Strong, bold and flavorful!  I would recommend!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                </td></tr>\n",
       "<tr><td>B0041CIR62 </td><td>A16I6WJUEBJ1C3</td><td>okay but not as healthy as it appears </td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">                       1</td><td style=\"text-align: right;\">138997</td><td>doctorsirena &quot;doctorsirena&quot;                     </td><td style=\"text-align: right;\">                     1</td><td style=\"text-align: right;\">1.34369e+09</td><td>I am always looking for healthier, whole grain versions of foods I enjoy.  Unfortunately, these Peacock brand noodles are yet another food masquerading as healthy.  The product title in big letters on the front says &quot;Brown Rice Vermicelli&quot;, making the consumer think &quot;this is made with brown rice, so it should be a healthy choice&quot;.  But the first indication that it is not is when looking at the fiber content on the nutrition facts - only 0.6g per 2oz serving.  Then onto the ingredients list to see why so low... contains brown rice, sago starch and water.  The sago starch comes from palms and must not have much (if any) fiber.&lt;br /&gt;&lt;br /&gt;The Annie Chun&#x27;s Maifun Brown Rice Noodles (sold on Amazon and in my local healthy grocer) has become one of my staples and is my frame of reference when comparing to the Peacock brand.  The Annie Chun&#x27;s product is made with 100% whole grain, with ingredients brown rice flour and water.  Per 2oz serving, it has 4g fiber and pretty much the same calories and other nutrients as the Peacock brand.&lt;br /&gt;&lt;br /&gt;If you do try this Peacock brand noodles and have not used rice noodles before, you will need to seek guidance elsewhere on preparation.  As others have pointed out, the Peacock package gives almost no directions on how to prepare the product, aside from a brief mention in the recipes (in the header text it does say that they are &quot;easy-to-cook&quot; but does not say how).  It also contains a very strange recipe for rice noodles: Aglio Olio style - this is an Italian recipe for noodles with olive oil/garlic/sprinkled with grated cheese that I think would not be very tasty.  The second recipe appears to be for a soup with veggie strips.  Neither recipe gives amounts or much direction.  In comparison, the Annie Chun&#x27;s package gives clear, specific directions on rice noodle preparation and two recipes.&lt;br /&gt;&lt;br /&gt;I use rice noodles = maifun = rice sticks = sometimes called vermicelli for making the Vietnamese salad &quot;bun tofu&quot;, to serve with stir-fried veggies or in lettuce rolls.  They can also be used in spring rolls/egg rolls.  When cooking with thin rice noodles, be careful not to oversoak/overcook/overmix or they tend to disintegrate.  Asian rice noodle vermicelli (maifun) are not the same as Italian vermicelli and are not readily interchangeable.  If making an Italian recipe, the best results would be expected from Italian pasta and not maifun.&lt;br /&gt;&lt;br /&gt;A few final notes...  Both Peacock and Annie Chun&#x27;s brown rice noodles are gluten free.  The Peacock is made in Singapore and the Annie Chun&#x27;s in Thailand.  The Peacock noodles do taste fine (kind of bland), but so do the Annie Chun&#x27;s.  At this time, they are both approximately the same price.  Peacock come in an plastic bag with some noodle crushage upon shipping; Annie Chun&#x27;s are perfect upon removal from their cellophane bag in a box.  Overall, I highly recommend the Annie Chun&#x27;s Maifun as a healthier option over the Peacock brand.  On a related note, the Annie Chun&#x27;s soba and brown rice pad thai noodles are also excellent.&lt;br /&gt;&lt;br /&gt;Rating for this product: 2.5 stars rounded down to 2 stars.</td></tr>\n",
       "<tr><td>B001R3BQFW </td><td>AM50E42AFUVNL </td><td>Taste great.                          </td><td style=\"text-align: right;\">      5</td><td style=\"text-align: right;\">                       0</td><td style=\"text-align: right;\">126555</td><td>T. Higley &quot;Tina&quot;                                </td><td style=\"text-align: right;\">                     0</td><td style=\"text-align: right;\">1.32356e+09</td><td>I have tried many different drink mix, this is the best tasting by far. It does not have the after taste of the sweetener and I really like it, it is pretty strong, so I use a big water bottle (20 oz) for one tube, it still a little stronger than I like, but it is just my taste.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     </td></tr>\n",
       "<tr><td>B005HGAV8I </td><td>A2I5KDNOESGJ1H</td><td>variety galore                        </td><td style=\"text-align: right;\">      5</td><td style=\"text-align: right;\">                       1</td><td style=\"text-align: right;\">438837</td><td>TJ                                              </td><td style=\"text-align: right;\">                     1</td><td style=\"text-align: right;\">1.33402e+09</td><td>This is my favorite item to order for my Keurig. There are so many flavors, my finicky palate never gets bored!  The only downside is there are probably 5-6 decaf varieties.  I don&#x27;t drink decaf (I REQUIRE copious amounts of caffeine), so they sit on the shelf...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     </td></tr>\n",
       "<tr><td>B000GFYRHQ </td><td>A3A7YUR6FS6ZCI</td><td>Bigelow Earl Grey Green Tea           </td><td style=\"text-align: right;\">      5</td><td style=\"text-align: right;\">                       0</td><td style=\"text-align: right;\">245379</td><td>Tea Lover                                       </td><td style=\"text-align: right;\">                     0</td><td style=\"text-align: right;\">1.17841e+09</td><td>Tastes like Earl Grey, but it&#x27;s green tea so it&#x27;s healthier.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"https://splice-demo.s3.amazonaws.com/AmazonReviews.csv\"\n",
    "# Load data into H2O\n",
    "reviews = h2o.import_file(data_path)\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H2O offers great functions to convert H2OFrames into Pandas and Spark DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------------+-----+----------------------+------+--------------------+--------------------+----------+--------------------+\n",
      "| ProductId|        UserId|             Summary|Score|HelpfulnessDenominator|    Id|         ProfileName|HelpfulnessNumerator|      Time|                Text|\n",
      "+----------+--------------+--------------------+-----+----------------------+------+--------------------+--------------------+----------+--------------------+\n",
      "|B00141QYSQ|A1YS02UZZGRDCT|          Do Not Buy|    1|                     2| 41471|      Evan Eberhardt|                   2|1348358400|These are made in...|\n",
      "|B0089SPEO2|A3JOYNYL458QHP|Less lemon and le...|    3|                     0| 28582|           coleridge|                   0|1323907200|Everything is ok,...|\n",
      "|B001PMCDK2|A14TTMM0Z03Y2W|my cat goes crazy...|    5|                     0|389965| Lindsay S. Bradford|                   0|1310601600|Best cat treat ev...|\n",
      "|B002Q8JOSI|A17UQD2RSSQH5X|My dogs tell me t...|    5|                     1|212536|         in the dark|                   1|1316131200|My two Corgis wer...|\n",
      "|B00176G870|A2F2MZW8EOGH5J|  Yummy to the tummy|    5|                     0|115971|daemoncycler \"Whe...|                   0|1334793600|We used to have d...|\n",
      "|B001CHFUGY|A2M8VROSDPU4JT|    Very good coffee|    5|                     1|434484|Officefan \"Office...|                   1|1277251200|I really liked th...|\n",
      "|B0041CIR62|A16I6WJUEBJ1C3|okay but not as h...|    2|                     1|138997|doctorsirena \"doc...|                   1|1343692800|I am always looki...|\n",
      "|B001R3BQFW| AM50E42AFUVNL|        Taste great.|    5|                     0|126555|    T. Higley \"Tina\"|                   0|1323561600|I have tried many...|\n",
      "|B005HGAV8I|A2I5KDNOESGJ1H|      variety galore|    5|                     1|438837|                  TJ|                   1|1334016000|This is my favori...|\n",
      "|B000GFYRHQ|A3A7YUR6FS6ZCI|Bigelow Earl Grey...|    5|                     0|245379|           Tea Lover|                   0|1178409600|Tastes like Earl ...|\n",
      "|B001KUUNHE|A35R32TA60XD57|Best simple cat food|    5|                     0| 64358|            M. Torma|                   0|1338940800|My cat that can e...|\n",
      "|B000RHXLDO|A1P8ZVG6MJX9O1|Organic is Better...|    5|                     5| 75530|          Mark Emdee|                   4|1277164800|UPDATE: I now no ...|\n",
      "|B00013UQOG|A2AJNKK1S0W8F0|      Lots of Roses!|    4|                     1|312517|     James R. Kelley|                   1|1331337600|Much more roses t...|\n",
      "|B003E6COLK|A1S4RN19B0G9TQ|The best Gummy Be...|    5|                     0|444328|S. Moon \"Moons Girl\"|                   0|1313971200|I recommend these...|\n",
      "|B009PFJUF2|A16HJRHRHNSUZ6|Looks very pretty...|    5|                     1|286942|      Danielle Tietz|                   1|1350432000|My neighborhood r...|\n",
      "|B000VK08OC|A15DZG6P6YB869|Fresh and delicious!|    5|                     3|292099|            P. Banks|                   3|1313107200|We purchased this...|\n",
      "|B005K4Q4KG|A13L66J35SMYE5|            Not good|    1|                     2|243065|  Elizabeth Ramsoram|                   1|1330646400|This does not tas...|\n",
      "|B000LKZD4W|A2MQ5M9IJAXX6C|Great high-protei...|    5|                     0|152409|        Carol Mathis|                   0|1238025600|This is a great p...|\n",
      "|B00213ERI0|A20GEEXSF3DULO|Gluten Free yummi...|    5|                     0| 11177|  Vicki \"Noah's Mom\"|                   0|1274832000|to all of you glu...|\n",
      "|B0000D9MXL|A16OEO4JDUIP48|A fantastic Cheddar!|    5|                     2|213880|D. Kennedy \"zibed...|                   2|1234569600|The epitome of an...|\n",
      "+----------+--------------+--------------------+-----+----------------------+------+--------------------+--------------------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Score</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Id</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>Time</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B00141QYSQ</td>\n",
       "      <td>A1YS02UZZGRDCT</td>\n",
       "      <td>Do Not Buy</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>41471</td>\n",
       "      <td>Evan Eberhardt</td>\n",
       "      <td>2</td>\n",
       "      <td>1348358400</td>\n",
       "      <td>These are made in China (do not buy ANY pet fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B0089SPEO2</td>\n",
       "      <td>A3JOYNYL458QHP</td>\n",
       "      <td>Less lemon and less zing</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>28582</td>\n",
       "      <td>coleridge</td>\n",
       "      <td>0</td>\n",
       "      <td>1323907200</td>\n",
       "      <td>Everything is ok, except it just isn't as good...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B001PMCDK2</td>\n",
       "      <td>A14TTMM0Z03Y2W</td>\n",
       "      <td>my cat goes crazy for these!</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>389965</td>\n",
       "      <td>Lindsay S. Bradford</td>\n",
       "      <td>0</td>\n",
       "      <td>1310601600</td>\n",
       "      <td>Best cat treat ever. There isn't anything comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B002Q8JOSI</td>\n",
       "      <td>A17UQD2RSSQH5X</td>\n",
       "      <td>My dogs tell me these treats are YUMMY</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>212536</td>\n",
       "      <td>in the dark</td>\n",
       "      <td>1</td>\n",
       "      <td>1316131200</td>\n",
       "      <td>My two Corgis were thoroughly spoiled by my la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B00176G870</td>\n",
       "      <td>A2F2MZW8EOGH5J</td>\n",
       "      <td>Yummy to the tummy</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>115971</td>\n",
       "      <td>daemoncycler \"When you arrive at a fork in th...</td>\n",
       "      <td>0</td>\n",
       "      <td>1334793600</td>\n",
       "      <td>We used to have drive down to the specialty pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B001CHFUGY</td>\n",
       "      <td>A2M8VROSDPU4JT</td>\n",
       "      <td>Very good coffee</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>434484</td>\n",
       "      <td>Officefan \"Officefankt\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1277251200</td>\n",
       "      <td>I really liked this coffee, it was just as goo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>B0041CIR62</td>\n",
       "      <td>A16I6WJUEBJ1C3</td>\n",
       "      <td>okay but not as healthy as it appears</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>138997</td>\n",
       "      <td>doctorsirena \"doctorsirena\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1343692800</td>\n",
       "      <td>I am always looking for healthier, whole grain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>B001R3BQFW</td>\n",
       "      <td>AM50E42AFUVNL</td>\n",
       "      <td>Taste great.</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>126555</td>\n",
       "      <td>T. Higley \"Tina\"</td>\n",
       "      <td>0</td>\n",
       "      <td>1323561600</td>\n",
       "      <td>I have tried many different drink mix, this is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>B005HGAV8I</td>\n",
       "      <td>A2I5KDNOESGJ1H</td>\n",
       "      <td>variety galore</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>438837</td>\n",
       "      <td>TJ</td>\n",
       "      <td>1</td>\n",
       "      <td>1334016000</td>\n",
       "      <td>This is my favorite item to order for my Keuri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>B000GFYRHQ</td>\n",
       "      <td>A3A7YUR6FS6ZCI</td>\n",
       "      <td>Bigelow Earl Grey Green Tea</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>245379</td>\n",
       "      <td>Tea Lover</td>\n",
       "      <td>0</td>\n",
       "      <td>1178409600</td>\n",
       "      <td>Tastes like Earl Grey, but it's green tea so i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ProductId          UserId                                 Summary  Score  \\\n",
       "0  B00141QYSQ  A1YS02UZZGRDCT                              Do Not Buy      1   \n",
       "1  B0089SPEO2  A3JOYNYL458QHP                Less lemon and less zing      3   \n",
       "2  B001PMCDK2  A14TTMM0Z03Y2W            my cat goes crazy for these!      5   \n",
       "3  B002Q8JOSI  A17UQD2RSSQH5X  My dogs tell me these treats are YUMMY      5   \n",
       "4  B00176G870  A2F2MZW8EOGH5J                      Yummy to the tummy      5   \n",
       "5  B001CHFUGY  A2M8VROSDPU4JT                        Very good coffee      5   \n",
       "6  B0041CIR62  A16I6WJUEBJ1C3   okay but not as healthy as it appears      2   \n",
       "7  B001R3BQFW   AM50E42AFUVNL                            Taste great.      5   \n",
       "8  B005HGAV8I  A2I5KDNOESGJ1H                          variety galore      5   \n",
       "9  B000GFYRHQ  A3A7YUR6FS6ZCI             Bigelow Earl Grey Green Tea      5   \n",
       "\n",
       "   HelpfulnessDenominator      Id  \\\n",
       "0                       2   41471   \n",
       "1                       0   28582   \n",
       "2                       0  389965   \n",
       "3                       1  212536   \n",
       "4                       0  115971   \n",
       "5                       1  434484   \n",
       "6                       1  138997   \n",
       "7                       0  126555   \n",
       "8                       1  438837   \n",
       "9                       0  245379   \n",
       "\n",
       "                                        ProfileName  HelpfulnessNumerator  \\\n",
       "0                                    Evan Eberhardt                     2   \n",
       "1                                         coleridge                     0   \n",
       "2                               Lindsay S. Bradford                     0   \n",
       "3                                       in the dark                     1   \n",
       "4  daemoncycler \"When you arrive at a fork in th...                     0   \n",
       "5                           Officefan \"Officefankt\"                     1   \n",
       "6                       doctorsirena \"doctorsirena\"                     1   \n",
       "7                                  T. Higley \"Tina\"                     0   \n",
       "8                                                TJ                     1   \n",
       "9                                         Tea Lover                     0   \n",
       "\n",
       "         Time                                               Text  \n",
       "0  1348358400  These are made in China (do not buy ANY pet fo...  \n",
       "1  1323907200  Everything is ok, except it just isn't as good...  \n",
       "2  1310601600  Best cat treat ever. There isn't anything comp...  \n",
       "3  1316131200  My two Corgis were thoroughly spoiled by my la...  \n",
       "4  1334793600  We used to have drive down to the specialty pe...  \n",
       "5  1277251200  I really liked this coffee, it was just as goo...  \n",
       "6  1343692800  I am always looking for healthier, whole grain...  \n",
       "7  1323561600  I have tried many different drink mix, this is...  \n",
       "8  1334016000  This is my favorite item to order for my Keuri...  \n",
       "9  1178409600  Tastes like Earl Grey, but it's green tea so i...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# Spark DataFrame\n",
    "df = hc.asSparkFrame(reviews)\n",
    "df.limit(100).show()\n",
    "print(type(df))\n",
    "# Pandas DataFrame\n",
    "pdf = reviews.head().as_data_frame()\n",
    "display(pdf)\n",
    "print(type(pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nice!\n",
    "<style>\n",
    "blockquote{\n",
    "  font-size: 15px;\n",
    "  background: #f9f9f9;\n",
    "  border-left: 10px solid #ccc;\n",
    "  margin: .5em 10px;\n",
    "  padding: 30em, 10px;\n",
    "  quotes: \"\\201C\"\"\\201D\"\"\\2018\"\"\\2019\";\n",
    "  padding: 10px 20px;\n",
    "  line-height: 1.4;\n",
    "}\n",
    "\n",
    "blockquote:before {\n",
    "  content: open-quote;\n",
    "  display: inline;\n",
    "  height: 0;\n",
    "  line-height: 0;\n",
    "  left: -10px;\n",
    "  position: relative;\n",
    "  top: 30px;\n",
    "  bottom:30px;\n",
    "  color: #ccc;\n",
    "  font-size: 3em;\n",
    "    display:none;\n",
    "\n",
    "}\n",
    "\n",
    "p{\n",
    "  margin: 0;\n",
    "}\n",
    "\n",
    "footer{\n",
    "  margin:0;\n",
    "  text-align: right;\n",
    "  font-size: 1em;\n",
    "  font-style: italic;\n",
    "}\n",
    "</style>\n",
    "<blockquote><p class='quotation'><b><br><span style='font-size:25px'>H2O Import</span></b> <br><br>Now that we have our Spark DataFrame, we can create a table and insert data using <code>splice.createTable</code> and <code>splice.insert</code><br><b>Note: </b>If your code is hanging on the <code>insert</code> your cluser may be out of memory. Try configuring your Spark or H2O cluster with more memory. Read about that <a href=https://docs.h2o.ai/sparkling-water/2.1/latest-stable/doc/configuration/configuration_properties.html>here</a> and <a href=https://spark.apache.org/docs/latest/configuration.html#available-properties>here</a></footer></blockquote><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method createTable in module splicemachine.spark.context:\n",
      "\n",
      "createTable(dataframe, schema_table_name, primary_keys=None, create_table_options=None, to_upper=False, drop_table=False) method of splicemachine.spark.context.PySpliceContext instance\n",
      "    Creates a schema.table from a dataframe\n",
      "    :param dataframe: The Spark DataFrame to base the table off\n",
      "    :param schema_table_name: str The schema.table to create\n",
      "    :param primary_keys: List[str] the primary keys. Default None\n",
      "    :param create_table_options: str The additional table-level SQL options default None\n",
      "    :param to_upper: bool If the dataframe columns should be converted to uppercase before table creation\n",
      "                        If False, the table will be created with lower case columns. Default False\n",
      "    :param drop_table: bool whether to drop the table if it exists. Default False. If False and the table exists,\n",
      "                       the function will throw an exception.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "Help on method insert in module splicemachine.spark.context:\n",
      "\n",
      "insert(dataframe, schema_table_name, to_upper=False) method of splicemachine.spark.context.PySpliceContext instance\n",
      "    Insert a dataframe into a table (schema.table).\n",
      "    \n",
      "    :param dataframe: (DF) The dataframe you would like to insert\n",
      "    :param schema_table_name: (string) The table in which you would like to insert the DF\n",
      "    :param to_upper: bool If the dataframe columns should be converted to uppercase before table creation\n",
      "                        If False, the table will be created with lower case columns. Default False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(splice.createTable)\n",
    "print('----------------------------------------------------------------------------------------------------------------')\n",
    "help(splice.insert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+http://www.github.com/splicemachine/pysplice@DBAAS-3990\n",
      "  Cloning http://www.github.com/splicemachine/pysplice (to revision DBAAS-3990) to /tmp/pip-req-build-g1rnih7t\n",
      "  Running command git clone -q http://www.github.com/splicemachine/pysplice /tmp/pip-req-build-g1rnih7t\n",
      "  Running command git checkout -b DBAAS-3990 --track origin/DBAAS-3990\n",
      "  Switched to a new branch 'DBAAS-3990'\n",
      "  Branch 'DBAAS-3990' set up to track remote branch 'DBAAS-3990' from 'origin'.\n",
      "Requirement already satisfied, skipping upgrade: py4j==0.10.8.1 in /opt/conda/lib/python3.7/site-packages (from splicemachine==2.0.0) (0.10.8.1)\n",
      "Requirement already satisfied, skipping upgrade: pytest==5.1.3 in /opt/conda/lib/python3.7/site-packages (from splicemachine==2.0.0) (5.1.3)\n",
      "Requirement already satisfied, skipping upgrade: mlflow==1.6.0 in /opt/conda/lib/python3.7/site-packages (from splicemachine==2.0.0) (1.6.0)\n",
      "Requirement already satisfied, skipping upgrade: mleap==0.15.0 in /opt/conda/lib/python3.7/site-packages (from splicemachine==2.0.0) (0.15.0)\n",
      "Requirement already satisfied, skipping upgrade: graphviz==0.13 in /opt/conda/lib/python3.7/site-packages (from splicemachine==2.0.0) (0.13)\n",
      "Requirement already satisfied, skipping upgrade: requests in /opt/conda/lib/python3.7/site-packages (from splicemachine==2.0.0) (2.23.0)\n",
      "Requirement already satisfied, skipping upgrade: gorilla==0.3.0 in /opt/conda/lib/python3.7/site-packages (from splicemachine==2.0.0) (0.3.0)\n",
      "Requirement already satisfied, skipping upgrade: tqdm==4.43.0 in /opt/conda/lib/python3.7/site-packages (from splicemachine==2.0.0) (4.43.0)\n",
      "Requirement already satisfied, skipping upgrade: pyspark-dist-explore==0.1.8 in /opt/conda/lib/python3.7/site-packages (from splicemachine==2.0.0) (0.1.8)\n",
      "Requirement already satisfied, skipping upgrade: numpy==1.18.2 in /opt/conda/lib/python3.7/site-packages (from splicemachine==2.0.0) (1.18.2)\n",
      "Requirement already satisfied, skipping upgrade: pandas==1.0.3 in /opt/conda/lib/python3.7/site-packages (from splicemachine==2.0.0) (1.0.3)\n",
      "Requirement already satisfied, skipping upgrade: scipy==1.4.1 in /opt/conda/lib/python3.7/site-packages (from splicemachine==2.0.0) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: more-itertools>=4.0.0 in /opt/conda/lib/python3.7/site-packages (from pytest==5.1.3->splicemachine==2.0.0) (8.3.0)\n",
      "Requirement already satisfied, skipping upgrade: atomicwrites>=1.0 in /opt/conda/lib/python3.7/site-packages (from pytest==5.1.3->splicemachine==2.0.0) (1.4.0)\n",
      "Requirement already satisfied, skipping upgrade: py>=1.5.0 in /opt/conda/lib/python3.7/site-packages (from pytest==5.1.3->splicemachine==2.0.0) (1.8.1)\n",
      "Requirement already satisfied, skipping upgrade: wcwidth in /opt/conda/lib/python3.7/site-packages (from pytest==5.1.3->splicemachine==2.0.0) (0.1.9)\n",
      "Requirement already satisfied, skipping upgrade: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from pytest==5.1.3->splicemachine==2.0.0) (19.3.0)\n",
      "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.12; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from pytest==5.1.3->splicemachine==2.0.0) (1.6.0)\n",
      "Requirement already satisfied, skipping upgrade: pluggy<1.0,>=0.12 in /opt/conda/lib/python3.7/site-packages (from pytest==5.1.3->splicemachine==2.0.0) (0.13.1)\n",
      "Requirement already satisfied, skipping upgrade: packaging in /opt/conda/lib/python3.7/site-packages (from pytest==5.1.3->splicemachine==2.0.0) (20.4)\n",
      "Requirement already satisfied, skipping upgrade: prometheus-flask-exporter in /opt/conda/lib/python3.7/site-packages (from mlflow==1.6.0->splicemachine==2.0.0) (0.13.0)\n",
      "Requirement already satisfied, skipping upgrade: pyyaml in /opt/conda/lib/python3.7/site-packages (from mlflow==1.6.0->splicemachine==2.0.0) (5.3.1)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.6.0 in /opt/conda/lib/python3.7/site-packages (from mlflow==1.6.0->splicemachine==2.0.0) (3.9.1)\n",
      "Requirement already satisfied, skipping upgrade: databricks-cli>=0.8.7 in /opt/conda/lib/python3.7/site-packages (from mlflow==1.6.0->splicemachine==2.0.0) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: cloudpickle in /opt/conda/lib/python3.7/site-packages (from mlflow==1.6.0->splicemachine==2.0.0) (1.2.2)\n",
      "Requirement already satisfied, skipping upgrade: click>=7.0 in /opt/conda/lib/python3.7/site-packages (from mlflow==1.6.0->splicemachine==2.0.0) (7.1.2)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /opt/conda/lib/python3.7/site-packages (from mlflow==1.6.0->splicemachine==2.0.0) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: querystring-parser in /opt/conda/lib/python3.7/site-packages (from mlflow==1.6.0->splicemachine==2.0.0) (1.2.4)\n",
      "Requirement already satisfied, skipping upgrade: entrypoints in /opt/conda/lib/python3.7/site-packages (from mlflow==1.6.0->splicemachine==2.0.0) (0.3)\n",
      "Requirement already satisfied, skipping upgrade: gitpython>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from mlflow==1.6.0->splicemachine==2.0.0) (3.1.2)\n",
      "Requirement already satisfied, skipping upgrade: alembic in /opt/conda/lib/python3.7/site-packages (from mlflow==1.6.0->splicemachine==2.0.0) (1.4.2)\n",
      "Requirement already satisfied, skipping upgrade: sqlalchemy in /opt/conda/lib/python3.7/site-packages (from mlflow==1.6.0->splicemachine==2.0.0) (1.3.8)\n",
      "Requirement already satisfied, skipping upgrade: sqlparse in /opt/conda/lib/python3.7/site-packages (from mlflow==1.6.0->splicemachine==2.0.0) (0.3.1)\n",
      "Requirement already satisfied, skipping upgrade: Flask in /opt/conda/lib/python3.7/site-packages (from mlflow==1.6.0->splicemachine==2.0.0) (1.1.2)\n",
      "Requirement already satisfied, skipping upgrade: docker>=4.0.0 in /opt/conda/lib/python3.7/site-packages (from mlflow==1.6.0->splicemachine==2.0.0) (4.2.0)\n",
      "Requirement already satisfied, skipping upgrade: simplejson in /opt/conda/lib/python3.7/site-packages (from mlflow==1.6.0->splicemachine==2.0.0) (3.17.0)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil in /opt/conda/lib/python3.7/site-packages (from mlflow==1.6.0->splicemachine==2.0.0) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: gunicorn; platform_system != \"Windows\" in /opt/conda/lib/python3.7/site-packages (from mlflow==1.6.0->splicemachine==2.0.0) (20.0.4)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn>=0.18.dev0 in /opt/conda/lib/python3.7/site-packages (from mleap==0.15.0->splicemachine==2.0.0) (0.21.3)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->splicemachine==2.0.0) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->splicemachine==2.0.0) (2.9)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->splicemachine==2.0.0) (1.25.9)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->splicemachine==2.0.0) (2020.4.5.1)\n",
      "Requirement already satisfied, skipping upgrade: matplotlib in /opt/conda/lib/python3.7/site-packages (from pyspark-dist-explore==0.1.8->splicemachine==2.0.0) (3.1.1)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas==1.0.3->splicemachine==2.0.0) (2020.1)\n",
      "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.12; python_version < \"3.8\"->pytest==5.1.3->splicemachine==2.0.0) (3.1.0)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->pytest==5.1.3->splicemachine==2.0.0) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: prometheus-client in /opt/conda/lib/python3.7/site-packages (from prometheus-flask-exporter->mlflow==1.6.0->splicemachine==2.0.0) (0.7.1)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /opt/conda/lib/python3.7/site-packages (from protobuf>=3.6.0->mlflow==1.6.0->splicemachine==2.0.0) (46.4.0.post20200518)\n",
      "Requirement already satisfied, skipping upgrade: configparser>=0.3.5 in /opt/conda/lib/python3.7/site-packages (from databricks-cli>=0.8.7->mlflow==1.6.0->splicemachine==2.0.0) (5.0.0)\n",
      "Requirement already satisfied, skipping upgrade: tabulate>=0.7.7 in /opt/conda/lib/python3.7/site-packages (from databricks-cli>=0.8.7->mlflow==1.6.0->splicemachine==2.0.0) (0.8.7)\n",
      "Requirement already satisfied, skipping upgrade: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.7/site-packages (from gitpython>=2.1.0->mlflow==1.6.0->splicemachine==2.0.0) (4.0.5)\n",
      "Requirement already satisfied, skipping upgrade: Mako in /opt/conda/lib/python3.7/site-packages (from alembic->mlflow==1.6.0->splicemachine==2.0.0) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: python-editor>=0.3 in /opt/conda/lib/python3.7/site-packages (from alembic->mlflow==1.6.0->splicemachine==2.0.0) (1.0.4)\n",
      "Requirement already satisfied, skipping upgrade: Jinja2>=2.10.1 in /opt/conda/lib/python3.7/site-packages (from Flask->mlflow==1.6.0->splicemachine==2.0.0) (2.11.2)\n",
      "Requirement already satisfied, skipping upgrade: itsdangerous>=0.24 in /opt/conda/lib/python3.7/site-packages (from Flask->mlflow==1.6.0->splicemachine==2.0.0) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: Werkzeug>=0.15 in /opt/conda/lib/python3.7/site-packages (from Flask->mlflow==1.6.0->splicemachine==2.0.0) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: websocket-client>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from docker>=4.0.0->mlflow==1.6.0->splicemachine==2.0.0) (0.57.0)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.18.dev0->mleap==0.15.0->splicemachine==2.0.0) (0.15.1)\n",
      "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->pyspark-dist-explore==0.1.8->splicemachine==2.0.0) (1.2.0)\n",
      "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->pyspark-dist-explore==0.1.8->splicemachine==2.0.0) (0.10.0)\n",
      "Requirement already satisfied, skipping upgrade: smmap<4,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->gitpython>=2.1.0->mlflow==1.6.0->splicemachine==2.0.0) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.9.2 in /opt/conda/lib/python3.7/site-packages (from Mako->alembic->mlflow==1.6.0->splicemachine==2.0.0) (1.1.1)\n",
      "Building wheels for collected packages: splicemachine\n",
      "  Building wheel for splicemachine (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for splicemachine: filename=splicemachine-2.0.0-py3-none-any.whl size=54452 sha256=9078468eb246d79d8a4b90da2efe9676214f1d648f2874877456e0baf830cd3a\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-5pnf6puv/wheels/4e/a7/2b/f72322ec9cfd5fdd9467dfbcae791279bb5a69cab50e58af12\n",
      "Successfully built splicemachine\n",
      "Installing collected packages: splicemachine\n",
      "  Attempting uninstall: splicemachine\n",
      "    Found existing installation: splicemachine 2.0.0\n",
      "    Uninstalling splicemachine-2.0.0:\n",
      "      Successfully uninstalled splicemachine-2.0.0\n",
      "Successfully installed splicemachine-2.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade git+http://www.github.com/splicemachine/pysplice@DBAAS-3990"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Droping table BEN.AMAZON_REVIEWS_H2O\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-34b3fc678957>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0msplice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dropTableIfExists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{schema}.AMAZON_REVIEWS_H2O'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msplice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreateTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'{schema}.AMAZON_REVIEWS_H2O'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_upper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0msplice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'{schema}.AMAZON_REVIEWS_H2O'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mto_upper\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Use to_upper to give the SQL table uppercase letters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/splicemachine/spark/context.py\u001b[0m in \u001b[0;36minsert\u001b[0;34m(self, dataframe, schema_table_name, to_upper)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mto_upper\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0mdataframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoUpper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema_table_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema_table_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create the table\n",
    "schema = 'REPLACE_ME_DBSCHEMA'\n",
    "df = df.withColumnRenamed('Time', 'Review_Time')\n",
    "df = df.withColumnRenamed('Text', 'Review')\n",
    "splice._dropTableIfExists(f'{schema}.AMAZON_REVIEWS_H2O')\n",
    "splice.createTable(df, f'{schema}.AMAZON_REVIEWS_H2O', to_upper=True)\n",
    "splice.insert(df, f'{schema}.AMAZON_REVIEWS_H2O',to_upper=True) # Use to_upper to give the SQL table uppercase letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-03fa5f3c8ec7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sql'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-- select top 10 ProductId, UserId, Score  from AMAZON_REVIEWS_H2O\\nselect top 10 * from sys.systables;\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2360\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2362\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2363\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-133>\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, line, cell)\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/beakerx_magics/sql_magic.py\u001b[0m in \u001b[0;36msql\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcell_magic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msql\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/beakerx_magics/kernel_magic.py\u001b[0m in \u001b[0;36mrun_cell\u001b[0;34m(self, line, code)\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_stdin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shell_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_iopub_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/jupyter_client/client.py\u001b[0m in \u001b[0;36mget_shell_msg\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_shell_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;34m\"\"\"Get a message from the shell channel\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshell_channel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_iopub_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/jupyter_client/blocking/channels.py\u001b[0m in \u001b[0;36mget_msg\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mtimeout\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0;36m1000\u001b[0m  \u001b[0;31m# seconds to ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout, flags)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poller_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m         \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0mevts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m         \u001b[0;31m# return 0 if no events, otherwise return event bitfield\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mevts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/zmq/sugar/poll.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mzmq_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msockets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/_poll.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython._poll.zmq_poll\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%sql\n",
    "select top 10 ProductId, UserId, Score  from AMAZON_REVIEWS_H2O;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Awesome! Let's get modeling\n",
    "<style>\n",
    "blockquote{\n",
    "  font-size: 15px;\n",
    "  background: #f9f9f9;\n",
    "  border-left: 10px solid #ccc;\n",
    "  margin: .5em 10px;\n",
    "  padding: 30em, 10px;\n",
    "  quotes: \"\\201C\"\"\\201D\"\"\\2018\"\"\\2019\";\n",
    "  padding: 10px 20px;\n",
    "  line-height: 1.4;\n",
    "}\n",
    "\n",
    "blockquote:before {\n",
    "  content: open-quote;\n",
    "  display: inline;\n",
    "  height: 0;\n",
    "  line-height: 0;\n",
    "  left: -10px;\n",
    "  position: relative;\n",
    "  top: 30px;\n",
    "  bottom:30px;\n",
    "  color: #ccc;\n",
    "  font-size: 3em;\n",
    "    display:none;\n",
    "\n",
    "}\n",
    "\n",
    "p{\n",
    "  margin: 0;\n",
    "}\n",
    "\n",
    "footer{\n",
    "  margin:0;\n",
    "  text-align: right;\n",
    "  font-size: 1em;\n",
    "  font-style: italic;\n",
    "}\n",
    "</style>\n",
    "<blockquote><p class='quotation'><b><br><span style='font-size:25px'>Modeling</span></b> <br><br>We're going to try three different ways to approach this problem, and track it all with MLManager. \n",
    "    <ol>\n",
    "        <li>No Text Model: We will try to predict the customer reviews without using the text from the review. Just the Numeric Columns</li>\n",
    "        <li>Using the reviews: We will use Word2Vec to create vectors from the text of the reviews. We will then train a model on that word embedding feature-vector</li>\n",
    "        <li>Using the review summaries: We will use Word2Vec to create vectors from the text of the review summaries</li>\n",
    "    </ol>\n",
    "    Which do you think will perform the best?\n",
    "    </i></br><footer>Splice Machine</footer></blockquote><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Attempt\n",
    "Let's create a simple model using the non-review columns\n",
    "<br>\n",
    "<blockquote>\n",
    "    First, let's start our mlflow experiment! We can start a run and log import parameters, tags, and metrics as they come<br>\n",
    "Next, we can turn this into a binary-classification problem by turning score into a positive or negative review. We will say that 4 and 5 start reviews are positive, but you can change this and try other things!\n",
    "</br><footer>Splice Machine</footer></blockquote><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: 'Sentiment Analysis' does not exist. Creating a new experiment\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: right;\">  PositiveReview</th><th style=\"text-align: right;\">  Count</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: right;\">               0</td><td style=\"text-align: right;\">  21791</td></tr>\n",
       "<tr><td style=\"text-align: right;\">               1</td><td style=\"text-align: right;\">  78209</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set our mlflow experiment\n",
    "mlflow.set_experiment('Sentiment Analysis')\n",
    "# Look at our dataframe\n",
    "reviews[\"PositiveReview\"] = (reviews[\"Score\"] >= 4).ifelse(\"1\", \"0\")\n",
    "reviews[\"PositiveReview\"].table()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see our Data Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PILLOW_VERSION is deprecated and will be removed in a future release. Use __version__ instead.\n",
      "pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 792x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from pyspark.sql.types import FloatType, IntegerType\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pdf = reviews[['ProductId', 'UserId', 'HelpfulnessNumerator', 'HelpfulnessDenominator', 'Time','PositiveReview']].as_data_frame()\n",
    "corr = pdf.corr()\n",
    "\n",
    "ticks = [i for i in range(len(corr.columns))]\n",
    "# Generate a mask for the upper triangle\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Color Scheme\n",
    "cmap = \"coolwarm\"\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr,  cmap=cmap, vmax=.3, center=0,\n",
    "            square=False, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "\n",
    "plt.xticks(ticks, corr.columns)\n",
    "plt.yticks(ticks, corr.columns)\n",
    "plt.title('Sentiment Data correlation heatmap')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can see some of our features have decent correlation (remember that we aren't using the reviews yet). Let's try a basic model\n",
    "### First, let's log some important information in our <code>run</code>\n",
    "<blockquote>H2O Has a <a href=https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science.html>lot</a> of algorithms, so we'll use a Gradient Boosting Estimator<br>We'll log some things like our feature vector, label, train/test/validation split, training time, and even the model and notebook themselves directly to <a href='/mlflow'>mlflow</a></blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Code Block train_time... gbm Model Build progress: |███████████████████████████████████████████████| 100%\n",
      "Done.\n",
      "Code Block train_time:\n",
      "Ran in 6.216 secs\n",
      "Ran in 0.104 mins\n",
      "Saving artifact of size: 1584.281 KB to Splice Machine DB\n",
      "Saving artifact of size: 310.352 KB to Splice Machine DB\n",
      "CPU times: user 750 ms, sys: 68.5 ms, total: 818 ms\n",
      "Wall time: 12.1 s\n",
      "Model Details\n",
      "=============\n",
      "H2OGradientBoostingEstimator :  Gradient Boosting Machine\n",
      "Model Key:  gbm_baseline.hex\n",
      "\n",
      "\n",
      "Model Summary: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>number_of_trees</th>\n",
       "      <th>number_of_internal_trees</th>\n",
       "      <th>model_size_in_bytes</th>\n",
       "      <th>min_depth</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>mean_depth</th>\n",
       "      <th>min_leaves</th>\n",
       "      <th>max_leaves</th>\n",
       "      <th>mean_leaves</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>25959.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>30.78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     number_of_trees  number_of_internal_trees  model_size_in_bytes  \\\n",
       "0               50.0                      50.0              25959.0   \n",
       "\n",
       "   min_depth  max_depth  mean_depth  min_leaves  max_leaves  mean_leaves  \n",
       "0        5.0        5.0         5.0        26.0        32.0        30.78  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ModelMetricsBinomial: gbm\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 0.13519480049697202\n",
      "RMSE: 0.36768845575700637\n",
      "LogLoss: 0.43326115755931044\n",
      "Mean Per-Class Error: 0.3027351317098821\n",
      "AUC: 0.7641567683211429\n",
      "AUCPR: 0.9067923294138088\n",
      "Gini: 0.5283135366422858\n",
      "\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.5075220558469353: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>Error</th>\n",
       "      <th>Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3904.0</td>\n",
       "      <td>11261.0</td>\n",
       "      <td>0.7426</td>\n",
       "      <td>(11261.0/15165.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1186.0</td>\n",
       "      <td>53583.0</td>\n",
       "      <td>0.0217</td>\n",
       "      <td>(1186.0/54769.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Total</td>\n",
       "      <td>5090.0</td>\n",
       "      <td>64844.0</td>\n",
       "      <td>0.178</td>\n",
       "      <td>(12447.0/69934.0)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0        1   Error                Rate\n",
       "0      0  3904.0  11261.0  0.7426   (11261.0/15165.0)\n",
       "1      1  1186.0  53583.0  0.0217    (1186.0/54769.0)\n",
       "2  Total  5090.0  64844.0   0.178   (12447.0/69934.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Maximum Metrics: Maximum metrics at their respective thresholds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>threshold</th>\n",
       "      <th>value</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>max f1</td>\n",
       "      <td>0.507522</td>\n",
       "      <td>0.895939</td>\n",
       "      <td>267.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>max f2</td>\n",
       "      <td>0.267858</td>\n",
       "      <td>0.949974</td>\n",
       "      <td>339.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>max f0point5</td>\n",
       "      <td>0.751349</td>\n",
       "      <td>0.868164</td>\n",
       "      <td>164.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>max accuracy</td>\n",
       "      <td>0.577385</td>\n",
       "      <td>0.823448</td>\n",
       "      <td>241.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max precision</td>\n",
       "      <td>0.949650</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>max recall</td>\n",
       "      <td>0.076643</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>393.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>max specificity</td>\n",
       "      <td>0.949650</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>max absolute_mcc</td>\n",
       "      <td>0.749414</td>\n",
       "      <td>0.397431</td>\n",
       "      <td>165.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>max min_per_class_accuracy</td>\n",
       "      <td>0.818968</td>\n",
       "      <td>0.687966</td>\n",
       "      <td>114.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>max mean_per_class_accuracy</td>\n",
       "      <td>0.786125</td>\n",
       "      <td>0.697265</td>\n",
       "      <td>141.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>max tns</td>\n",
       "      <td>0.949650</td>\n",
       "      <td>15165.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>max fns</td>\n",
       "      <td>0.949650</td>\n",
       "      <td>54754.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>max fps</td>\n",
       "      <td>0.042665</td>\n",
       "      <td>15165.000000</td>\n",
       "      <td>399.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>max tps</td>\n",
       "      <td>0.076643</td>\n",
       "      <td>54769.000000</td>\n",
       "      <td>393.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>max tnr</td>\n",
       "      <td>0.949650</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>max fnr</td>\n",
       "      <td>0.949650</td>\n",
       "      <td>0.999726</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>max fpr</td>\n",
       "      <td>0.042665</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>399.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>max tpr</td>\n",
       "      <td>0.076643</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>393.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         metric  threshold         value    idx\n",
       "0                        max f1   0.507522      0.895939  267.0\n",
       "1                        max f2   0.267858      0.949974  339.0\n",
       "2                  max f0point5   0.751349      0.868164  164.0\n",
       "3                  max accuracy   0.577385      0.823448  241.0\n",
       "4                 max precision   0.949650      1.000000    0.0\n",
       "5                    max recall   0.076643      1.000000  393.0\n",
       "6               max specificity   0.949650      1.000000    0.0\n",
       "7              max absolute_mcc   0.749414      0.397431  165.0\n",
       "8    max min_per_class_accuracy   0.818968      0.687966  114.0\n",
       "9   max mean_per_class_accuracy   0.786125      0.697265  141.0\n",
       "10                      max tns   0.949650  15165.000000    0.0\n",
       "11                      max fns   0.949650  54754.000000    0.0\n",
       "12                      max fps   0.042665  15165.000000  399.0\n",
       "13                      max tps   0.076643  54769.000000  393.0\n",
       "14                      max tnr   0.949650      1.000000    0.0\n",
       "15                      max fnr   0.949650      0.999726    0.0\n",
       "16                      max fpr   0.042665      1.000000  399.0\n",
       "17                      max tpr   0.076643      1.000000  393.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gains/Lift Table: Avg response rate: 78.32 %, avg score: 78.31 %\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>cumulative_data_fraction</th>\n",
       "      <th>lower_threshold</th>\n",
       "      <th>lift</th>\n",
       "      <th>cumulative_lift</th>\n",
       "      <th>response_rate</th>\n",
       "      <th>score</th>\n",
       "      <th>cumulative_response_rate</th>\n",
       "      <th>cumulative_score</th>\n",
       "      <th>capture_rate</th>\n",
       "      <th>cumulative_capture_rate</th>\n",
       "      <th>gain</th>\n",
       "      <th>cumulative_gain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>0.010009</td>\n",
       "      <td>0.927720</td>\n",
       "      <td>1.256825</td>\n",
       "      <td>1.256825</td>\n",
       "      <td>0.984286</td>\n",
       "      <td>0.935431</td>\n",
       "      <td>0.984286</td>\n",
       "      <td>0.935431</td>\n",
       "      <td>0.012580</td>\n",
       "      <td>0.012580</td>\n",
       "      <td>25.682479</td>\n",
       "      <td>25.682479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>0.020047</td>\n",
       "      <td>0.919124</td>\n",
       "      <td>1.245968</td>\n",
       "      <td>1.251389</td>\n",
       "      <td>0.975783</td>\n",
       "      <td>0.922727</td>\n",
       "      <td>0.980029</td>\n",
       "      <td>0.929070</td>\n",
       "      <td>0.012507</td>\n",
       "      <td>0.025087</td>\n",
       "      <td>24.596837</td>\n",
       "      <td>25.138884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>0.030071</td>\n",
       "      <td>0.914102</td>\n",
       "      <td>1.236817</td>\n",
       "      <td>1.246531</td>\n",
       "      <td>0.968616</td>\n",
       "      <td>0.916376</td>\n",
       "      <td>0.976224</td>\n",
       "      <td>0.924839</td>\n",
       "      <td>0.012398</td>\n",
       "      <td>0.037485</td>\n",
       "      <td>23.681662</td>\n",
       "      <td>24.653143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td>0.040081</td>\n",
       "      <td>0.910879</td>\n",
       "      <td>1.231287</td>\n",
       "      <td>1.242724</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.912374</td>\n",
       "      <td>0.973243</td>\n",
       "      <td>0.921726</td>\n",
       "      <td>0.012324</td>\n",
       "      <td>0.049809</td>\n",
       "      <td>23.128699</td>\n",
       "      <td>24.272440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>0.050133</td>\n",
       "      <td>0.908283</td>\n",
       "      <td>1.198787</td>\n",
       "      <td>1.233914</td>\n",
       "      <td>0.938834</td>\n",
       "      <td>0.909526</td>\n",
       "      <td>0.966343</td>\n",
       "      <td>0.919280</td>\n",
       "      <td>0.012051</td>\n",
       "      <td>0.061860</td>\n",
       "      <td>19.878740</td>\n",
       "      <td>23.391444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td></td>\n",
       "      <td>6</td>\n",
       "      <td>0.100194</td>\n",
       "      <td>0.898486</td>\n",
       "      <td>1.195557</td>\n",
       "      <td>1.214750</td>\n",
       "      <td>0.936304</td>\n",
       "      <td>0.903580</td>\n",
       "      <td>0.951334</td>\n",
       "      <td>0.911435</td>\n",
       "      <td>0.059851</td>\n",
       "      <td>0.121711</td>\n",
       "      <td>19.555730</td>\n",
       "      <td>21.474956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "      <td>7</td>\n",
       "      <td>0.149999</td>\n",
       "      <td>0.891077</td>\n",
       "      <td>1.177540</td>\n",
       "      <td>1.202395</td>\n",
       "      <td>0.922194</td>\n",
       "      <td>0.895366</td>\n",
       "      <td>0.941659</td>\n",
       "      <td>0.906100</td>\n",
       "      <td>0.058646</td>\n",
       "      <td>0.180358</td>\n",
       "      <td>17.753987</td>\n",
       "      <td>20.239481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td></td>\n",
       "      <td>8</td>\n",
       "      <td>0.200003</td>\n",
       "      <td>0.881357</td>\n",
       "      <td>1.173921</td>\n",
       "      <td>1.195276</td>\n",
       "      <td>0.919359</td>\n",
       "      <td>0.886103</td>\n",
       "      <td>0.936084</td>\n",
       "      <td>0.901100</td>\n",
       "      <td>0.058701</td>\n",
       "      <td>0.239059</td>\n",
       "      <td>17.392108</td>\n",
       "      <td>19.527587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td></td>\n",
       "      <td>9</td>\n",
       "      <td>0.300026</td>\n",
       "      <td>0.862609</td>\n",
       "      <td>1.136149</td>\n",
       "      <td>1.175564</td>\n",
       "      <td>0.889778</td>\n",
       "      <td>0.874319</td>\n",
       "      <td>0.920646</td>\n",
       "      <td>0.892172</td>\n",
       "      <td>0.113641</td>\n",
       "      <td>0.352700</td>\n",
       "      <td>13.614935</td>\n",
       "      <td>17.556421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td></td>\n",
       "      <td>10</td>\n",
       "      <td>0.400063</td>\n",
       "      <td>0.840679</td>\n",
       "      <td>1.125766</td>\n",
       "      <td>1.163112</td>\n",
       "      <td>0.881647</td>\n",
       "      <td>0.849132</td>\n",
       "      <td>0.910894</td>\n",
       "      <td>0.881410</td>\n",
       "      <td>0.112618</td>\n",
       "      <td>0.465318</td>\n",
       "      <td>12.576598</td>\n",
       "      <td>16.311198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td></td>\n",
       "      <td>11</td>\n",
       "      <td>0.500543</td>\n",
       "      <td>0.831799</td>\n",
       "      <td>1.085366</td>\n",
       "      <td>1.147505</td>\n",
       "      <td>0.850007</td>\n",
       "      <td>0.836121</td>\n",
       "      <td>0.898672</td>\n",
       "      <td>0.872318</td>\n",
       "      <td>0.109058</td>\n",
       "      <td>0.574376</td>\n",
       "      <td>8.536577</td>\n",
       "      <td>14.750499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td></td>\n",
       "      <td>12</td>\n",
       "      <td>0.600352</td>\n",
       "      <td>0.819327</td>\n",
       "      <td>1.074563</td>\n",
       "      <td>1.135378</td>\n",
       "      <td>0.841547</td>\n",
       "      <td>0.826177</td>\n",
       "      <td>0.889175</td>\n",
       "      <td>0.864647</td>\n",
       "      <td>0.107250</td>\n",
       "      <td>0.681626</td>\n",
       "      <td>7.456348</td>\n",
       "      <td>13.537848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td></td>\n",
       "      <td>13</td>\n",
       "      <td>0.700060</td>\n",
       "      <td>0.799519</td>\n",
       "      <td>1.033159</td>\n",
       "      <td>1.120819</td>\n",
       "      <td>0.809121</td>\n",
       "      <td>0.811365</td>\n",
       "      <td>0.877773</td>\n",
       "      <td>0.857058</td>\n",
       "      <td>0.103014</td>\n",
       "      <td>0.784641</td>\n",
       "      <td>3.315855</td>\n",
       "      <td>12.081948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td></td>\n",
       "      <td>14</td>\n",
       "      <td>0.800040</td>\n",
       "      <td>0.758316</td>\n",
       "      <td>0.980495</td>\n",
       "      <td>1.103283</td>\n",
       "      <td>0.767878</td>\n",
       "      <td>0.784183</td>\n",
       "      <td>0.864039</td>\n",
       "      <td>0.847951</td>\n",
       "      <td>0.098030</td>\n",
       "      <td>0.882671</td>\n",
       "      <td>-1.950464</td>\n",
       "      <td>10.328335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td></td>\n",
       "      <td>15</td>\n",
       "      <td>0.900006</td>\n",
       "      <td>0.578001</td>\n",
       "      <td>0.792326</td>\n",
       "      <td>1.068745</td>\n",
       "      <td>0.620512</td>\n",
       "      <td>0.674008</td>\n",
       "      <td>0.836990</td>\n",
       "      <td>0.828631</td>\n",
       "      <td>0.079205</td>\n",
       "      <td>0.961876</td>\n",
       "      <td>-20.767419</td>\n",
       "      <td>6.874459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td></td>\n",
       "      <td>16</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.037130</td>\n",
       "      <td>0.381259</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.298584</td>\n",
       "      <td>0.372808</td>\n",
       "      <td>0.783153</td>\n",
       "      <td>0.783051</td>\n",
       "      <td>0.038124</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-61.874063</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      group  cumulative_data_fraction  lower_threshold      lift  \\\n",
       "0         1                  0.010009         0.927720  1.256825   \n",
       "1         2                  0.020047         0.919124  1.245968   \n",
       "2         3                  0.030071         0.914102  1.236817   \n",
       "3         4                  0.040081         0.910879  1.231287   \n",
       "4         5                  0.050133         0.908283  1.198787   \n",
       "5         6                  0.100194         0.898486  1.195557   \n",
       "6         7                  0.149999         0.891077  1.177540   \n",
       "7         8                  0.200003         0.881357  1.173921   \n",
       "8         9                  0.300026         0.862609  1.136149   \n",
       "9        10                  0.400063         0.840679  1.125766   \n",
       "10       11                  0.500543         0.831799  1.085366   \n",
       "11       12                  0.600352         0.819327  1.074563   \n",
       "12       13                  0.700060         0.799519  1.033159   \n",
       "13       14                  0.800040         0.758316  0.980495   \n",
       "14       15                  0.900006         0.578001  0.792326   \n",
       "15       16                  1.000000         0.037130  0.381259   \n",
       "\n",
       "    cumulative_lift  response_rate     score  cumulative_response_rate  \\\n",
       "0          1.256825       0.984286  0.935431                  0.984286   \n",
       "1          1.251389       0.975783  0.922727                  0.980029   \n",
       "2          1.246531       0.968616  0.916376                  0.976224   \n",
       "3          1.242724       0.964286  0.912374                  0.973243   \n",
       "4          1.233914       0.938834  0.909526                  0.966343   \n",
       "5          1.214750       0.936304  0.903580                  0.951334   \n",
       "6          1.202395       0.922194  0.895366                  0.941659   \n",
       "7          1.195276       0.919359  0.886103                  0.936084   \n",
       "8          1.175564       0.889778  0.874319                  0.920646   \n",
       "9          1.163112       0.881647  0.849132                  0.910894   \n",
       "10         1.147505       0.850007  0.836121                  0.898672   \n",
       "11         1.135378       0.841547  0.826177                  0.889175   \n",
       "12         1.120819       0.809121  0.811365                  0.877773   \n",
       "13         1.103283       0.767878  0.784183                  0.864039   \n",
       "14         1.068745       0.620512  0.674008                  0.836990   \n",
       "15         1.000000       0.298584  0.372808                  0.783153   \n",
       "\n",
       "    cumulative_score  capture_rate  cumulative_capture_rate       gain  \\\n",
       "0           0.935431      0.012580                 0.012580  25.682479   \n",
       "1           0.929070      0.012507                 0.025087  24.596837   \n",
       "2           0.924839      0.012398                 0.037485  23.681662   \n",
       "3           0.921726      0.012324                 0.049809  23.128699   \n",
       "4           0.919280      0.012051                 0.061860  19.878740   \n",
       "5           0.911435      0.059851                 0.121711  19.555730   \n",
       "6           0.906100      0.058646                 0.180358  17.753987   \n",
       "7           0.901100      0.058701                 0.239059  17.392108   \n",
       "8           0.892172      0.113641                 0.352700  13.614935   \n",
       "9           0.881410      0.112618                 0.465318  12.576598   \n",
       "10          0.872318      0.109058                 0.574376   8.536577   \n",
       "11          0.864647      0.107250                 0.681626   7.456348   \n",
       "12          0.857058      0.103014                 0.784641   3.315855   \n",
       "13          0.847951      0.098030                 0.882671  -1.950464   \n",
       "14          0.828631      0.079205                 0.961876 -20.767419   \n",
       "15          0.783051      0.038124                 1.000000 -61.874063   \n",
       "\n",
       "    cumulative_gain  \n",
       "0         25.682479  \n",
       "1         25.138884  \n",
       "2         24.653143  \n",
       "3         24.272440  \n",
       "4         23.391444  \n",
       "5         21.474956  \n",
       "6         20.239481  \n",
       "7         19.527587  \n",
       "8         17.556421  \n",
       "9         16.311198  \n",
       "10        14.750499  \n",
       "11        13.537848  \n",
       "12        12.081948  \n",
       "13        10.328335  \n",
       "14         6.874459  \n",
       "15         0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ModelMetricsBinomial: gbm\n",
      "** Reported on validation data. **\n",
      "\n",
      "MSE: 0.1434762548934531\n",
      "RMSE: 0.37878259581645657\n",
      "LogLoss: 0.45506218475133076\n",
      "Mean Per-Class Error: 0.3298965890404755\n",
      "AUC: 0.7334817035757148\n",
      "AUCPR: 0.8865098863706123\n",
      "Gini: 0.4669634071514297\n",
      "\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.46405678657606786: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>Error</th>\n",
       "      <th>Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>708.0</td>\n",
       "      <td>2671.0</td>\n",
       "      <td>0.7905</td>\n",
       "      <td>(2671.0/3379.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>198.0</td>\n",
       "      <td>11467.0</td>\n",
       "      <td>0.017</td>\n",
       "      <td>(198.0/11665.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Total</td>\n",
       "      <td>906.0</td>\n",
       "      <td>14138.0</td>\n",
       "      <td>0.1907</td>\n",
       "      <td>(2869.0/15044.0)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0        1   Error               Rate\n",
       "0      0  708.0   2671.0  0.7905    (2671.0/3379.0)\n",
       "1      1  198.0  11467.0   0.017    (198.0/11665.0)\n",
       "2  Total  906.0  14138.0  0.1907   (2869.0/15044.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Maximum Metrics: Maximum metrics at their respective thresholds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>threshold</th>\n",
       "      <th>value</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>max f1</td>\n",
       "      <td>0.464057</td>\n",
       "      <td>0.888811</td>\n",
       "      <td>288.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>max f2</td>\n",
       "      <td>0.295058</td>\n",
       "      <td>0.947700</td>\n",
       "      <td>340.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>max f0point5</td>\n",
       "      <td>0.699999</td>\n",
       "      <td>0.856361</td>\n",
       "      <td>194.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>max accuracy</td>\n",
       "      <td>0.538577</td>\n",
       "      <td>0.812217</td>\n",
       "      <td>260.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max precision</td>\n",
       "      <td>0.946930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>max recall</td>\n",
       "      <td>0.092448</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>394.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>max specificity</td>\n",
       "      <td>0.946930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>max absolute_mcc</td>\n",
       "      <td>0.663169</td>\n",
       "      <td>0.372715</td>\n",
       "      <td>207.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>max min_per_class_accuracy</td>\n",
       "      <td>0.822941</td>\n",
       "      <td>0.659751</td>\n",
       "      <td>112.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>max mean_per_class_accuracy</td>\n",
       "      <td>0.805133</td>\n",
       "      <td>0.670103</td>\n",
       "      <td>130.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>max tns</td>\n",
       "      <td>0.946930</td>\n",
       "      <td>3379.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>max fns</td>\n",
       "      <td>0.946930</td>\n",
       "      <td>11658.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>max fps</td>\n",
       "      <td>0.040330</td>\n",
       "      <td>3379.000000</td>\n",
       "      <td>399.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>max tps</td>\n",
       "      <td>0.092448</td>\n",
       "      <td>11665.000000</td>\n",
       "      <td>394.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>max tnr</td>\n",
       "      <td>0.946930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>max fnr</td>\n",
       "      <td>0.946930</td>\n",
       "      <td>0.999400</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>max fpr</td>\n",
       "      <td>0.040330</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>399.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>max tpr</td>\n",
       "      <td>0.092448</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>394.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         metric  threshold         value    idx\n",
       "0                        max f1   0.464057      0.888811  288.0\n",
       "1                        max f2   0.295058      0.947700  340.0\n",
       "2                  max f0point5   0.699999      0.856361  194.0\n",
       "3                  max accuracy   0.538577      0.812217  260.0\n",
       "4                 max precision   0.946930      1.000000    0.0\n",
       "5                    max recall   0.092448      1.000000  394.0\n",
       "6               max specificity   0.946930      1.000000    0.0\n",
       "7              max absolute_mcc   0.663169      0.372715  207.0\n",
       "8    max min_per_class_accuracy   0.822941      0.659751  112.0\n",
       "9   max mean_per_class_accuracy   0.805133      0.670103  130.0\n",
       "10                      max tns   0.946930   3379.000000    0.0\n",
       "11                      max fns   0.946930  11658.000000    0.0\n",
       "12                      max fps   0.040330   3379.000000  399.0\n",
       "13                      max tps   0.092448  11665.000000  394.0\n",
       "14                      max tnr   0.946930      1.000000    0.0\n",
       "15                      max fnr   0.946930      0.999400    0.0\n",
       "16                      max fpr   0.040330      1.000000  399.0\n",
       "17                      max tpr   0.092448      1.000000  394.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gains/Lift Table: Avg response rate: 77.54 %, avg score: 78.43 %\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>cumulative_data_fraction</th>\n",
       "      <th>lower_threshold</th>\n",
       "      <th>lift</th>\n",
       "      <th>cumulative_lift</th>\n",
       "      <th>response_rate</th>\n",
       "      <th>score</th>\n",
       "      <th>cumulative_response_rate</th>\n",
       "      <th>cumulative_score</th>\n",
       "      <th>capture_rate</th>\n",
       "      <th>cumulative_capture_rate</th>\n",
       "      <th>gain</th>\n",
       "      <th>cumulative_gain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>0.010037</td>\n",
       "      <td>0.925631</td>\n",
       "      <td>1.255507</td>\n",
       "      <td>1.255507</td>\n",
       "      <td>0.973510</td>\n",
       "      <td>0.933215</td>\n",
       "      <td>0.973510</td>\n",
       "      <td>0.933215</td>\n",
       "      <td>0.012602</td>\n",
       "      <td>0.012602</td>\n",
       "      <td>25.550651</td>\n",
       "      <td>25.550651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>0.020008</td>\n",
       "      <td>0.918181</td>\n",
       "      <td>1.195094</td>\n",
       "      <td>1.225401</td>\n",
       "      <td>0.926667</td>\n",
       "      <td>0.921323</td>\n",
       "      <td>0.950166</td>\n",
       "      <td>0.927289</td>\n",
       "      <td>0.011916</td>\n",
       "      <td>0.024518</td>\n",
       "      <td>19.509416</td>\n",
       "      <td>22.540069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>0.030178</td>\n",
       "      <td>0.913738</td>\n",
       "      <td>1.222236</td>\n",
       "      <td>1.224334</td>\n",
       "      <td>0.947712</td>\n",
       "      <td>0.915802</td>\n",
       "      <td>0.949339</td>\n",
       "      <td>0.923418</td>\n",
       "      <td>0.012430</td>\n",
       "      <td>0.036948</td>\n",
       "      <td>22.223623</td>\n",
       "      <td>22.433425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td>0.040016</td>\n",
       "      <td>0.910767</td>\n",
       "      <td>1.193816</td>\n",
       "      <td>1.216831</td>\n",
       "      <td>0.925676</td>\n",
       "      <td>0.912035</td>\n",
       "      <td>0.943522</td>\n",
       "      <td>0.920619</td>\n",
       "      <td>0.011745</td>\n",
       "      <td>0.048693</td>\n",
       "      <td>19.381611</td>\n",
       "      <td>21.683145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>0.050253</td>\n",
       "      <td>0.908108</td>\n",
       "      <td>1.155678</td>\n",
       "      <td>1.204374</td>\n",
       "      <td>0.896104</td>\n",
       "      <td>0.909277</td>\n",
       "      <td>0.933862</td>\n",
       "      <td>0.918309</td>\n",
       "      <td>0.011830</td>\n",
       "      <td>0.060523</td>\n",
       "      <td>15.567827</td>\n",
       "      <td>20.437432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td></td>\n",
       "      <td>6</td>\n",
       "      <td>0.100040</td>\n",
       "      <td>0.898880</td>\n",
       "      <td>1.188080</td>\n",
       "      <td>1.196265</td>\n",
       "      <td>0.921228</td>\n",
       "      <td>0.903672</td>\n",
       "      <td>0.927575</td>\n",
       "      <td>0.911025</td>\n",
       "      <td>0.059151</td>\n",
       "      <td>0.119674</td>\n",
       "      <td>18.808046</td>\n",
       "      <td>19.626529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "      <td>7</td>\n",
       "      <td>0.150027</td>\n",
       "      <td>0.891282</td>\n",
       "      <td>1.188486</td>\n",
       "      <td>1.193673</td>\n",
       "      <td>0.921543</td>\n",
       "      <td>0.895565</td>\n",
       "      <td>0.925565</td>\n",
       "      <td>0.905874</td>\n",
       "      <td>0.059408</td>\n",
       "      <td>0.179083</td>\n",
       "      <td>18.848574</td>\n",
       "      <td>19.367325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td></td>\n",
       "      <td>8</td>\n",
       "      <td>0.200678</td>\n",
       "      <td>0.881355</td>\n",
       "      <td>1.145809</td>\n",
       "      <td>1.181592</td>\n",
       "      <td>0.888451</td>\n",
       "      <td>0.885977</td>\n",
       "      <td>0.916197</td>\n",
       "      <td>0.900852</td>\n",
       "      <td>0.058037</td>\n",
       "      <td>0.237120</td>\n",
       "      <td>14.580913</td>\n",
       "      <td>18.159228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td></td>\n",
       "      <td>9</td>\n",
       "      <td>0.300053</td>\n",
       "      <td>0.862832</td>\n",
       "      <td>1.120589</td>\n",
       "      <td>1.161389</td>\n",
       "      <td>0.868896</td>\n",
       "      <td>0.874643</td>\n",
       "      <td>0.900532</td>\n",
       "      <td>0.892172</td>\n",
       "      <td>0.111359</td>\n",
       "      <td>0.348478</td>\n",
       "      <td>12.058948</td>\n",
       "      <td>16.138865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td></td>\n",
       "      <td>10</td>\n",
       "      <td>0.401555</td>\n",
       "      <td>0.841010</td>\n",
       "      <td>1.104707</td>\n",
       "      <td>1.147061</td>\n",
       "      <td>0.856582</td>\n",
       "      <td>0.849479</td>\n",
       "      <td>0.889422</td>\n",
       "      <td>0.881380</td>\n",
       "      <td>0.112130</td>\n",
       "      <td>0.460609</td>\n",
       "      <td>10.470746</td>\n",
       "      <td>14.706119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td></td>\n",
       "      <td>11</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.832847</td>\n",
       "      <td>1.085900</td>\n",
       "      <td>1.135019</td>\n",
       "      <td>0.841999</td>\n",
       "      <td>0.836583</td>\n",
       "      <td>0.880085</td>\n",
       "      <td>0.872560</td>\n",
       "      <td>0.106901</td>\n",
       "      <td>0.567510</td>\n",
       "      <td>8.590036</td>\n",
       "      <td>13.501929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td></td>\n",
       "      <td>12</td>\n",
       "      <td>0.600173</td>\n",
       "      <td>0.820078</td>\n",
       "      <td>1.063742</td>\n",
       "      <td>1.123123</td>\n",
       "      <td>0.824818</td>\n",
       "      <td>0.826936</td>\n",
       "      <td>0.870861</td>\n",
       "      <td>0.864945</td>\n",
       "      <td>0.106558</td>\n",
       "      <td>0.674068</td>\n",
       "      <td>6.374237</td>\n",
       "      <td>12.312270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td></td>\n",
       "      <td>13</td>\n",
       "      <td>0.700279</td>\n",
       "      <td>0.800656</td>\n",
       "      <td>1.018206</td>\n",
       "      <td>1.108125</td>\n",
       "      <td>0.789509</td>\n",
       "      <td>0.811935</td>\n",
       "      <td>0.859231</td>\n",
       "      <td>0.857367</td>\n",
       "      <td>0.101929</td>\n",
       "      <td>0.775997</td>\n",
       "      <td>1.820556</td>\n",
       "      <td>10.812458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td></td>\n",
       "      <td>14</td>\n",
       "      <td>0.800253</td>\n",
       "      <td>0.761944</td>\n",
       "      <td>0.998980</td>\n",
       "      <td>1.094489</td>\n",
       "      <td>0.774601</td>\n",
       "      <td>0.785776</td>\n",
       "      <td>0.848659</td>\n",
       "      <td>0.848424</td>\n",
       "      <td>0.099871</td>\n",
       "      <td>0.875868</td>\n",
       "      <td>-0.102028</td>\n",
       "      <td>9.448940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td></td>\n",
       "      <td>15</td>\n",
       "      <td>0.899960</td>\n",
       "      <td>0.579868</td>\n",
       "      <td>0.830547</td>\n",
       "      <td>1.065247</td>\n",
       "      <td>0.644000</td>\n",
       "      <td>0.676423</td>\n",
       "      <td>0.825984</td>\n",
       "      <td>0.829367</td>\n",
       "      <td>0.082812</td>\n",
       "      <td>0.958680</td>\n",
       "      <td>-16.945255</td>\n",
       "      <td>6.524700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td></td>\n",
       "      <td>16</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.040330</td>\n",
       "      <td>0.413037</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.320266</td>\n",
       "      <td>0.379206</td>\n",
       "      <td>0.775392</td>\n",
       "      <td>0.784333</td>\n",
       "      <td>0.041320</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-58.696285</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      group  cumulative_data_fraction  lower_threshold      lift  \\\n",
       "0         1                  0.010037         0.925631  1.255507   \n",
       "1         2                  0.020008         0.918181  1.195094   \n",
       "2         3                  0.030178         0.913738  1.222236   \n",
       "3         4                  0.040016         0.910767  1.193816   \n",
       "4         5                  0.050253         0.908108  1.155678   \n",
       "5         6                  0.100040         0.898880  1.188080   \n",
       "6         7                  0.150027         0.891282  1.188486   \n",
       "7         8                  0.200678         0.881355  1.145809   \n",
       "8         9                  0.300053         0.862832  1.120589   \n",
       "9        10                  0.401555         0.841010  1.104707   \n",
       "10       11                  0.500000         0.832847  1.085900   \n",
       "11       12                  0.600173         0.820078  1.063742   \n",
       "12       13                  0.700279         0.800656  1.018206   \n",
       "13       14                  0.800253         0.761944  0.998980   \n",
       "14       15                  0.899960         0.579868  0.830547   \n",
       "15       16                  1.000000         0.040330  0.413037   \n",
       "\n",
       "    cumulative_lift  response_rate     score  cumulative_response_rate  \\\n",
       "0          1.255507       0.973510  0.933215                  0.973510   \n",
       "1          1.225401       0.926667  0.921323                  0.950166   \n",
       "2          1.224334       0.947712  0.915802                  0.949339   \n",
       "3          1.216831       0.925676  0.912035                  0.943522   \n",
       "4          1.204374       0.896104  0.909277                  0.933862   \n",
       "5          1.196265       0.921228  0.903672                  0.927575   \n",
       "6          1.193673       0.921543  0.895565                  0.925565   \n",
       "7          1.181592       0.888451  0.885977                  0.916197   \n",
       "8          1.161389       0.868896  0.874643                  0.900532   \n",
       "9          1.147061       0.856582  0.849479                  0.889422   \n",
       "10         1.135019       0.841999  0.836583                  0.880085   \n",
       "11         1.123123       0.824818  0.826936                  0.870861   \n",
       "12         1.108125       0.789509  0.811935                  0.859231   \n",
       "13         1.094489       0.774601  0.785776                  0.848659   \n",
       "14         1.065247       0.644000  0.676423                  0.825984   \n",
       "15         1.000000       0.320266  0.379206                  0.775392   \n",
       "\n",
       "    cumulative_score  capture_rate  cumulative_capture_rate       gain  \\\n",
       "0           0.933215      0.012602                 0.012602  25.550651   \n",
       "1           0.927289      0.011916                 0.024518  19.509416   \n",
       "2           0.923418      0.012430                 0.036948  22.223623   \n",
       "3           0.920619      0.011745                 0.048693  19.381611   \n",
       "4           0.918309      0.011830                 0.060523  15.567827   \n",
       "5           0.911025      0.059151                 0.119674  18.808046   \n",
       "6           0.905874      0.059408                 0.179083  18.848574   \n",
       "7           0.900852      0.058037                 0.237120  14.580913   \n",
       "8           0.892172      0.111359                 0.348478  12.058948   \n",
       "9           0.881380      0.112130                 0.460609  10.470746   \n",
       "10          0.872560      0.106901                 0.567510   8.590036   \n",
       "11          0.864945      0.106558                 0.674068   6.374237   \n",
       "12          0.857367      0.101929                 0.775997   1.820556   \n",
       "13          0.848424      0.099871                 0.875868  -0.102028   \n",
       "14          0.829367      0.082812                 0.958680 -16.945255   \n",
       "15          0.784333      0.041320                 1.000000 -58.696285   \n",
       "\n",
       "    cumulative_gain  \n",
       "0         25.550651  \n",
       "1         22.540069  \n",
       "2         22.433425  \n",
       "3         21.683145  \n",
       "4         20.437432  \n",
       "5         19.626529  \n",
       "6         19.367325  \n",
       "7         18.159228  \n",
       "8         16.138865  \n",
       "9         14.706119  \n",
       "10        13.501929  \n",
       "11        12.312270  \n",
       "12        10.812458  \n",
       "13         9.448940  \n",
       "14         6.524700  \n",
       "15         0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Scoring History: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>duration</th>\n",
       "      <th>number_of_trees</th>\n",
       "      <th>training_rmse</th>\n",
       "      <th>training_logloss</th>\n",
       "      <th>training_auc</th>\n",
       "      <th>training_pr_auc</th>\n",
       "      <th>training_lift</th>\n",
       "      <th>training_classification_error</th>\n",
       "      <th>validation_rmse</th>\n",
       "      <th>validation_logloss</th>\n",
       "      <th>validation_auc</th>\n",
       "      <th>validation_pr_auc</th>\n",
       "      <th>validation_lift</th>\n",
       "      <th>validation_classification_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>2020-05-29 14:18:17</td>\n",
       "      <td>0.201 sec</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.412098</td>\n",
       "      <td>0.522889</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.216847</td>\n",
       "      <td>0.417396</td>\n",
       "      <td>0.532854</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.224608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>2020-05-29 14:18:19</td>\n",
       "      <td>2.205 sec</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.384889</td>\n",
       "      <td>0.467709</td>\n",
       "      <td>0.719285</td>\n",
       "      <td>0.884236</td>\n",
       "      <td>1.238747</td>\n",
       "      <td>0.187176</td>\n",
       "      <td>0.391486</td>\n",
       "      <td>0.479752</td>\n",
       "      <td>0.711396</td>\n",
       "      <td>0.875962</td>\n",
       "      <td>1.227600</td>\n",
       "      <td>0.194961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>2020-05-29 14:18:20</td>\n",
       "      <td>3.161 sec</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.376304</td>\n",
       "      <td>0.451300</td>\n",
       "      <td>0.732970</td>\n",
       "      <td>0.890938</td>\n",
       "      <td>1.226607</td>\n",
       "      <td>0.184946</td>\n",
       "      <td>0.383442</td>\n",
       "      <td>0.464608</td>\n",
       "      <td>0.722386</td>\n",
       "      <td>0.880443</td>\n",
       "      <td>1.225186</td>\n",
       "      <td>0.191704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>2020-05-29 14:18:20</td>\n",
       "      <td>3.696 sec</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.372844</td>\n",
       "      <td>0.444146</td>\n",
       "      <td>0.742384</td>\n",
       "      <td>0.895676</td>\n",
       "      <td>1.245880</td>\n",
       "      <td>0.182343</td>\n",
       "      <td>0.380811</td>\n",
       "      <td>0.459299</td>\n",
       "      <td>0.728109</td>\n",
       "      <td>0.883967</td>\n",
       "      <td>1.230665</td>\n",
       "      <td>0.188780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>2020-05-29 14:18:21</td>\n",
       "      <td>4.423 sec</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.369946</td>\n",
       "      <td>0.438014</td>\n",
       "      <td>0.754500</td>\n",
       "      <td>0.902222</td>\n",
       "      <td>1.257107</td>\n",
       "      <td>0.179412</td>\n",
       "      <td>0.379515</td>\n",
       "      <td>0.456591</td>\n",
       "      <td>0.731758</td>\n",
       "      <td>0.885341</td>\n",
       "      <td>1.255507</td>\n",
       "      <td>0.190840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td></td>\n",
       "      <td>2020-05-29 14:18:22</td>\n",
       "      <td>4.845 sec</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.367688</td>\n",
       "      <td>0.433261</td>\n",
       "      <td>0.764157</td>\n",
       "      <td>0.906792</td>\n",
       "      <td>1.256825</td>\n",
       "      <td>0.177982</td>\n",
       "      <td>0.378783</td>\n",
       "      <td>0.455062</td>\n",
       "      <td>0.733482</td>\n",
       "      <td>0.886510</td>\n",
       "      <td>1.255507</td>\n",
       "      <td>0.190707</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               timestamp    duration  number_of_trees  training_rmse  \\\n",
       "0    2020-05-29 14:18:17   0.201 sec              0.0       0.412098   \n",
       "1    2020-05-29 14:18:19   2.205 sec             10.0       0.384889   \n",
       "2    2020-05-29 14:18:20   3.161 sec             20.0       0.376304   \n",
       "3    2020-05-29 14:18:20   3.696 sec             30.0       0.372844   \n",
       "4    2020-05-29 14:18:21   4.423 sec             40.0       0.369946   \n",
       "5    2020-05-29 14:18:22   4.845 sec             50.0       0.367688   \n",
       "\n",
       "   training_logloss  training_auc  training_pr_auc  training_lift  \\\n",
       "0          0.522889      0.500000         0.000000       1.000000   \n",
       "1          0.467709      0.719285         0.884236       1.238747   \n",
       "2          0.451300      0.732970         0.890938       1.226607   \n",
       "3          0.444146      0.742384         0.895676       1.245880   \n",
       "4          0.438014      0.754500         0.902222       1.257107   \n",
       "5          0.433261      0.764157         0.906792       1.256825   \n",
       "\n",
       "   training_classification_error  validation_rmse  validation_logloss  \\\n",
       "0                       0.216847         0.417396            0.532854   \n",
       "1                       0.187176         0.391486            0.479752   \n",
       "2                       0.184946         0.383442            0.464608   \n",
       "3                       0.182343         0.380811            0.459299   \n",
       "4                       0.179412         0.379515            0.456591   \n",
       "5                       0.177982         0.378783            0.455062   \n",
       "\n",
       "   validation_auc  validation_pr_auc  validation_lift  \\\n",
       "0        0.500000           0.000000         1.000000   \n",
       "1        0.711396           0.875962         1.227600   \n",
       "2        0.722386           0.880443         1.225186   \n",
       "3        0.728109           0.883967         1.230665   \n",
       "4        0.731758           0.885341         1.255507   \n",
       "5        0.733482           0.886510         1.255507   \n",
       "\n",
       "   validation_classification_error  \n",
       "0                         0.224608  \n",
       "1                         0.194961  \n",
       "2                         0.191704  \n",
       "3                         0.188780  \n",
       "4                         0.190840  \n",
       "5                         0.190707  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Variable Importances: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>relative_importance</th>\n",
       "      <th>scaled_importance</th>\n",
       "      <th>percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HelpfulnessNumerator</td>\n",
       "      <td>5264.813477</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.421998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HelpfulnessDenominator</td>\n",
       "      <td>4238.071289</td>\n",
       "      <td>0.804980</td>\n",
       "      <td>0.339700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Time</td>\n",
       "      <td>1359.102417</td>\n",
       "      <td>0.258148</td>\n",
       "      <td>0.108938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ProductId</td>\n",
       "      <td>1185.762085</td>\n",
       "      <td>0.225224</td>\n",
       "      <td>0.095044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UserId</td>\n",
       "      <td>428.166962</td>\n",
       "      <td>0.081326</td>\n",
       "      <td>0.034319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 variable  relative_importance  scaled_importance  percentage\n",
       "0    HelpfulnessNumerator          5264.813477           1.000000    0.421998\n",
       "1  HelpfulnessDenominator          4238.071289           0.804980    0.339700\n",
       "2                    Time          1359.102417           0.258148    0.108938\n",
       "3               ProductId          1185.762085           0.225224    0.095044\n",
       "4                  UserId           428.166962           0.081326    0.034319"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from h2o.estimators import H2OGradientBoostingEstimator\n",
    "from splicemachine.mlflow_support.utilities import get_user\n",
    "\n",
    "RATIOS = [0.7,0.15]\n",
    "\n",
    "# Start our run to keep track of important information\n",
    "mlflow.start_run(run_name='simple_run')\n",
    "\n",
    "predictors = ['ProductId', 'UserId', 'HelpfulnessNumerator', 'HelpfulnessDenominator', 'Time']\n",
    "response = 'PositiveReview'\n",
    "\n",
    "# lp is short for log_param\n",
    "# lm is short for log_metric\n",
    "mlflow.lp('predictors', predictors)\n",
    "mlflow.lp('label', response)\n",
    "mlflow.lp('source data table', f'{get_user()}.AMAZON_REVIEWS')\n",
    "\n",
    "# Train Test Split\n",
    "train,test,valid = reviews.split_frame(ratios=RATIOS)\n",
    "# Log our ratios\n",
    "mlflow.lp('ratios',RATIOS)\n",
    "\n",
    "gbm_baseline = H2OGradientBoostingEstimator(stopping_metric = \"AUC\", stopping_tolerance = 0.001,\n",
    "                                            stopping_rounds = 5, score_tree_interval = 10,\n",
    "                                            model_id = \"gbm_baseline.hex\"\n",
    "                                           )\n",
    "\n",
    "mlflow.lp('model_type', gbm_baseline.__class__)\n",
    "\n",
    "# Code block to time training\n",
    "with mlflow.timer('train_time'):\n",
    "    gbm_baseline.train(x = predictors, y = response, \n",
    "                       training_frame = train, validation_frame = test\n",
    "                      )\n",
    "# Log the model params to mlflow\n",
    "mlflow.log_params(gbm_baseline.get_params())\n",
    "# Log the model to MLFlow\n",
    "mlflow.log_model(gbm_baseline, 'baseline_model')\n",
    "# Log the training notebook to MLFlow\n",
    "mlflow.log_artifact('MLManager H2O Demo.ipynb', 'training_notebook')\n",
    "gbm_baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You can see above that H2O gives you loads of details about your model. Let's inspect it a bit more and log some results to MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Summary: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>number_of_trees</th>\n",
       "      <th>number_of_internal_trees</th>\n",
       "      <th>model_size_in_bytes</th>\n",
       "      <th>min_depth</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>mean_depth</th>\n",
       "      <th>min_leaves</th>\n",
       "      <th>max_leaves</th>\n",
       "      <th>mean_leaves</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>25959.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>30.78</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     number_of_trees  number_of_internal_trees  model_size_in_bytes  \\\n",
       "0               50.0                      50.0              25959.0   \n",
       "\n",
       "   min_depth  max_depth  mean_depth  min_leaves  max_leaves  mean_leaves  \n",
       "0        5.0        5.0         5.0        26.0        32.0        30.78  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline  \n",
    "\n",
    "# Print and Log Model params\n",
    "params = dict(zip(gbm_baseline.summary().col_header[1:],\n",
    "                    gbm_baseline.summary().cell_values[0][1:]))\n",
    "print(gbm_baseline.summary())\n",
    "mlflow.log_params(params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Validation Data: 0.733\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "See your logged plot <a href=/mlflow/#/metric/training_auc?runs=[\"4746a1aa947e\"]&experiment=4&plot_metric_keys=[\"training_logloss\",\"validation_logloss\",\"training_rmse\",\"validation_rmse\"]>here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.core.display import HTML\n",
    "#Plot and Log Scoring history\n",
    "gbm_baseline.plot()\n",
    "print(\"AUC on Validation Data: \" + str(round(gbm_baseline.auc(valid = True), 3)))\n",
    "# Log training and validation metrics over time\n",
    "for step, row in gbm_baseline.scoring_history().iterrows():\n",
    "    row_dict = row.to_dict()\n",
    "    for r in row_dict:\n",
    "        if 'train' in r or 'valid' in r:\n",
    "            mlflow.log_metric(r, row_dict[r],step=step)\n",
    "\n",
    "cur_run = mlflow.current_run_id()\n",
    "cur_exp = mlflow.current_exp_id()\n",
    "link = f'/mlflow/#/metric/training_auc?runs=[\"{cur_run}\"]&experiment={cur_exp}&plot_metric_keys=[\"training_logloss\",\\\"validation_logloss\\\",\\\"training_rmse\\\",\\\"validation_rmse\\\"]'\n",
    "HTML(f'See your metrics plot <a href={link}>here</a>')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.46405678657606786: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>Error</th>\n",
       "      <th>Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>708.0</td>\n",
       "      <td>2671.0</td>\n",
       "      <td>0.7905</td>\n",
       "      <td>(2671.0/3379.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>198.0</td>\n",
       "      <td>11467.0</td>\n",
       "      <td>0.017</td>\n",
       "      <td>(198.0/11665.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Total</td>\n",
       "      <td>906.0</td>\n",
       "      <td>14138.0</td>\n",
       "      <td>0.1907</td>\n",
       "      <td>(2869.0/15044.0)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0        1   Error               Rate\n",
       "0      0  708.0   2671.0  0.7905    (2671.0/3379.0)\n",
       "1      1  198.0  11467.0   0.017    (198.0/11665.0)\n",
       "2  Total  906.0  14138.0  0.1907   (2869.0/15044.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Print and Log Confusion Matrix\n",
    "print(gbm_baseline.confusion_matrix(valid = True))\n",
    "mlflow.lm('fpr', gbm_baseline.fpr(valid=True)[0][0])\n",
    "mlflow.lm('tpr', gbm_baseline.tpr(valid=True)[0][0])\n",
    "mlflow.lm('fnr', gbm_baseline.fnr(valid=True)[0][0])\n",
    "mlflow.lm('tnr', gbm_baseline.fnr(valid=True)[0][0])\n",
    "mlflow.lm('F0point5', gbm_baseline.F0point5(valid=True)[0][1])\n",
    "mlflow.lm('F1', gbm_baseline.F1(valid=True)[0][1])\n",
    "mlflow.lm('F2', gbm_baseline.F2(valid=True)[0][1])\n",
    "mlflow.lm('auc', gbm_baseline.auc(valid = True))\n",
    "mlflow.lp('threshold', gbm_baseline.F1(valid=True)[0][0]) # First element is the threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot and Log Variable Importance\n",
    "gbm_baseline.varimp_plot()\n",
    "for var in gbm_baseline.varimp():\n",
    "    mlflow.lm(f'varimp_{var[0]}',var[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PartialDependencePlot progress: |█████████████████████████████████████████| 100%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAf4AAALWCAYAAACnePHjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nOzdeZwcdZ3/8dcnN0kI4QgJAcIZgsACAnK7BgUNKIsi6wK6Cor8dAXX3yqrK7iyXuutCCgiKuLFDzUguiCwuiMosALKKSABxCRMuJnJnUzy/f1RPU7P3T3pnu6uej0fj3708a2u/vS3a+bdVV3fqkgpIUmSimFMowuQJEmjx+CXJKlADH5JkgrE4JckqUAMfkmSCsTglySpQAx+bbKI+HBEXFbhtJdHxCfqXdNoiIg/R8TRja6jEnnq96FExM4RkSJiXIXTvyEiFkfEioh46TDTzo+IJbWpVGocg78ASgG1uvTP7amI+HZETB3hvPr980spfSqldEYN6jwtIjaU6lwREY+Xat1jU+ddBBHRFhFrSn33bEQsjIjtRjCfFBG7D9He93NaEREXbVr1f533+RHxvWpr2gSfB85KKU1NKf2hDvOvibIvNP/V5/HvRcT5DSqrKnX8DFUlg784jk8pTQUOAF4GnFftDCpdi9pEt5Xq3AI4GlgN3BUR+4zCa+fBWaX+2wOYDnypTq9zWyksuy9nVTuDUVqehrMT8ECji6jCoRFxRKOL6Kven2WTLCu5YfAXTEppKXA9sA9ARJweEQ9GxPKIeCwi/k/3tN1r9xHxwYhYBvyw9NzZZWt6s/uupUXEjyJiWUR0RMTNEbH3COrckFJ6NKX0T8CvgfPL5n9oRNwaES9GxD0RMb+srS0i/jMifld6/Z9GxFZVPPfjEfHbUn/cGBHblLX/Y0Q8ERHPRcS55fVGxJiI+FBEPFpqv6r7dcvW1t4WEX8prY2fW/bcsaWfSx4tve5dEbFjqW3PiLgpIp6PiIcj4k0V9t/zwE8ofc59RcQ7I2JRab7XRsTs0uM3lya5p/T5/kMlr1c23y0i4oqIeKbUV+dFxJhS22mlvv1SRDxP2Wda5WsM2tcDTDvg8hAREyNiBTC29F4fLU3fa600hviJJLItaR+IiHtL8/5/ETGprP11EXF3aVm7NSL2LWv7YEQsLX3eD0fEq0qPHxwRd0ZEZ2Rb577Y52U/CwxWz2kR8Zs+j/31/ZTey1cj4vrSZ/vbiJgVEV+OiBci4qEo+7kjsr/tn5Q+y8cj4r1lbedHxI8j2+LQCZxWqv220vttj4iLImJCafoBl6vBlsOy2t8TEY8Ajwz0njUyBn/BlALlOKB7s+bTwOuAacDpwJci4oCyp8wCtiJbM3orcCzwZNma3pMDvMz1wFxgW+D3wPc3seyFwMtL9W8P/BfZP7+tgA8AP4mIGWXTvxV4OzAb6AK+UsVzTyXrh22BCaVpiIi9gK8B/1ia79bADmXPey/weuAVpfYXgIv7vI8jgXnAq4B/j4iXlB7/F+AUss9lWqn2VRExBbgJ+EGpnlOAr0YFX6Qi+8LyRno+5/K2VwL/CbwJ2A54ArgSIKX0t6XJ9it9vv9vuNfq40KyrTW7kvXFW8n6s9shwGOl9/PJKufdrZK+LtdveUgprS1tGYHsve42wlreBCwAdgH2BU4DKP0NfQv4P2TLyteBa0tfOOYBZwEvSyltDrwG+HNpfhcAF6SUpgG7AVf1eb2LgT1i5PuWvIlsa982wFrgNrK/0W2AHwNfLNU/BvgZcA+wPdky+76IeE3ZvE4oPWc62d/4BuD/luZ1WOk5/wQDL1dDLYdlXk+2zOw1wvergaSUvOT8QvZPZQXwItkf11eBzQaZ9hrgn0u35wPrgEll7fOBJX2ecz7wvUHmNx1IwBal+5cDnxhk2tOA3wzw+AJgfen2B4Hv9mm/AXhb6XYb8Omytr1K72Fshc89r6ztn4BflG7/O3BlWduU0nyPLt1/EHhVWft2wHpgHLBzqQ92KGv/HXBy6fbDwAkDvO9/AG7p89jXgY8O0n9twKrS57yU7J/xjL79DnwT+GzZ86aWat25dD8Buw+xPJ1GFqAvll0OLfXxWmCvsmn/D9BW9ry/DLOsnl/q1xf7XP5aU4V9PW645WGg9zrA/fJ+m0/Zsk/2d/WWsvufBS4p3f4a8PE+7+1hsi8ru5N94T4aGN9nmpuB/wC26fP4X98X2XJ5e+nx7wHnD/b306ffLge+UdZ2NvBg2f2/AV4s3T6k72cF/Bvw7bLP6eZhPsv3AVcP0beVLIevHOo1vIzs4hp/cbw+pTQ9pbRTSumfUkqrASLi2Ii4vbSp7UWytc5typ73TEppTaUvEtlm60+XNsN20rMms80QTxvO9sDzpds7AX9f2pz4YqnmI8n++XdbXHb7CWB86fUree6ysturyP4ZQba2+Nf5ppRWAs+VTbsTcHXZfB8kWwOaWcG8dwQeHeB97wQc0qfeN5NthRnMe0uf8/YppTenlJ4ZYJrZZP3S/V5WlN7L9kPMt6/bS6/TfbmdrI8nlM+7dLt8vuWfzWCu6jPv6X3aK+nrcoMtD7Uw2Ge6E/D+Pp/djsDslNIislA8H3g6Iq4s28T9DrL9Mx6KiDsi4nUDvOY3gJkRcfwI6n2q7PbqAe6X1z+7T/0fpncf9/osI2KPiPh5ZD/zdQKfYuh+rmQ5rGR5UZUM/gKLiIlkvwN/HphZ+gd7HRBlk/U9feNwp3M8lWwT4NFkm3x37n65TSj1DcAtpduLydbay4NhSkrp02XT71h2ew7ZWsSzFT53MO3l842IyWSbcLstBo7tM+9JKdunYjiLyTbrDvT4r/vMc2pK6d0VzHMoT5L9Yweg9JPC1mRbCTbFs2R9vVPZY3P6zLcWpwOttq8HWx4GsgqYXHZ/qC9Zw9X4yT41Tk4p/RAgpfSDlNKRZH2VgM+UHn8kpXQK2U8hnwF+XPp8/iqltJ5sq8DH6f13tbK89ogYae3d9T/ep/7NU0rHlZfS5zlfAx4C5qbsp4oPM/TffSXLoaePrQODv9gmABOBZ4CuiDgWePUwz3kK2DoithikfXOyzb3Pkf0T+tRICittOdglIi4k28T6H6Wm7wHHR8RrStNMimwnxPLf298SEXuVwvljwI9TShsqfO5gfgy8LiKOLO2w9DF6//1cAnwyInYq1T8jIk6o8O1eBnw8IuZGZt+I2Br4Odnvuf8YEeNLl5eV7RswUj8ATo+I/Utf/j4F/G9K6c+l9qfIfqOvSqmPryLrh81LffEvZP1eS9X29WDLw0DuBk4tLR8LyDbNj8Q3gHdFxCGlz3RKRLy21C/zIuKVpb5fQ7amvaH0Xt4SETNSShvJfuKgu62P75L97S4oe+weYO/S5zqJEe48WfI7oDOynRA3K/XHPhHxsiGesznQCayIiD2Bvl9Q+y5Xwy2HqhODv8BSSsvJdpS6imwHqVOBa4d5zkNke/c/VtoEOLvPJFeQbb5bCvwRuL3Ksg6LbG/rTrLfZ6eR7QR1X+n1F5NtUfgw2ReWxcA59F6Wv0v2e+YyYFLpPVb63AGllB4A3kP2z6qdrL/Kj2dwAVnf3RgRy0vv+5AK3/MXyT6DG0vv+5tk+2AsJ/sidjLZ2tEysrXAiRXOd7D38kvgI2Rbe9rJtjacXDbJ+cB3Sp9vRaMIypxNtub5GPAbsv761qbUO4Bq+3rA5WEQ/wwcTxa6bybb56VqKaU7gXcCF5EtK4so7fhH9vl9mmyrwzKytfsPl9oWAA+U/gYuINsPpN9PbaUvLh8l20m1+7E/kX2x+W+yveB/0/d5VdS/gawf9gceL9V6GdlWvMF8gOx/yHKyLz59dww9n7LlqoLlUHUSKbklRfkREW1kOxpWdCRB5ZvLg9Sfa/ySJBWIwS9JUoG4qV+SpAJxjV+SpAIx+CVJKpCWO+PR9OnT0+67e2bHTbVy5UqmTJky/IQakv1YO/ZlbdiPtdHq/XjXXXc9m1KaMVBbywX/zJkzufPOOxtdRstra2tj/vz5jS6j5dmPtWNf1ob9WBut3o8R8cRgbW7qlySpQAx+SZIKxOCXJKlADH5JkgrE4JckqUAMfkmSCsTglySpQAx+SZIKxOCXJKlADH5JkgrE4JckqUAMfkmSCsTglySpQAx+SZIKxOCXJKlADH5JkgrE4JckqUAMfkmSCsTglySpQAx+SZIKxOCXJKlADH5JkgrE4JckqUAMfkmSCsTglySpQAx+SZIKxOCXJKlADH5JkgrE4JckqUAMfkmSCsTglySpQAx+SZIKxOCXJKlADH5JkgrE4JckqUAMfkmSCsTglySpQAx+SZIKxOCXJKlADH5JkgrE4JckqUAMfkmSCsTglySpQAx+SZIKxOCXJKlADH5JkgrE4JckqUAMfkmSCsTglySpQAx+SZIKxOCXJKlADH5JkgrE4JckqUAMfkmSCsTglySpQOoW/BHxrYh4OiLuH6Q9IuIrEbEoIu6NiAPqVYskScrUc43/cmDBEO3HAnNLlzOBr9WxFkmSRB2DP6V0M/D8EJOcAFyRMrcD0yNiu3rVI0mSYFwDX3t7YHHZ/SWlx9r7ThgRZ5JtFWDGjBm0tbWNRn25tmLFCvuxBuzH2rEva8N+rI0892Mjgz8GeCwNNGFK6VLgUoB58+al+fPn17GsYmhra8N+3HT2Y+3Yl7VhP9ZGnvuxkXv1LwF2LLu/A/Bkg2qRJKkQGhn81wJvLe3dfyjQkVLqt5lfkiTVTt029UfED4H5wDYRsQT4KDAeIKV0CXAdcBywCFgFnF6vWiRJUqZuwZ9SOmWY9gS8p16vL0mS+vPIfZIkFYjBL0lSgRj8kiQViMEvSVKBGPySJBWIwS9JUoEY/JIkFYjBL0lSgRj8kiQViMEvSVKBGPySJBWIwS9JUoEY/JIkFYjBL0lSgRj8kiQViMEvSVKBGPySJBWIwS9JUoEY/JIkFYjB38fKlbB2baOrkCSpPgz+Pi64ACZNgq23hn32gWOOgbe9DVasyNoffBB++1t47DFYvbqxtUqSVK1xjS6g2Rx1FHz849Denl2efBJuvjn7MgBw4YXwta/1TD99OsyZA3ffDRGwcCH85S8wezZst13P9eTJjXk/kiSVM/hLOjuhrS1bwz/ssMGnO+ccOOGE7AtB9xeDNWuy0Af47nfhmmt6P2eHHWDx4uz2hz4Ed93Vu32PPeDii7Pb7343LFrUu32//eDzn89un3YaLF3au/3QQ7MvKwBvehO8+GJWz5gx2fVRR2V1A5x6KqxbB88+uxczZ2bTHH00vOMdkBKcfnr2WPnlmGPgjW/M3ue//mvPe+127LGwYEH2uh/9aP8+e/3rsxqeego+9an+7f/wD3D44fDEE/DFL/Zvf9vb4IAD4JFH4KKL+refeSbsvTfcdx9cdln/9rPPht13hzvvzD6fvj7wAdhxx2xLzlVX9W8/7zyYMQN++Uu49trebUuW7M4BB8C0aXDddXDDDf2f/9nPwsSJcPXV2TLW1wUXZNdXXgm33da7beLE7PkAl18Of/hD7/YttoCPfSy7/fWvwx//2Lt9223h3HOz21/5Cjz6aO/2OXPg/e/Pbn/uc7BkSe/23XfP+g+yZezZZ3u377MPvPOd2e3zzoPly3u3H3ggvPWt2e1zzsmWvXKHH559/inBhRfuztVX925/5Suzv7c1a+CDH6SfIi97kH0mfZe9JUt6+tFlL7s93LL3vvfRz4wZWzN/fusue0Mx+Et+/nN485vh1luHDv5ddskug1m4EF54ofcXg5R62p99Flat6v2cNWt63+7bXr7PwerV/dvLF+jVq7OfJTZuzF5340bo6OhpX7Qoe/6KFVN46qmsfe7crG3jRvif/+l5Xvdlu+2y4F+7duB/XrNnZ38Aq1bBFVf0b99jj+wPoKNj4PYDDsj+AJ57buD2v/3bbJplywZuP+647A9gyZKB29/0puyfyOOPD9z+9rdn/3wffnjg9ve+N/vn+8AD/du7umayenX2z/eeewZ+/qc+lf0TvfPOgdu7//nedlv/9s037/nne8st2fJVbrvtev75/upXcOONvdvnzu3553vDDdnyXe6lL+355/vzn8O99/ZuP/LInn++11yT/cRVbsGCnn++P/oRPP107/ZVq3r++f7gB/2X3bFjs3+AADfeOJNxff4jbbFFFvzr1g3cd0Ve9gA+/OH+y15XV08/uuxlt4db9gbqm+OP3xxo3WVvKJHKU6kFzJs3Lz388MM1n+9JJ2UL5pIl2Vpu3rW1tTF//vxGl9Hy7MfasS9rw36sjVbvx4i4K6V00EBtBYi44a1aBddfD294QzFCX5JUXMYc2SaqVavgxBMbXYkkSfVl8JPtOLPlltlvKpIk5ZnBT7bH6R/+AOPHN7oSSZLqy+AnG562006NrkKSpPorfPCfd17PkBJJkvKu0MG/YUN24IPug+tIkpR3hQ7+22/Pjqrk3vySpKIodPAvXAgTJmRHQJIkqQgKG/wpZcevPuaY7JCXkiQVQWGP1b96NbzmNdlJQCRJKorCBv/kyb1PrytJUhEUdlP/Pff0PmueJElFUMjg/9OfYP/94dJLG12JJEmjq5DBf/XV2bV780uSiqaQwb9wIbzsZbDjjo2uRJKk0VW44F+8GH73Ow/aI0kqpsIF/09/ml0b/JKkIirccL4zzoA994Q99mh0JZIkjb7CrfFPmgRHH93oKiRJaoxCBf9Pfwof/SisXdvoSiRJaoxCBf+ll8IVV2Qn5pEkqYgKE/ydnfDf/53t1BfR6GokSWqMwgT/ddfBunXuzS9JKrbCBP/ChTBrFhx2WKMrkSSpcQoT/BMmwCmnwJjCvGNJkvorzDj+732v0RVIktR4hVj/7ehodAWSJDWH3Af/+vWw667wkY80uhJJkhov98F/883w/PNw4IGNrkSSpMbLffAvXAiTJ8OrX93oSiRJarxcB//GjXD11XDssVn4S5JUdLkO/t/9DtrbPWiPJEndch38e+wB3/gGvPa1ja5EkqTmkOtx/FttBWec0egqJElqHrld43/kEfja17KT80iSpExug//734f3vAfWrGl0JZIkNY/cBv/ChfDyl8O22za6EkmSmkcug3/RIrjvPnjDGxpdiSRJzSWXwX/11dm1wS9JUm+5DP5HHoGDDoKddmp0JZIkNZdcDue79FJ36pMkaSC5W+NPKbueNKmxdUiS1IxyF/yvfS2cfXajq5AkqTnlKviffRZuvBGmTWt0JZIkNadcBf/PfgYbNnhSHkmSBpOr4L/6apgzBw44oNGVSJLUnHIT/MuXZ5v5TzwRIhpdjSRJzSk3w/m6uuC88+DYYxtdiSRJzSs3wb/lllnwS5KkweViU/+aNdnv+6tWNboSSZKaWy6C/5e/zH7bv/nmRlciSVJzy0XwL1yYjd1/5SsbXYkkSc2t5YO/qwt++lM4/niYMKHR1UiS1NxaPvhvuQWee86D9kiSVIm67tUfEQuAC4CxwGUppU/3ad8S+BawG7AGeHtK6f5qXuOXv4TNNoPXvKZGRRdJStmhDru6YP363tdD3e4+E5KY9sADMHFio8vIBfuyNuzH2shzP0aq0z/xiBgL/Ak4BlgC3AGcklL6Y9k0nwNWpJT+IyL2BC5OKb1qqPnOmzcvPfzww3+9nxI8/jjsums93kWVUoKODli2rP+lvT27fuYZ2LixvjVs2DBsgG9cv54xGzbUrw5JUsME3JVSOmigtnqu8R8MLEopPQYQEVcCJwB/LJtmL+A/AVJKD0XEzhExM6X0VKUvEjEKob927cBhPlCwr13b//kTJsCsWdll9mwYV+fDJ4wbl13Gjx/09uKlS9lpt92GnKbf7fHjYexYGNPyvxDVzD333MN+++3X6DJywb6sDfuxNlq+HxcsGLSpngm0PbC47P4S4JA+09wDnAj8JiIOBnYCdgAqCv7PfAaeeAIuvrhGh+l9+mn4/OdhyZLeof7CCwNPP2NGT6DPndtze9Ys2G67ntvTpzfdcYQfb2tjp/nzG11Gy3th4kSwH2vCvqwN+7E28tyP9Qz+gZKu7+8KnwYuiIi7gfuAPwBd/WYUcSZwJsCMGTNoa2sjJbjwwoPZbrs1/PrX99ak4O2vvpq5X/kKq2fPZu3WW7Nu221Zt+eerNtqK9ZtuWV2Xbqs33JL0nBr7k89lV2a0IoVK2hra2t0GS3Pfqwd+7I27MfayHM/1jP4lwA7lt3fAXiyfIKUUidwOkBEBPB46UKf6S4FLoXsN/758+dz//2wdCl85COTmV+rb2U33gjjxrHZ4sVslvPN2W1tbbXrtwKzH2vHvqwN+7E28tyP9Uy3O4C5EbFLREwATgauLZ8gIqaX2gDOAG4ufRkY1sKF2dbzE06oYcXt7dmm+ZyHviSpuOq2xp9S6oqIs4AbyIbzfSul9EBEvKvUfgnwEuCKiNhAttPfOyqd/8KFcPjhWU7XTHt79tu8JEk5Vdfdy1NK1wHX9XnskrLbtwFzq51vVxcceSQccMCm19hLezvsvHONZypJUvNoydPyjhsHF11Uhxm3t8Nhh9VhxpIkNYeW/DH7vvvqcAyc9euzg+u4qV+SlGMtF/xdXcG++8IXvlDjGXcPuzP4JUk51nLBv2JF9uvEccfVeMbt7dm1wS9JyrGWDP499oC99qrxjA1+SVIBtFzwr1o1jhNPrMMRcA1+SVIBtFzwA5x4Yh1m2t6efZuYObMOM5ckqTm0XPBvv/1qDhrwRIObqL09O+lOvc+cJ0lSA7Vc8E+Z0lWfE9151D5JUgG0XPDXjcEvSSoAg7+bwS9JKgCDH7LDAD71lMEvSco9gx/g2WezM/8Y/JKknDP4wTH8kqTCMPjB4JckFYbBDwa/JKkwDH4w+CVJhWHwQxb806fDpEmNrkSSpLoy+MEx/JKkwjD4AZ580uCXJBWCwQ+u8UuSCsPgT8nglyQVhsH/4ouwdq3BL0kqBIPfoXySpAIx+A1+SVKBGPwGvySpQAx+g1+SVCAGf3s7TJkCm2/e6EokSao7g797KF9EoyuRJKnuDH7H8EuSCsTgN/glSQVi8Bv8kqQCKXbwr1wJy5cb/JKkwih28DuUT5JUMAY/GPySpMIw+MHglyQVhsEPBr8kqTAM/gkTYKutGl2JJEmjwuCfNcuj9kmSCsPgdzO/JKlADH6DX5JUIAa/wS9JKpDiBv+6dfDccwa/JKlQihv8y5Zl1wa/JKlAihv8juGXJBWQwW/wS5IKxOA3+CVJBVLs4B8zBrbdttGVSJI0aood/NtuC2PHNroSSZJGTbGD3838kqSCGdfoAhqmAMG/YQOsWpVdJk2CLbaANWvgttvg9tu35umnYeXKrP3ww+GlL8265ROfyB7rblu5Et7/fnjd6+D3v4c3vKH/a114Ifzd38Fvfwunntq//dvfhle+Em66Cc44o3/7VVfBIYfANdfAP/9z//b/+i/YZx/43vfg3HP7t7e1wS67wNe/Dp/6VP/2O+7INvB84Qvwla/0b3/wQZg8GT7+cbjsst5tEfDnP2e3P/hBuPLKnra1aw9l663hgQey+2edBT/7We/nz5wJv/tddvu00+B//qd3+267wa9+ld3++7/vmbbbvvv2zPO443peq9vhh8MPf5jdfsUremrtdvTR8M1vZrcPOgieeaZ3+wkn9PTJnnvC6tW929/85qxPN27M+rivM8/MPpPly7PPqK9/+ZfsM122LPuM+zrvPHjnO2Hp0s3Yaaf+7Z/+NJxyCtxzT7aM9VXUZQ9g6tT+y97atYcycWL2mMteZcveokXwqlf1blu79lC+9KXWXvYGU+zgf+lLR+Wlnn8eFi+G/fbL7r/sZT37FnZ73evgkkuy23vtBZ2dvdtPPhk+//ns9k47ZaFe7h3vgP/4D1ixAubMyQJ77dqe9n//96z9hReyBRH+ptfzP/OZrDtWr84WxsmTYcqUnuuUsum23rr7+b3NnJldb7XVwO1bb51db7PNwO3Tp/fMZ6D2qVOz6+23H7h98uTses6cgdu7/xHuuuvA7d2/+Oy+e//28nM47bln7/Zly15g5517vkDuvXf2Ralc93uDbBnoe06oWbN6bh9wQM977bbzzj23Dz64p6/La+p26KHZeyy37749t484ov+ytffePbdf8Yrs2Fbl5s3ruT1Q3+2+e3Y9btzA7d31T5w4cPucOdn1pEkbBmyfPTu7njbNZa9v+6RJPbe7l71ly15g1qzter03cNkbatmbMqV/+7JlLzB7dtaPrbjsXX55/+m7Rer+j94i5s2blx5++OFNm8mGDdnpeD/84exrdo1t3Ah33w3XXw/XXQe3356F+X33Ze3nnJMFcLmDDoJ3vSu7ffbZ/b/5Hnlk9o0dsm+5Gzf2bj/66OzLwYYN8H//b7ZAlIf3y16Wvcb69XDrrfDQQ3fx8pcf+Nf2adN6/kGpcm1tbcyfP7/RZeSCfVkb9mNttHo/RsRdKaWDBmor5hr/M89kyVnDTf3Ll8Pmm2e3zzyz9+atc8/NNpN1+9znhp7XhRcO3X7ppYO3jR078ObEbuPHZ9+sU1rOXnsN/TqSpPwpZvDXYAx/StkafPda/a23wp/+lP3e8pa3wMtfDgsW9N80JklSIxn8I/C//wtvfCMsXZrd339/+Nd/zX49AGjhrUOSpJwz+IeQUrbHbfda/d//ffY7/K67Zntivva12Vp9985HkiQ1u2IHf/kurWVSgve+NxvG8sQT2WP77NOz89uMGfCTn4xCnZIk1Vhxg3+rrQbdjT0i+71+//2zHf+PPRZ23HGUa5QkqQ6KG/wDbOa/+mr4wQ/gO9+BX/yi/5hXSZJaXTEP2TtI8N97L/z4x9lOeoa+JCmPDP4ynZ3ZAW/GFXM7iCSpAIoX/CkNGvwdHdkR7CRJyqviBf/zz2cHhB5kjX+LLRpQkyRJo6R4wT/EGP4ttoC5c0e5HkmSRlHxfs0eIvi/8Y1RrkWSpFHmGr8kSQVi8Jd5/euHPrOdJEmtrpjBP3Vqdunjppt6DtErSVIeFTP4B1jbX78eVq1yr35JUr4Z/CXLl2fXjuOXJOWZwV/S0ZFdu8YvScozg79k40Y44ADYfvsG1PVG5kQAACAASURBVCRJ0igp1jj+5cth5coBg3+33eCuuxpQkyRJo6hYa/yO4ZckFZzBX3LttXDggbB06SjXJEnSKDL4SxYvht//HsaPH+WaJEkaRQZ/iXv1S5KKoHjBP3EibLllv6bOTpgwIWuWJCmvihf8s2ZBRL+mjg7X9iVJ+Ve84B9kj/5dd4VXvWqU65EkaZTVNfgjYkFEPBwRiyLiQwO0bxERP4uIeyLigYg4vZ71DBX855wDP/xhXV9dkqSGq1vwR8RY4GLgWGAv4JSI2KvPZO8B/phS2g+YD3whIibUq6ahgl+SpCKo5xr/wcCilNJjKaV1wJXACX2mScDmERHAVOB5oKsu1axZAy+8ALNnD9j86lfD2WfX5ZUlSWoa9Txk7/bA4rL7S4BD+kxzEXAt8CSwOfAPKaWNfWcUEWcCZwLMmDGDtra2qouZtGwZhwIPdXSwbIDn33vvoYwZ8yJtbQ9VPe9WtGLFihH1o3qzH2vHvqwN+7E28tyP9Qz+/rvOZ2v45V4D3A28EtgNuCkibkkpdfZ6UkqXApcCzJs3L82fP7/6am67DYA9jzqKPQd4/po1sMces5g/f1b1825BbW1tjKgf1Yv9WDv2ZW3Yj7WR536s56b+JcCOZfd3IFuzL3c6sDBlFgGPA3vWpZohDt6TUjaO3+F8kqS8q2fw3wHMjYhdSjvsnUy2Wb/cX4BXAUTETGAe8Fhdqhki+FesyMLf4Jck5V3dNvWnlLoi4izgBmAs8K2U0gMR8a5S+yXAx4HLI+I+sp8GPphSerYuBbW3w9ixMGNGv6YNG+Ckk2DvvevyypIkNY16/sZPSuk64Lo+j11SdvtJ4NX1rOGv2tth5kwY038jx/Tp8KMfjUoVkiQ1VHGO3OcYfkmSDH6Am26CrbbKTssrSVKeGfzA889nx/aZNGmUa5IkaZQVI/i7uuDppwcN/s7SUQOmTRvFmiRJaoBiBP/TT2fj9QYJ/o6O7NrhfJKkvCtG8A8xhh+y4B8zBqZOHcWaJElqAIMf2G8/OOMMiIEOMixJUo7UdRx/0xgm+E86KbtIkpR3xVrjnzlzwOYNG0axFkmSGqg4wb/NNjBhwoDNRx8Nr3zlKNckSVIDFCf4hzhqX0cHTJ48ivVIktQgBj/ZOH7H8EuSisDgJ1vjdwy/JKkI8h/8KcGyZYMGf0oGvySpOPIf/M89B+vXDxr8GzfC+94H8+ePblmSJDVC/sfxDzOGf+xY+OxnR7EeSZIaKP9r/MME//r18OKL2Zq/JEl5l//gf/LJ7HqQ4L/7bthyS7juulGsSZKkBsl/8Fdwgh5wOJ8kqRiKEfzTpg16hJ7OzuzavfolSUVQjOAfZgw/GPySpGIw+N3UL0kqkMIH/yGHwEc/avBLkooh3+P4Uxo2+A87LLtIklQE+V7j7+yE1auHDP5ly+Cpp0axJkmSGijfwT/MUD6As8+Go44apXokSWqwwge/p+SVJBVJ4YPfM/NJkorE4Df4JUkFkv/gnzRpyGQ3+CVJRZLv4XzdQ/kiBp3kE5+AnXcevZIkSWqkYgT/EN7+9lGqRZKkJpD/Tf1DBP+6dfCHP8CLL45iTZIkNVChg3/pUjjgALjmmlGsSZKkBspv8K9ene25N8wYfnAcvySpOPIb/BUO5QP36pckFYfBj8EvSSoOgx+DX5JUHIUO/iOOgO98B2bPHqWaJElqsPyO429vh3HjYJttBp1kl12yiyRJRZHvNf6ZM2HM4G/x4Yfh9ttHsSZJkhos38E/zFH7vvxlOP74UapHkqQmUOjg9wQ9kqSiMfgNfklSgeQz+Nevh2eeGTb4OzsNfklSseQz+J96Krt2jV+SpF7yOZyvgjH8ABdfDBMnjkI9kiQ1iUIH/8tfPgq1SJLURPK5qb+C4N+4EX7yE3j00VGqSZKkJpDf4I/IDuAziJUr4aSTYOHCUaxLkqQGqyj4I2KziJhX72Jqpr0dZsyA8eMHncQT9EiSimjY4I+I44G7gV+U7u8fEdfWu7BNUuEYfjD4JUnFUska//nAwcCLACmlu4Gd61dSDRj8kiQNqJLg70opddS9kloy+CVJGlAlw/nuj4hTgbERMRd4L3BrfcvaBBs3ZgfwGSb4Dz0Ubr4Z9t57lOqSJKkJVLLGfzawN7AW+CHQCbyvnkVtkmefha6uYYN/yy2zcfzTpo1SXZIkNYFh1/hTSquAc4FzI2IsMCWltKbulY1UhQfv+cMf4P774dRTYezYUahLkqQmUMle/T+IiGkRMQV4AHg4Is6pf2kjVGHwL1wIp50GY/J5JANJkgZUSeztlVLqBF4PXAfMAf6xrlVtigqDv6Mj28wfMQo1SZLUJCoJ/vERMZ4s+H+aUloPpPqWtQmqCH736JckFU0lwf914M/AFODmiNiJbAe/5tTeDtOnw6RJQ07WvcYvSVKRVLJz31eAr5Q99EREHFW/kjZRBWP4wTV+SVIxDRv8ETEReCPZ0frKp/9YnWraNE8+WVHwf+c7sHbtKNQjSVITqeQAPj8FOoC7yMbyN7f2djjiiGEnmzNnFGqRJKnJVBL8O6SUFtS9klpIqeJN/RddBPvvD0ceOQp1SZLUJCrZue/WiPibuldSCy++mG2/ryD43/9++PnPR6EmSZKaSCVr/EcCp0XE42Sb+gNIKaV961rZSFQ4lG/NGli3zp37JEnFU0nwH1v3KmqlijH8YPBLkopn2E39KaUngOnA8aXL9NJjzcfglyRpSJUcq/+fge8D25Yu34uIs+td2IhUGfwewEeSVDSVbOp/B3BISmklQER8BrgNuLCehY1IeztMngybbz7kZPvvD4sXw1ZbjVJdkiQ1iUqCP4ANZfc3lB5rPt1D+YY588748bDDDqNUkyRJTaSS4P828L8RcTVZ4J8AfLOuVY1UhWP477gDrr8e3vc+N/dLkoqlkp37vgicDjwPPAecnlL6cr0LG5EKg//WW+GjH4WurlGoSZKkJlLJAXy6BdnpeJtzMz9UdYIecG1fklQ8lezV/+/Ad4AtgW2Ab0fEefUurGorV8Ly5RUH/5QpMK6SHzokScqRSqLvFOClKaU1ABHxaeD3wCfqWVjVKhzKB56SV5JUXJVs6v8zMKns/kTg0bpUsymqCP7OTjfzS5KKqZI1/rXAAxFxE9lv/McAv4mIrwCklN5bx/oqV0Xw/+AHsHp1neuRJKkJVRL8V5cu3drqU8omqiL4x40b9hg/kiTl0rDBn1L6TvftiNgS2DGldG8lM4+IBcAFwFjgspTSp/u0nwO8uayWlwAzUkrPV1Z+mfb27Mg8W2897KQf+xjsvjucemrVryJJUkurZK/+toiYFhFbAfeQ7dX/xQqeNxa4mOzsfnsBp0TEXuXTpJQ+l1LaP6W0P/BvwK9HFPqQBf+sWcMetQ/gkkvgV78a0atIktTSKtm5b4uUUidwIvDtlNKBwNEVPO9gYFFK6bGU0jrgSrKj/g3mFOCHFcx3YBWO4Qf36pckFVclwT8uIrYD3gT8vIp5bw8sLru/pPRYPxExGVgA/KSK+fdWYfB3dcGqVQa/JKmYKtm572PADcBvU0p3RMSuwCMVPG+gbe5pkGmPL81/wM38EXEmcCbAjBkzaGtr6zfNEX/5C0/vuiuPDNBWrrNzHHAkTz/9CG1tS4ecNs9WrFgxYD+qOvZj7diXtWE/1kae+7GSnft+BPyo7P5jwBsrmPcSYMey+zsATw4y7ckMsZk/pXQpcCnAvHnz0vz583tPsG4ddHay/YEHsn3ftj7+8pdsH8ADD5zL/Plzh3kL+dXW1ka/flTV7MfasS9rw36sjTz347DBHxF7AF8DZqaU9omIfYG/SykNd+S+O4C5EbELsJQs3PvtRx8RWwCvAN5SbfF/tWxZdl3Bpv45c7LvCRs3jvjVJElqWZX8xv8Nsj3u1wOUhvKdPNyTUkpdwFlkPxM8CFyVUnogIt4VEe8qm/QNwI0ppZXVFv9XVYzh7zammtMTSZKUE5XE3+SU0u/6PFbRCW1TStellPZIKe2WUvpk6bFLUkqXlE1zeUpp2C8SQ6oi+O+8E04/PdvkL0lS0VQS/M9GxG6UdsyLiJOA9rpWVa0qgv+hh+Dyy2HNmvqWJElSM6pkr/73kO1Yt2dELAUep+doe82hvT07cM+22w47aUdHdu1wPklSEQ0Z/BExBjgopXR0REwBxqSUlo9OaVVob89Cf9zw32MMfklSkQ25qT+ltJFsBz1SSiubMvSh6qP2jR8PEyfWuSZJkppQJb/x3xQRH4iIHSNiq+5L3SurRhXBP3ZsNqSvgkP6S5KUO5UE/9vJfue/GbirdLmznkVVrYrg/9SnYNGiOtcjSVKTquTIfbsM1R4Rx6SUbqpdSVXasAGeeqqqMfySJBVVLQ5j85kazGPknnkmOwxfhcH/gQ/Axz9e55okSWpSlQznG05jfy2v8qh9N94Iu+1Wx3okSWpitVjjH+yMe6OjyuDv6IBp0+pYjyRJTaz1j1g/guB3DL8kqahqEfx/rsE8Rq47+GfNGnbSlKCz0+CXJBVXRb/xR8ThwM7l06eUrihdn1iXyirV3g5bbgmTJg076Zo1sMceMHv2KNQlSVITGjb4I+K7wG7A3cCG0sMJuKKOdVWuijH8m22WnaRHkqSiqmSN/yBgr5RSY3fiG0wVwS9JUtFV8hv//cDwP6A3ShXBf++98Ld/C7//fZ1rkiSpSVWyxr8N8MeI+B2wtvvBlNLf1a2qSqVUVfA/+STccgusXTv8tJIk5VElwX9+vYsYseefh3XrKt5bz1PySpKKrpJj9f96NAoZkRGM4QeDX5JUXMP+xh8Rh0bEHRGxIiLWRcSGiOgcjeKGVWXwd5aq9sh9kqSiqmTnvouAU4BHgM2AM0qPNV6Vwb/llnDAATB1ah1rkiSpiVV05L6U0iJgbEppQ0rp28D8ulZVqSqD/x3vgLvugmjsaYUkSWqYSnbuWxURE4C7I+KzQDswpb5lVai9PVt9dxVekqSKVLLG/4+l6c4CVgI7Am+sZ1EVq/LgPWedBaeeWsd6JElqcpXs1f9ERGwGbJdS+o9RqKlyVQb/H/8I69fXsR5JkppcJXv1H092nP5flO7vHxHX1ruwilQZ/J6SV5JUdJVs6j8fOBh4ESCldDfZmfoabwTB71A+SVKRVRL8XSmljrpXUq3ly2HlyqqCv7PTNX5JUrFVslf//RFxKjA2IuYC7wVurW9ZFahyKB/A4YfDvvvWqR5JklpAJcF/NnAu2Ql6fgDcAHy8nkVVZATBf801dapFkqQWUcmm/r1Kl3HAJOAE4I56FlWREQS/JElFV0nwfx/4FnAi8LrS5fh6FlWRKoP/0Udhxx3hZz+rY02SJDW5Sjb1P5NSar64bG+HiROzA/BX4PnnYcmSOtckSVKTqyT4PxoRlwG/JPudH4CU0sK6VVWJ9naYNaviA+93n5nPvfolSUVWSfCfDuwJjAc2lh5LQOODv8ox/OA4fklSsVUS/PullP6m7pVUq70d5s2reHLX+CVJqmznvtsjYq+6V1KtKtf4t98eTjwRttqqjjVJktTkKlnjPxJ4W0Q8TvYbfwAppdS4Q+GsWQMvvFBV8B9zTHaRJKnIKgn+BXWvolrLlmXXjuGXJKkqFZ2WdzQKqcoIDt7z7nfDLbfA/ffXqSZJklpAJb/xN58RBP+zz8KGDXWqR5KkFlGY4O/ocI9+SZJaN/jHjIEZMyp+SmenY/glSWrd4J85E8aOrfgprvFLklTZXv3Np8ox/JCN4d9llzrVI0lSi2jd4N9++6qe8slP1qkWSZJaSOtu6q9ijT8l6OqqYz2SJLWI1gz+p5+uKvhfeAHGj4eLLqpjTZIktYCWC/7o6spW4UdwZr6pU+tUlCRJLaLlgn9M9zZ7T8krSVLVWi74o/vwe1UEv6fklSQp03rBvwlr/Aa/JKnoWjf4Z82q+Dk77wwf+ADssEN9apIkqVW03Dj+MV1dsPXWMGFCxc/5m7+Bz32ujkVJktQiWnONv8qj9i1fnv3On1KdipIkqUW0XvBv2FB18H/yk9n5fCLqVJQkSS2i5YJ/zAjW+D1BjyRJmZYL/pFs6veUvJIkZVou+AHX+CVJGiGDX5KkAmm54XxA1cH/zndmJ+mRJKnoChH8b31rneqQJKnFFGJT/6JFPYftlSSpyFou+NOYMTBlSuXTJ5g3Dz7/+ToWJUlSi2i54F81Z05V069YARs3unOfJEnQgsG/sYpj9EPPKXkdxy9JUgsGf7U8Ja8kST0MfkmSCiT3wb/TTnDxxbDPPo2uRJKkxmvNcfxVmD0b/umfGl2FJEnNIfdr/MuWwT33QFdXoyuRJKnxch/83/0u7L8/rF7d6EokSWq83Ad/RwdEVHXMH0mSciv3wd/ZmY3hH5P7dypJ0vByH4eekleSpB4GvyRJBZL74Xzvf3/PYXslSSq63Af/y1/e6AokSWoeud/Uf/PN8Mgjja5CkqTmkPvgf+Mb4UtfanQVkiQ1h1wHf0rZzn2ekleSpExdgz8iFkTEwxGxKCI+NMg08yPi7oh4ICJ+XcvXX7sW1q93r35JkrrVbee+iBgLXAwcAywB7oiIa1NKfyybZjrwVWBBSukvEbFtLWvoPiWva/ySJGXqucZ/MLAopfRYSmkdcCVwQp9pTgUWppT+ApBSerqWBXQHv2v8kiRlIqVUnxlHnES2Jn9G6f4/AoeklM4qm+bLwHhgb2Bz4IKU0hUDzOtM4EyAGTNmHHjVVVdVVMPq1WO5774t2GWXFcyYsW5T31KurFixgqlTpza6jJZnP9aOfVkb9mNttHo/HnXUUXellA4aqK2e4/hjgMf6fssYBxwIvArYDLgtIm5PKf2p15NSuhS4FGDevHlp/vz5FRdx7LFVVFwgbW1tVNOPGpj9WDv2ZW3Yj7WR536s56b+JcCOZfd3AJ4cYJpfpJRWppSeBW4G9qtVAU88AVdfDStW1GqOkiS1tnoG/x3A3IjYJSImACcD1/aZ5qfAyyNiXERMBg4BHqxVAb/6FZx4Ijz7bK3mKElSa6vbpv6UUldEnAXcAIwFvpVSeiAi3lVqvySl9GBE/AK4F9gIXJZSur9WNXQfo9+9+iVJytT1WP0ppeuA6/o8dkmf+58DPleP13c4nyRJveX6yH0dHTBlCozL/amIJEmqTO6D3zH8kiT1yHXwn3suXHNNo6uQJKl55Hoj+C67ZBdJkpTJ9Rr/woXwP//T6CokSWoeuQ7+c8+Fr3610VVIktQ8ch38nZ3u3CdJUrlcB7979UuS1Ftug7+rC1auNPglSSqX2+DvPlyvwS9JUo/cDuebNg0eegi23rrRlUiS1DxyG/zjxsG8eY2uQpKk5pLbTf1//jN84Qvw5JONrkSSpOaR2+C/7z74wAdg6dJGVyJJUvPIbfB3n5LXnfskSeqR2+B3r35JkvrLbfC7xi9JUn+5Dv4JE2DSpEZXIklS88ht8H/kI/Doo42uQpKk5pLbcfxTpmQXSZLUI7dr/N/6Fnzzm42uQpKk5pLb4P/2t+F732t0FZIkNZfcBr+n5JUkqT+DX5KkAjH4JUkqkFwGf0rZkfsMfkmSesvlcL4IWLMGNm5sdCWSJDWXXAY/ZEftkyRJveVyU/+SJXDWWdmpeSVJUo9cBv/ixXDxxbB0aaMrkSSpueQy+D0znyRJA8t18E+b1tg6JElqNrkOftf4JUnqLZfBv3YtjB1r8EuS1Fcug//ss2H9epg6tdGVSJLUXHI7jj+i0RVIktR8crnGf9FF8MEPNroKSZKaTy6D/5e/hOuvb3QVkiQ1n1wGv2fmkyRpYAa/JEkFYvBLklQguQz+qVNh1qxGVyFJUvPJ5XC+u+9udAWSJDWnXK7xS5KkgeUu+J9/Hl77WrjxxkZXIklS88ld8D/3HFx3HTz9dKMrkSSp+eQu+D0znyRJgzP4JUkqEINfkqQCyV3wjx8Pc+fClls2uhJJkppP7oL/+OPhT3+COXMaXYkkSc0nd8EvSZIGl7vg/9KX4LjjGl2FJEnNKXfB/8ADHrJXkqTB5C74Ozvdo1+SpMHkLvg9Ja8kSYMz+CVJKpDcnZZ3jz1g5sxGVyFJUnPKXfBfcUWjK5AkqXnlblO/JEkaXK6Cv6sLXvISuOyyRlciSVJzylXwd3bCQw/BihWNrkSSpOaUq+D3zHySJA3N4JckqUByGfzTpjW2DkmSmlWugn/qVHj1q2H27EZXIklSc8rVOP4DD4Qbbmh0FZIkNa9crfFLkqSh5Sr4v/xl2GknWLeu0ZVIktScchX8y5ZllwkTGl2JJEnNKVfB75n5JEkamsEvSVKB5Cr4Ozsdwy9J0lByNZzviCNg9epGVyFJUvPKVfD/2781ugJJkppbrjb1S5KkoeUq+LfbzrV+SZKGkpvgTwmefhrG5erHC0mSais3wb9iBWzc6HA+SZKGkpvg7z4lr8EvSdLg6hr8EbEgIh6OiEUR8aEB2udHREdE3F26/PtIX6uzM7t2HL8kSYOr2y/iETEWuBg4BlgC3BER16aU/thn0ltSSq/b1NebOhXe/W7Yc89NnZMkSflVz13hDgYWpZQeA4iIK4ETgL7BXxNz5sBXv1qPOUuSlB/13NS/PbC47P6S0mN9HRYR90TE9RGx90hfbP162LBhpM+WJKkY6rnGHwM8lvrc/z2wU0ppRUQcB1wDzO03o4gzgTMBZsyYQVtbW78Z//zn2/GFL8zjqqtuZcaMdZtae+6tWLFiwH5UdezH2rEva8N+rI0892M9g38JsGPZ/R2AJ8snSCl1lt2+LiK+GhHbpJSe7TPdpcClAPPmzUvz58/v92J33pldL1hwOJtvXps3kGdtbW0M1I+qjv1YO/ZlbdiPtZHnfqznpv47gLkRsUtETABOBq4tnyAiZkVElG4fXKrnuZG8WEcHRGQ7+UmSpIHVbY0/pdQVEWcBNwBjgW+llB6IiHeV2i8BTgLeHRFdwGrg5JRS358DKtLRkQ3li4F+YJAkSUCdz86XUroOuK7PY5eU3b4IuKgWr9XZ6cF7JEkaTm6ObH/ccbD3iMcESJJUDLkJ/je9qdEVSJLU/HJzrP5nnoFVqxpdhSRJzS03wX/YYXDGGY2uQpKk5pab4O/ocOc+SZKGk4vgT8nglySpErkI/rVrs2P1e0peSZKGlovg7+jIrl3jlyRpaLkI/kmT4LOfhSOOaHQlkiQ1t1yM499iCzjnnEZXIUlS88vFGv/y5bBoEazzbLySJA0pF8F/000wdy48+GCjK5EkqbnlIvjduU+SpMrkKvgdzidJ0tByEfydndm1wS9J0tByEfwdHTB5MozLxRgFSZLqJxdRedJJsPfeja5CkqTml4vgP+yw7CJJkoaWi03999+fjeOXJElDy8Ua/xlnZEP5brih0ZVIktTccrHG7yl5JUmqTG6C36F8kiQNLxfB39npGr8kSZVo+eDv6oKVKw1+SZIqkYud+370I3jJSxpdhSRJza/lg3/cuOwAPpIkaXgtv6n/xRfhxhvh+ecbXYkkSc2v5YP/3nvhNa+BP/yh0ZVIktT8Wj74u0/J6859kiQNr+WD31PySpJUuZYPftf4JUmqnMEvSVKBtPxwvlNOgf32g0mTGl2JJEnNr+WDf+eds4skSRpey2/qv+UW+OUvG12FJEmtoeXX+D/9aVi2DO66q9GVSJLU/Fp+jd8z80mSVLmWD/6ODsfwS5JUqZYPftf4JUmqXMsHf0eHwS9JUqVafue+X/0Kpk9vdBWSJLWGlg/+l7600RVIktQ6WnpTf2cnfP3r8Oijja5EkqTW0NLBv3gxvOtdcOedja5EkqTW0NLB332CHofzSZJUmZYO/s7O7Nq9+iVJqkxLB7+n5JUkqToGvyRJBdLSwX/KKfDggzBrVqMrkSSpNbT0OP7NN4c992x0FZIktY6WXuO//nq45JJGVyFJUuto6eC/8kr4z/9sdBWSJLWOlg5+z8wnSVJ1Wjr4PTOfJEnVafng96h9kiRVruWD3zV+SZIq19LD+e68EzZubHQVkiS1jpYO/unTG12BJEmtpWU39a9bBx/8INx6a6MrkSSpdbRs8L/wAnz2s3D33Y2uRJKk1tGywd99Sl736pckqXItG/yemU+SpOoZ/JIkFUjLBn/3pn6DX5KkyrXscL43vAHWrIFxLfsOJEkafS0dmxMnNroCSZJaS8tu6v/pT+G974WUGl2JJEmto2WD/+ab4ZvfhIhGVyJJUuto2eDv7HTHPkmSqtWywe+Z+SRJqp7BL0lSgbRs8G/cCFtt1egqJElqLS07nO+mm9yjX5KkarXsGj+4R78kSdVq2eB/y1vgqqsaXYUkSa2lJTf1d3XB978Pc+c2uhJJklpLS67xe4IeSZJGxuCXJKlAWjL4Ozqya4NfkqTqtGTwr18Ps2fD1ls3uhJJklpLXYM/IhZExMMRsSgiPjTEdC+LiA0RcVIl8z3oIFi6FF7xitrVKklSEdQt+CNiLHAxcCywF3BKROw1yHSfAW6oVy2SJClTzzX+g4FFKaXHUkrrgCuBEwaY7mzgJ8DTlc746qvhda/r+a1fkiRVpp7Bvz2wuOz+ktJjfxUR2wNvAC6pZsYPPQT/9V8wYcIm1yhJUqHU8wA+Ax1Qt+/R9b8MfDCltCGGOP5uRJwJnAkwY8YM7rvvL4wbtwO3336zh+0doRUrVtDW1tboMlqe/Vg79mVt2I+1ked+rGfwLwF2LLu/A/Bkn2kOAq4shf42wHER0ZVSuqZ8opTSpcClAPPmzUtbbDGH6dPhqKPm16v23Gtra2P+/PmNLqPl2Y+1Y1/Whv1YG3nux3oG/x3A3IjYBVgKnAycWj5BSmmX7tsRcTnw876hP5CODsfwS5I0EnUL/pRSV0ScRba3/ljgWymlByLiXaX2qn7XLzdjBuyzT40KlSSpQOp6kp6U0nXAdX0eGzDwU0qniFxVNAAACmJJREFUVTrfCy7YtLokSSqqljxynyRJGpmWDP7jj4fPfKbRVUiS1HpaMvh/85vskL2SJKk6LRn8nZ3u1S9J0ki0XPBv3Bhs3AjTpjW6EkmSWk9LBj+4xi9J0ki0XPADHHEEzJnT6CokSWo9dR3HXw/jxm3kN79pdBWSJLWmllzjlyRJI9Nywb9y5Tj22gsef7zRlUiS1HpaLvi7uoIHH4Tx4xtdiSRJraflgt+9+iVJGrmWC/4NG4IxY2Dq1EZXIklS62m54O8+eE9EoyuRJKn1tNxwvgkTEkcf3egqJElqTS23xj99+jq+//1GVyFJUmtqueCXJEkj13LBv3jxZN7+9kZXIUlSa2q54O/qCtasaXQVkiS1ppYL/o0bwzH8kiSNkMEvSVKBtFzwp+RR+yRJGqmWC/7NN1/Pfvs1ugpJklpTywX/dtut4bjjGl2FJEmtqeWCX5IkjVzLBf8jj2zOrbc2ugr9//buPMausozj+PcnVJCtaoAGgViiRESjqICiRAEN4hKXuEHUKG5xi6KiETUq/qESiRKCoLgEEtxwA4LKIoISRTbZWpaAUJZArEaEogGBPv5x3srN5LadO0x6e+Z8P0kz97z3nHOfPm3mN2eZ80qS+ql3wV8FW2wx7SokSeqn3gU/eFe/JElzZfBLkjQgvQz+rbeedgWSJPVT74J/8eIHWbRo2lVIktRPvQv+JUucoUeSpLnqXfBLkqS5M/glSRoQg1+SpAEx+CVJGhCDX5KkATH4JUkaEINfkqQBMfglSRoQg1+SpAEx+CVJGhCDX5KkATH4JUkaEINfkqQBMfglSRoQg1+SpAEx+CVJGhCDX5KkATH4JUkaEINfkqQBMfglSRoQg1+SpAEx+CVJGhCDX5KkATH4JUkaEINfkqQBMfglSRoQg1+SpAEx+CVJGhCDX5KkATH4JUkaEINfkqQBSVVNu4aJJFkF3DDtOhaAbYF/TLuIBcA+zh97OT/s4/zoex+fXFXbjXtj0w1dyTy4oar2nHYRfZfkMvv46NnH+WMv54d9nB8LuY+e6pckaUAMfkmSBqSPwX/itAtYIOzj/LCP88dezg/7OD8WbB97d3OfJEmauz4e8UuSpDnqVfAnOSjJDUluSvLpadezMUvy/SQrkywbGXtiknOT3Ni+PmHkvSNaX29I8vLpVL3xSbJzkvOTXJdkeZKPtnF7OYEkmye5JMlVrY9HtnH7OAdJNklyRZIz27J9nFCSFUmuSXJlksva2CD62JvgT7IJ8E3gFcDuwCFJdp9uVRu1k4CDZox9GjivqnYFzmvLtD4eDDyjbXN867fgIeATVfV04AXAh1q/7OVkHgAOqKpnA3sAByV5AfZxrj4KXDeybB/nZv+q2mPk1/YG0cfeBD+wN3BTVd1cVf8Ffgy8dso1bbSq6g/AP2cMvxY4ub0+GXjdyPiPq+qBqroFuImu34NXVXdV1V/a61V032x3xF5OpDr3tcVF7U9hHyeWZCfgVcB3R4bt4/wYRB/7FPw7ArePLN/RxjR7S6rqLugCDdi+jdvbWUiyFHgOcDH2cmLt9PSVwErg3Kqyj3NzDPApYPXImH2cXAHnJLk8yfva2CD62Kcn92XMmL+SMD/s7Xok2Qr4OXBYVd2bjGtZt+qYMXsJVNXDwB5JHg/8Mskz17G6fRwjyauBlVV1eZL9ZrPJmLHB97F5UVXdmWR74Nwk169j3QXVxz4d8d8B7DyyvBNw55Rq6au/JdkBoH1d2cbt7TokWUQX+j+oql+0YXs5R1X1L+ACumul9nEyLwJek2QF3eXOA5Kcgn2cWFXd2b6uBH5Jd+p+EH3sU/BfCuyaZJckj6W70eKMKdfUN2cA72iv3wGcPjJ+cJLNkuwC7ApcMoX6NjrpDu2/B1xXVV8fecteTiDJdu1InySPA14GXI99nEhVHVFVO1XVUrrvgb+rqrdhHyeSZMskW695DRwILGMgfezNqf6qeijJh4GzgU2A71fV8imXtdFK8iNgP2DbJHcAXwC+Cpya5N3AbcCbAKpqeZJTgWvp7mL/UDstq+4I6+3ANe36NMBnsJeT2gE4ud0J/Rjg1Ko6M8lF2Mf54P/HySyhu9wEXQ7+sKrOSnIpA+ijT+6TJGlA+nSqX5IkPUoGvyRJA2LwS5I0IAa/JEkDYvBLkjQgBr+0gSRZmpHZEmex/heTHL6edTZL8ts2w9hb1rHeO5McN0m9C02Sw5JsMe06pGkz+KV+ew6wqM0w9pNpFzNN6azre9phwETB3+cZ2KS1MfilDWuTJN9JNyf9OUkel+QpSc5qk4VcmGS3mRsluSDJMUn+lGRZkr3bM8ZPoXv+/ZVtPyuSbNu22TPJBWP2dVKSY9u+bk7yxpH3Ppnk0iRXJzmyjW2Z5FdJrmqf/ZY2/tUk17Z1j17XvpPslzZ3fFs+Lsk72+sVSb6c5KIklyV5bpKzk/w1yfvXU9vSJNclOR74C7BzkhPafpaPrPcR4EnA+UnOb2OHpJuPfVmSo0Y+574kX0pyMbDPHP6NpY1ab57cJy0QuwKHVNV725PA3gAcCry/qm5M8nzgeOCAMdtuWVUvTPJiuidXPjPJe4DDq+rVAFn75EEz7QDsC+xG9zjSnyU5sNW3N92kJGe0z9oOuLOqXtU+Y3GSJwKvB3arqkp7HO/a9j2Lem6vqn2SfAM4ie6JiZsDy4FvraO224CnAYdW1QdbfZ+tqn+2o/Xzkjyrqo5N8nG6+df/keRJwFHA84C76WZpe11VnQZsCSyrqs/PtplSnxj80oZ1S1WtefTv5cBS4IXAT0dCe7O1bPsjgKr6Q5JtZoTtpE6rqtXAtUmWtLED258r2vJWdGF7IXB0Oyo+s6ouTLIpcD/w3SS/As5cz77XZ828G9cAW1XVKmBVkvvb33Nttd0G3FpVfx7Z15vTTbO6Kd0PIbsDV8/4vL2AC6rq7wBJfgC8GDgNeJhuUiZpQTL4pQ3rgZHXD9M9M/xfVbXHLLad+Xztcc/bfohHLuFtPss6MvL1K1X17ZkrJ3ke8ErgK0nOqaovJdkbeCndZDEf5pGzFOP2PVrXuNrWbLN6xvar6b5Pja0tyVLg3yPLuwCHA3tV1d1JThrzWaN1jXN/n5/DLq2P1/il6boXuCXJm+D/N6g9ey3rrrm2vi9wT1XdM2adFXSnr6G7jDCJs4F3Jdmqfc6OSbZvp8X/U1WnAEcDz23rLK6qX9PdNLe+H1xuBXZP91sIi+l+YHjUtY1Zbxu6HwTuaWcbXjHy3ipg6/b6YuAlSbZtlwQOAX4/YU1SL3nEL03fW4ETknwOWEQ3z/pVY9a7O8mf6MLtXWvZ15HA95J8hi7cZq2qzknydOCidtnhPuBtwFOBryVZDTwIfIAuQE9Psjnd0fPH1rPv29s9DVcDN/LIKftHW9vDM9a7KskVdPcG3Az8ceTtE4HfJLmrqvZPcgRwfqv/11V1OtIAODuf1APt7vzDq+qyadciqd881S9J0oB4xC9J0oB4xC9J0oAY/JIkDYjBL0nSgBj8kiQNiMEvSdKAGPySJA3I/wBHdVpOuJoSagAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 504x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Partial Dependence Plot\n",
    "pdp_helpfulness = gbm_baseline.partial_plot(train, cols = [\"HelpfulnessNumerator\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# There is room for improvement \n",
    "## Let's now Tokenize words in the Review\n",
    "### But first, let's start our new run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['all', 'just', 'being', 'over', 'both', 'through', 'yourselves', 'its', 'before', 'o', 'hadn', 'herself', 'll', 'had', 'should', 'to', 'only', 'won', 'under', 'ours', 'has', 'do', 'them', 'his', 'very', 'they', 'not', 'during', 'now', 'him', 'nor', 'd', 'did', 'didn', 'this', 'she', 'each', 'further', 'where', 'few', 'because', 'doing', 'some', 'hasn', 'are', 'our', 'ourselves', 'out', 'what', 'for', 'while', 're', 'does', 'above', 'between', 'mustn', 't', 'be', 'we', 'who', 'were', 'here', 'shouldn', 'hers', 'by', 'on', 'about', 'couldn', 'of', 'against', 's', 'isn', 'or', 'own', 'into', 'yourself', 'down', 'mightn', 'wasn', 'your', 'from', 'her', 'their', 'aren', 'there', 'been', 'whom', 'too', 'wouldn', 'themselves', 'weren', 'was', 'until', 'more', 'himself', 'that', 'but', 'don', 'with', 'than', 'those', 'he', 'me', 'myself', 'ma', 'these', 'up', 'will', 'below', 'ain', 'can', 'theirs', 'my', 'and', 've', 'then', 'is', 'am', 'it', 'doesn', 'an', 'as', 'itself', 'at', 'have', 'in', 'any', 'if', 'again', 'no', 'when', 'same', 'how', 'other', 'which', 'you', 'shan', 'needn', 'haven', 'after', 'most', 'such', 'why', 'a', 'off', 'i', 'm', 'yours', 'so', 'y', 'the', 'having', 'once']\n"
     ]
    }
   ],
   "source": [
    "# Start new mlflow run\n",
    "mlflow.start_run(run_name='review_tokenizer')\n",
    "mlflow.lp('source data table', f'{get_user()}.AMAZON_REVIEWS')\n",
    "# Get common stop words from H2O\n",
    "data_path = \"https://splice-demo.s3.amazonaws.com/stop_words.csv\"\n",
    "STOP_WORDS = pd.read_csv(data_path, header=0)\n",
    "STOP_WORDS = list(STOP_WORDS['STOP_WORD'])\n",
    "print(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>Text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>These are made in China (do not buy ANY pet food from China).  Dogswell has been using propylene glycol to soften their treats (what are they thinkng?).  Do not purchase or support this company in any way until they clean up their act.  And for whatever reason Amazon doesn&#x27;t allow returns of this item, so I had to toss mine out.  Bad business all around on this one.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            </td></tr>\n",
       "<tr><td>Everything is ok, except it just isn&#x27;t as good as it is in the bags.  Just considerably more bland -- less lemon and less zing.  Boring.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </td></tr>\n",
       "<tr><td>Best cat treat ever. There isn&#x27;t anything comparable to the love my cat has for these treats, he snubs away any other kind now.&lt;br /&gt;I know he likes to manipulate me with his cattiness but these treats are my way of manipulating him to come sit on my lap and have some chill time. :)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 </td></tr>\n",
       "<tr><td>My two Corgis were thoroughly spoiled by my late husband (I spent a year and a half dieting them down a combined total of 25 pounds!)&lt;br /&gt;&lt;br /&gt;They are accustomed to the finest of fare, and they absolutely love the Wellness brand of treats.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          </td></tr>\n",
       "<tr><td>We used to have drive down to the specialty pet food store for this product.  So glad we discovered Amazon.  As far as I can tell it is no more expensive &amp; in some cases less - Prime membership is awesome.  Loving Pets treats are some of the best according to my dog.  They do not develop that nasty smell like some dog treats do.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  </td></tr>\n",
       "<tr><td>I really liked this coffee, it was just as good as everyone claimed it was.  Strong, bold and flavorful!  I would recommend!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                </td></tr>\n",
       "<tr><td>I am always looking for healthier, whole grain versions of foods I enjoy.  Unfortunately, these Peacock brand noodles are yet another food masquerading as healthy.  The product title in big letters on the front says &quot;Brown Rice Vermicelli&quot;, making the consumer think &quot;this is made with brown rice, so it should be a healthy choice&quot;.  But the first indication that it is not is when looking at the fiber content on the nutrition facts - only 0.6g per 2oz serving.  Then onto the ingredients list to see why so low... contains brown rice, sago starch and water.  The sago starch comes from palms and must not have much (if any) fiber.&lt;br /&gt;&lt;br /&gt;The Annie Chun&#x27;s Maifun Brown Rice Noodles (sold on Amazon and in my local healthy grocer) has become one of my staples and is my frame of reference when comparing to the Peacock brand.  The Annie Chun&#x27;s product is made with 100% whole grain, with ingredients brown rice flour and water.  Per 2oz serving, it has 4g fiber and pretty much the same calories and other nutrients as the Peacock brand.&lt;br /&gt;&lt;br /&gt;If you do try this Peacock brand noodles and have not used rice noodles before, you will need to seek guidance elsewhere on preparation.  As others have pointed out, the Peacock package gives almost no directions on how to prepare the product, aside from a brief mention in the recipes (in the header text it does say that they are &quot;easy-to-cook&quot; but does not say how).  It also contains a very strange recipe for rice noodles: Aglio Olio style - this is an Italian recipe for noodles with olive oil/garlic/sprinkled with grated cheese that I think would not be very tasty.  The second recipe appears to be for a soup with veggie strips.  Neither recipe gives amounts or much direction.  In comparison, the Annie Chun&#x27;s package gives clear, specific directions on rice noodle preparation and two recipes.&lt;br /&gt;&lt;br /&gt;I use rice noodles = maifun = rice sticks = sometimes called vermicelli for making the Vietnamese salad &quot;bun tofu&quot;, to serve with stir-fried veggies or in lettuce rolls.  They can also be used in spring rolls/egg rolls.  When cooking with thin rice noodles, be careful not to oversoak/overcook/overmix or they tend to disintegrate.  Asian rice noodle vermicelli (maifun) are not the same as Italian vermicelli and are not readily interchangeable.  If making an Italian recipe, the best results would be expected from Italian pasta and not maifun.&lt;br /&gt;&lt;br /&gt;A few final notes...  Both Peacock and Annie Chun&#x27;s brown rice noodles are gluten free.  The Peacock is made in Singapore and the Annie Chun&#x27;s in Thailand.  The Peacock noodles do taste fine (kind of bland), but so do the Annie Chun&#x27;s.  At this time, they are both approximately the same price.  Peacock come in an plastic bag with some noodle crushage upon shipping; Annie Chun&#x27;s are perfect upon removal from their cellophane bag in a box.  Overall, I highly recommend the Annie Chun&#x27;s Maifun as a healthier option over the Peacock brand.  On a related note, the Annie Chun&#x27;s soba and brown rice pad thai noodles are also excellent.&lt;br /&gt;&lt;br /&gt;Rating for this product: 2.5 stars rounded down to 2 stars.</td></tr>\n",
       "<tr><td>I have tried many different drink mix, this is the best tasting by far. It does not have the after taste of the sweetener and I really like it, it is pretty strong, so I use a big water bottle (20 oz) for one tube, it still a little stronger than I like, but it is just my taste.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     </td></tr>\n",
       "<tr><td>This is my favorite item to order for my Keurig. There are so many flavors, my finicky palate never gets bored!  The only downside is there are probably 5-6 decaf varieties.  I don&#x27;t drink decaf (I REQUIRE copious amounts of caffeine), so they sit on the shelf...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     </td></tr>\n",
       "<tr><td>Tastes like Earl Grey, but it&#x27;s green tea so it&#x27;s healthier.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews['Text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can train our Word2Vec model\n",
    "<blockquote>We can time how long it takes to train the word vectorizer, and log the vector size so we can change it and see how performance changes. Then we can look at some word synonyms to see how well the tokenizer did</i></br><footer>Splice Machine</footer></blockquote><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Code Block word vectorization train time... word2vec Model Build progress: |██████████████████████████████████████████| 100%\n",
      "Done.\n",
      "Code Block word vectorization train time:\n",
      "Ran in 105.062 secs\n",
      "Ran in 1.751 mins\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OrderedDict([('coffe', 0.8252993226051331),\n",
       "             ('expresso', 0.7982868552207947),\n",
       "             ('coffees', 0.7907992005348206),\n",
       "             ('espresso', 0.7904424667358398),\n",
       "             ('coffeehouse', 0.7591685056686401)])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from h2o.estimators.word2vec import H2OWord2vecEstimator\n",
    "vec_size = 100\n",
    "w2v_model = H2OWord2vecEstimator(vec_size = vec_size, model_id = \"w2v.hex\")\n",
    "# We can change this later and see how performance changes\n",
    "mlflow.lp('word_vec_size', vec_size)\n",
    "with mlflow.timer('word vectorization train time'):\n",
    "    w2v_model.train(training_frame=words)\n",
    "# Sanity check - find synonyms for the word 'coffee'\n",
    "w2v_model.find_synonyms(\"coffee\", count = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize( reviews ):\n",
    "    review_tokens = []\n",
    "    for review in reviews[0]:\n",
    "        # Remove non-letters\n",
    "        review = re.sub(\"[^a-zA-Z]\",\" \", review)\n",
    "        review = review.lower().split()\n",
    "\n",
    "        stops = {'all', 'just', 'being', 'over', 'both', 'through', 'yourselves', 'its', 'before', 'o', 'hadn', 'herself', 'll', 'had', 'should', 'to', 'only', 'won', 'under', \n",
    "                 'ours', 'has', 'do', 'them', 'his', 'very', 'they', 'not', 'during', 'now', 'him', 'nor', 'd', 'did', 'didn', 'this', 'she', 'each', 'further', 'where', 'few', \n",
    "                 'because', 'doing', 'some', 'hasn', 'are', 'our', 'ourselves', 'out', 'what', 'for', 'while', 're', 'does', 'above', 'between', 'mustn', 't', 'be', 'we', 'who', \n",
    "                 'were', 'here', 'shouldn', 'hers', 'by', 'on', 'about', 'couldn', 'of', 'against', 's', 'isn', 'or', 'own', 'into', 'yourself', 'down', 'mightn', 'wasn', 'your', \n",
    "                 'from', 'her', 'their', 'aren', 'there', 'been', 'whom', 'too', 'wouldn', 'themselves', 'weren', 'was', 'until', 'more', 'himself', 'that', 'but', 'don', 'with', \n",
    "                 'than', 'those', 'he', 'me', 'myself', 'ma', 'these', 'up', 'will', 'below', 'ain', 'can', 'theirs', 'my', 'and', 've', 'then', 'is', 'am', 'it', 'doesn', 'an', \n",
    "                 'as', 'itself', 'at', 'have', 'in', 'any', 'if', 'again', 'no', 'when', 'same', 'how', 'other', 'which', 'you', 'shan', 'needn', 'haven', 'after', 'most', 'such',\n",
    "                 'why', 'a', 'off', 'i', 'm', 'yours', 'so', 'y', 'the', 'having', 'once'}\n",
    "        review = [w for w in review if w not in stops]\n",
    "        review_tokens.append(review)\n",
    "    return(review_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('preprocessor',\n",
       "                 FunctionTransformer(accept_sparse=False, check_inverse=True,\n",
       "                                     func=<function tokenize at 0x7f81886b05f0>,\n",
       "                                     inv_kw_args=None, inverse_func=None,\n",
       "                                     kw_args=None, pass_y='deprecated',\n",
       "                                     validate=False)),\n",
       "                ('word2vec',\n",
       "                 D2VTransformer(alpha=0.025, batch_words=10000, cbow_mean=1,\n",
       "                                comment=None, dbow_words=0, dm=1, dm_concat=0,\n",
       "                                dm_mean=None, dm_tag_count=1, docvecs=None,\n",
       "                                docvecs_mapfile=None,\n",
       "                                hashfxn=<built-in function hash>, hs=0, iter=5,\n",
       "                                max_vocab_size=None, min_alpha=0.0001,\n",
       "                                min_count=5, negative=5, sample=0.001, seed=1,\n",
       "                                size=100, sorted_vocab=1, trim_rule=None,\n",
       "                                window=5, workers=3))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline as skPipe\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from gensim.sklearn_api import W2VTransformer, D2VTransformer\n",
    "import pandas as pd\n",
    "\n",
    "pdf = reviews['Text'].as_data_frame().astype('string')\n",
    "\n",
    "data_path = \"https://splice-demo.s3.amazonaws.com/stop_words.csv\"\n",
    "STOP_WORDS = pd.read_csv(data_path, header=0)\n",
    "STOP_WORDS = list(STOP_WORDS['STOP_WORD'])\n",
    "\n",
    "# vectorizer_model = skPipe(steps = [\n",
    "#     ('preprocessor', FunctionTransformer(lambda x: x[0], validate=False)),\n",
    "#     ('CountVector', CountVectorizer(stop_words=STOP_WORDS, token_pattern=\"[^a-zA-Z]\", max_features=100)),\n",
    "#     ('vectorizer', TfidfTransformer())\n",
    "#     #('vector_returner', FunctionTransformer(lambda x: x.toarray(), accept_sparse=True, validate=False))\n",
    "# ])\n",
    "vectorizer_model2 = skPipe(verbose=True,\n",
    "                            steps = [\n",
    "                                ('preprocessor', FunctionTransformer(tokenize, validate=False)),\n",
    "                                ('word2vec', D2VTransformer())\n",
    "                            ])\n",
    "\n",
    "# tokenize and build vocab\n",
    "vectorizer_model2.fit([pdf.dropna()['Text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4d87d419b7ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"Text\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"sentence\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pdf' is not defined"
     ]
    }
   ],
   "source": [
    "pdf.rename(columns={\"Text\": \"sentence\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model of size: 56999.089 KB to Splice Machine DB\n"
     ]
    }
   ],
   "source": [
    "mlflow.deploy_db(vectorizer_model2, pdf, 'ben', 'sk_vec_pipe', [('MOMENT_KEY', 'INT')], run_id=mlflow.current_run_id(),\n",
    "                     classes = [f'C{i+1}' for i in range(100)], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "insert into ben.sk_vec_pipe values('I really liked this coffee, it was just as good as everyone claimed it was. Strong, bold and flavorful! I would recommend!', 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can save this model and end our run\n",
    "<blockquote>We want to save this vectorizer as an <b>independent</b> <code>run</code>. This is because we may want to build more than 1 model that utilizes these word vectors. We don't want to duplicate those identical word vectors, so we can use the outputs for <b>more than one model</b>. This is the idea of a <i>feature store</i> where we use features from one dataset on multiple models. This is crucial to creating efficient ML workflow systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving artifact of size: 5.792 KB to Splice Machine DB\n"
     ]
    }
   ],
   "source": [
    "mlflow.log_model(vectorizer_model, 'review_vectorizer_model')\n",
    "mlflow.lp('vector_size',100)\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's vectorize our reviews\n",
    "<blockquote>Now that we have a word embedding for each word in our vocabulary, we will aggregate the words for each review using the <code>transform</code> function.  This will give us one aggregated word embedding for each review.</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>ProductId  </th><th>UserId        </th><th>Summary                               </th><th style=\"text-align: right;\">  Score</th><th style=\"text-align: right;\">  HelpfulnessDenominator</th><th style=\"text-align: right;\">    Id</th><th>ProfileName                                     </th><th style=\"text-align: right;\">  HelpfulnessNumerator</th><th style=\"text-align: right;\">       Time</th><th>Text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </th><th style=\"text-align: right;\">  PositiveReview</th><th style=\"text-align: right;\">        C1</th><th style=\"text-align: right;\">        C2</th><th style=\"text-align: right;\">          C3</th><th style=\"text-align: right;\">        C4</th><th style=\"text-align: right;\">         C5</th><th style=\"text-align: right;\">         C6</th><th style=\"text-align: right;\">        C7</th><th style=\"text-align: right;\">         C8</th><th style=\"text-align: right;\">         C9</th><th style=\"text-align: right;\">       C10</th><th style=\"text-align: right;\">        C11</th><th style=\"text-align: right;\">        C12</th><th style=\"text-align: right;\">       C13</th><th style=\"text-align: right;\">        C14</th><th style=\"text-align: right;\">        C15</th><th style=\"text-align: right;\">        C16</th><th style=\"text-align: right;\">        C17</th><th style=\"text-align: right;\">         C18</th><th style=\"text-align: right;\">        C19</th><th style=\"text-align: right;\">        C20</th><th style=\"text-align: right;\">         C21</th><th style=\"text-align: right;\">        C22</th><th style=\"text-align: right;\">        C23</th><th style=\"text-align: right;\">        C24</th><th style=\"text-align: right;\">      C25</th><th style=\"text-align: right;\">         C26</th><th style=\"text-align: right;\">        C27</th><th style=\"text-align: right;\">       C28</th><th style=\"text-align: right;\">        C29</th><th style=\"text-align: right;\">        C30</th><th style=\"text-align: right;\">       C31</th><th style=\"text-align: right;\">       C32</th><th style=\"text-align: right;\">        C33</th><th style=\"text-align: right;\">       C34</th><th style=\"text-align: right;\">       C35</th><th style=\"text-align: right;\">        C36</th><th style=\"text-align: right;\">        C37</th><th style=\"text-align: right;\">        C38</th><th style=\"text-align: right;\">        C39</th><th style=\"text-align: right;\">        C40</th><th style=\"text-align: right;\">         C41</th><th style=\"text-align: right;\">       C42</th><th style=\"text-align: right;\">       C43</th><th style=\"text-align: right;\">        C44</th><th style=\"text-align: right;\">         C45</th><th style=\"text-align: right;\">        C46</th><th style=\"text-align: right;\">        C47</th><th style=\"text-align: right;\">        C48</th><th style=\"text-align: right;\">       C49</th><th style=\"text-align: right;\">        C50</th><th style=\"text-align: right;\">        C51</th><th style=\"text-align: right;\">        C52</th><th style=\"text-align: right;\">         C53</th><th style=\"text-align: right;\">        C54</th><th style=\"text-align: right;\">       C55</th><th style=\"text-align: right;\">        C56</th><th style=\"text-align: right;\">         C57</th><th style=\"text-align: right;\">        C58</th><th style=\"text-align: right;\">        C59</th><th style=\"text-align: right;\">        C60</th><th style=\"text-align: right;\">        C61</th><th style=\"text-align: right;\">        C62</th><th style=\"text-align: right;\">        C63</th><th style=\"text-align: right;\">       C64</th><th style=\"text-align: right;\">        C65</th><th style=\"text-align: right;\">        C66</th><th style=\"text-align: right;\">        C67</th><th style=\"text-align: right;\">        C68</th><th style=\"text-align: right;\">        C69</th><th style=\"text-align: right;\">         C70</th><th style=\"text-align: right;\">        C71</th><th style=\"text-align: right;\">        C72</th><th style=\"text-align: right;\">         C73</th><th style=\"text-align: right;\">         C74</th><th style=\"text-align: right;\">        C75</th><th style=\"text-align: right;\">        C76</th><th style=\"text-align: right;\">        C77</th><th style=\"text-align: right;\">        C78</th><th style=\"text-align: right;\">        C79</th><th style=\"text-align: right;\">         C80</th><th style=\"text-align: right;\">        C81</th><th style=\"text-align: right;\">       C82</th><th style=\"text-align: right;\">        C83</th><th style=\"text-align: right;\">        C84</th><th style=\"text-align: right;\">        C85</th><th style=\"text-align: right;\">        C86</th><th style=\"text-align: right;\">         C87</th><th style=\"text-align: right;\">        C88</th><th style=\"text-align: right;\">        C89</th><th style=\"text-align: right;\">        C90</th><th style=\"text-align: right;\">         C91</th><th style=\"text-align: right;\">        C92</th><th style=\"text-align: right;\">        C93</th><th style=\"text-align: right;\">       C94</th><th style=\"text-align: right;\">        C95</th><th style=\"text-align: right;\">       C96</th><th style=\"text-align: right;\">        C97</th><th style=\"text-align: right;\">        C98</th><th style=\"text-align: right;\">        C99</th><th style=\"text-align: right;\">       C100</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>B00141QYSQ </td><td>A1YS02UZZGRDCT</td><td>Do Not Buy                            </td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">                       2</td><td style=\"text-align: right;\"> 41471</td><td>Evan Eberhardt                                  </td><td style=\"text-align: right;\">                     2</td><td style=\"text-align: right;\">1.34836e+09</td><td>These are made in China (do not buy ANY pet food from China).  Dogswell has been using propylene glycol to soften their treats (what are they thinkng?).  Do not purchase or support this company in any way until they clean up their act.  And for whatever reason Amazon doesn&#x27;t allow returns of this item, so I had to toss mine out.  Bad business all around on this one.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            </td><td style=\"text-align: right;\">               0</td><td style=\"text-align: right;\">-0.0341503</td><td style=\"text-align: right;\">-0.10818  </td><td style=\"text-align: right;\">-0.0378363  </td><td style=\"text-align: right;\">-0.0664373</td><td style=\"text-align: right;\"> 0.00766119</td><td style=\"text-align: right;\"> 0.0289788 </td><td style=\"text-align: right;\">0.0857406 </td><td style=\"text-align: right;\">-0.00785125</td><td style=\"text-align: right;\">-0.120758  </td><td style=\"text-align: right;\">-0.0575716</td><td style=\"text-align: right;\"> 0.0857301 </td><td style=\"text-align: right;\"> 0.104244  </td><td style=\"text-align: right;\">-0.141921 </td><td style=\"text-align: right;\">-0.00838397</td><td style=\"text-align: right;\"> 0.144448  </td><td style=\"text-align: right;\"> 0.00902313</td><td style=\"text-align: right;\"> 0.0614638 </td><td style=\"text-align: right;\">-0.00241944 </td><td style=\"text-align: right;\"> 0.0303611 </td><td style=\"text-align: right;\"> 0.0530984 </td><td style=\"text-align: right;\">-0.0460452  </td><td style=\"text-align: right;\">-0.0806975 </td><td style=\"text-align: right;\">-0.114665  </td><td style=\"text-align: right;\">-0.0307632 </td><td style=\"text-align: right;\">0.0705416</td><td style=\"text-align: right;\"> 0.12288    </td><td style=\"text-align: right;\">-0.0402827 </td><td style=\"text-align: right;\"> 0.0509848</td><td style=\"text-align: right;\">-0.109168  </td><td style=\"text-align: right;\">-0.0670598 </td><td style=\"text-align: right;\"> 0.0467393</td><td style=\"text-align: right;\"> 0.0661342</td><td style=\"text-align: right;\">-0.00150267</td><td style=\"text-align: right;\"> 0.064902 </td><td style=\"text-align: right;\"> 0.0437838</td><td style=\"text-align: right;\"> 0.0206769 </td><td style=\"text-align: right;\">-0.0727154 </td><td style=\"text-align: right;\">-0.00707331</td><td style=\"text-align: right;\">-0.00596179</td><td style=\"text-align: right;\"> 0.0232555 </td><td style=\"text-align: right;\">-0.0423598  </td><td style=\"text-align: right;\"> 0.0441994</td><td style=\"text-align: right;\"> 0.0154022</td><td style=\"text-align: right;\">-0.0688855 </td><td style=\"text-align: right;\"> 0.000505353</td><td style=\"text-align: right;\"> 0.110922  </td><td style=\"text-align: right;\">-0.0382762 </td><td style=\"text-align: right;\"> 0.113123  </td><td style=\"text-align: right;\"> 0.116474 </td><td style=\"text-align: right;\"> 0.0412931 </td><td style=\"text-align: right;\"> 0.0119825 </td><td style=\"text-align: right;\"> 0.0177    </td><td style=\"text-align: right;\"> 0.0249545  </td><td style=\"text-align: right;\"> 0.135765  </td><td style=\"text-align: right;\"> 0.0143485</td><td style=\"text-align: right;\">0.0489837  </td><td style=\"text-align: right;\"> 0.00161878 </td><td style=\"text-align: right;\">-0.111903  </td><td style=\"text-align: right;\">-0.102871  </td><td style=\"text-align: right;\"> 0.11677   </td><td style=\"text-align: right;\">-0.09998   </td><td style=\"text-align: right;\">-0.0131311 </td><td style=\"text-align: right;\"> 0.0747656 </td><td style=\"text-align: right;\">-0.0290306</td><td style=\"text-align: right;\">-0.0344619 </td><td style=\"text-align: right;\">-0.0191363 </td><td style=\"text-align: right;\">-0.0459339 </td><td style=\"text-align: right;\"> 0.0109364 </td><td style=\"text-align: right;\"> 0.0948219 </td><td style=\"text-align: right;\"> 0.0380888  </td><td style=\"text-align: right;\">-0.0222718 </td><td style=\"text-align: right;\">-0.0445089 </td><td style=\"text-align: right;\">-0.028453   </td><td style=\"text-align: right;\"> 0.000661888</td><td style=\"text-align: right;\">-0.0749557 </td><td style=\"text-align: right;\"> 0.0525382 </td><td style=\"text-align: right;\"> 0.0656178 </td><td style=\"text-align: right;\"> 0.0957749 </td><td style=\"text-align: right;\">-0.0598913 </td><td style=\"text-align: right;\">-0.0801278  </td><td style=\"text-align: right;\"> 0.0406965 </td><td style=\"text-align: right;\"> 0.0178941</td><td style=\"text-align: right;\"> 0.123787  </td><td style=\"text-align: right;\">-0.021983  </td><td style=\"text-align: right;\">-0.0491254 </td><td style=\"text-align: right;\"> 0.193216  </td><td style=\"text-align: right;\"> 0.0860584  </td><td style=\"text-align: right;\"> 0.0613115 </td><td style=\"text-align: right;\">-0.0100164 </td><td style=\"text-align: right;\">-0.169791  </td><td style=\"text-align: right;\">-0.1119     </td><td style=\"text-align: right;\"> 0.0402531 </td><td style=\"text-align: right;\">-0.102299  </td><td style=\"text-align: right;\">-0.0082157</td><td style=\"text-align: right;\">-0.0750572 </td><td style=\"text-align: right;\">-0.0340265</td><td style=\"text-align: right;\">-0.0126831 </td><td style=\"text-align: right;\">-0.0756751 </td><td style=\"text-align: right;\"> 0.0611046 </td><td style=\"text-align: right;\"> 0.0886016 </td></tr>\n",
       "<tr><td>B0089SPEO2 </td><td>A3JOYNYL458QHP</td><td>Less lemon and less zing              </td><td style=\"text-align: right;\">      3</td><td style=\"text-align: right;\">                       0</td><td style=\"text-align: right;\"> 28582</td><td>coleridge                                       </td><td style=\"text-align: right;\">                     0</td><td style=\"text-align: right;\">1.32391e+09</td><td>Everything is ok, except it just isn&#x27;t as good as it is in the bags.  Just considerably more bland -- less lemon and less zing.  Boring.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </td><td style=\"text-align: right;\">               0</td><td style=\"text-align: right;\">-0.0390761</td><td style=\"text-align: right;\">-0.0159853</td><td style=\"text-align: right;\"> 0.0525692  </td><td style=\"text-align: right;\">-0.0365459</td><td style=\"text-align: right;\"> 0.0239685 </td><td style=\"text-align: right;\">-0.00540454</td><td style=\"text-align: right;\">0.00574737</td><td style=\"text-align: right;\"> 0.0278542 </td><td style=\"text-align: right;\"> 0.035262  </td><td style=\"text-align: right;\">-0.0343393</td><td style=\"text-align: right;\">-0.0224695 </td><td style=\"text-align: right;\">-0.00701442</td><td style=\"text-align: right;\"> 0.0301193</td><td style=\"text-align: right;\">-0.0133648 </td><td style=\"text-align: right;\">-0.0229403 </td><td style=\"text-align: right;\"> 0.0142804 </td><td style=\"text-align: right;\">-0.0362945 </td><td style=\"text-align: right;\">-0.0189084  </td><td style=\"text-align: right;\">-0.0122248 </td><td style=\"text-align: right;\"> 0.00196249</td><td style=\"text-align: right;\"> 0.0115385  </td><td style=\"text-align: right;\"> 0.00271523</td><td style=\"text-align: right;\">-0.0263855 </td><td style=\"text-align: right;\">-0.0432941 </td><td style=\"text-align: right;\">0.043083 </td><td style=\"text-align: right;\"> 0.0731997  </td><td style=\"text-align: right;\">-0.0240156 </td><td style=\"text-align: right;\">-0.0116159</td><td style=\"text-align: right;\"> 0.0207294 </td><td style=\"text-align: right;\">-0.0229416 </td><td style=\"text-align: right;\">-0.0378723</td><td style=\"text-align: right;\"> 0.037957 </td><td style=\"text-align: right;\"> 0.044218  </td><td style=\"text-align: right;\"> 0.0113744</td><td style=\"text-align: right;\"> 0.0315505</td><td style=\"text-align: right;\">-0.0147176 </td><td style=\"text-align: right;\"> 0.0231034 </td><td style=\"text-align: right;\"> 0.0326607 </td><td style=\"text-align: right;\"> 0.00390567</td><td style=\"text-align: right;\"> 0.0286941 </td><td style=\"text-align: right;\">-0.0551709  </td><td style=\"text-align: right;\">-0.0132278</td><td style=\"text-align: right;\">-0.0218749</td><td style=\"text-align: right;\">-0.0309141 </td><td style=\"text-align: right;\">-0.000503425</td><td style=\"text-align: right;\">-0.00909946</td><td style=\"text-align: right;\"> 0.0426709 </td><td style=\"text-align: right;\"> 0.0254569 </td><td style=\"text-align: right;\"> 0.0146184</td><td style=\"text-align: right;\">-0.0278761 </td><td style=\"text-align: right;\"> 0.0370015 </td><td style=\"text-align: right;\"> 0.0615386 </td><td style=\"text-align: right;\"> 0.0141969  </td><td style=\"text-align: right;\"> 0.00994672</td><td style=\"text-align: right;\"> 0.0860573</td><td style=\"text-align: right;\">0.00857722 </td><td style=\"text-align: right;\">-0.000906741</td><td style=\"text-align: right;\">-0.104595  </td><td style=\"text-align: right;\"> 0.014108  </td><td style=\"text-align: right;\"> 0.018341  </td><td style=\"text-align: right;\">-0.021037  </td><td style=\"text-align: right;\"> 0.0251217 </td><td style=\"text-align: right;\">-0.0404073 </td><td style=\"text-align: right;\">-0.0254253</td><td style=\"text-align: right;\"> 0.00293042</td><td style=\"text-align: right;\"> 0.0748855 </td><td style=\"text-align: right;\">-0.00727587</td><td style=\"text-align: right;\"> 0.0410046 </td><td style=\"text-align: right;\"> 0.00575329</td><td style=\"text-align: right;\">-5.12304e-05</td><td style=\"text-align: right;\"> 0.0849097 </td><td style=\"text-align: right;\">-0.111972  </td><td style=\"text-align: right;\"> 0.013228   </td><td style=\"text-align: right;\">-0.0357114  </td><td style=\"text-align: right;\">-0.0518074 </td><td style=\"text-align: right;\"> 0.0310287 </td><td style=\"text-align: right;\">-0.00235838</td><td style=\"text-align: right;\"> 0.00408243</td><td style=\"text-align: right;\">-0.114568  </td><td style=\"text-align: right;\">-0.0808327  </td><td style=\"text-align: right;\"> 0.0208117 </td><td style=\"text-align: right;\">-0.0104827</td><td style=\"text-align: right;\">-0.100123  </td><td style=\"text-align: right;\">-0.0137341 </td><td style=\"text-align: right;\"> 0.00727252</td><td style=\"text-align: right;\"> 0.0182758 </td><td style=\"text-align: right;\"> 0.0409142  </td><td style=\"text-align: right;\"> 0.030828  </td><td style=\"text-align: right;\">-0.00658097</td><td style=\"text-align: right;\">-0.0826325 </td><td style=\"text-align: right;\"> 0.000459405</td><td style=\"text-align: right;\"> 0.0296497 </td><td style=\"text-align: right;\"> 0.041425  </td><td style=\"text-align: right;\">-0.0252306</td><td style=\"text-align: right;\">-0.0214588 </td><td style=\"text-align: right;\"> 0.0127377</td><td style=\"text-align: right;\">-0.00114045</td><td style=\"text-align: right;\">-0.0103909 </td><td style=\"text-align: right;\"> 0.00207492</td><td style=\"text-align: right;\">-0.00463952</td></tr>\n",
       "<tr><td>B001PMCDK2 </td><td>A14TTMM0Z03Y2W</td><td>my cat goes crazy for these!          </td><td style=\"text-align: right;\">      5</td><td style=\"text-align: right;\">                       0</td><td style=\"text-align: right;\">389965</td><td>Lindsay S. Bradford                             </td><td style=\"text-align: right;\">                     0</td><td style=\"text-align: right;\">1.3106e+09 </td><td>Best cat treat ever. There isn&#x27;t anything comparable to the love my cat has for these treats, he snubs away any other kind now.&lt;br /&gt;I know he likes to manipulate me with his cattiness but these treats are my way of manipulating him to come sit on my lap and have some chill time. :)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 </td><td style=\"text-align: right;\">               1</td><td style=\"text-align: right;\">-0.0636505</td><td style=\"text-align: right;\">-0.123733 </td><td style=\"text-align: right;\">-0.0231013  </td><td style=\"text-align: right;\">-0.0417403</td><td style=\"text-align: right;\"> 0.0693997 </td><td style=\"text-align: right;\"> 0.0463899 </td><td style=\"text-align: right;\">0.0279045 </td><td style=\"text-align: right;\"> 0.0087564 </td><td style=\"text-align: right;\">-0.0762662 </td><td style=\"text-align: right;\"> 0.0351494</td><td style=\"text-align: right;\"> 0.0971182 </td><td style=\"text-align: right;\"> 0.00450611</td><td style=\"text-align: right;\">-0.0498215</td><td style=\"text-align: right;\">-0.0250226 </td><td style=\"text-align: right;\">-0.00784616</td><td style=\"text-align: right;\"> 0.0326165 </td><td style=\"text-align: right;\">-0.0154473 </td><td style=\"text-align: right;\">-0.00110973 </td><td style=\"text-align: right;\"> 0.0860599 </td><td style=\"text-align: right;\"> 0.0218876 </td><td style=\"text-align: right;\"> 0.0267343  </td><td style=\"text-align: right;\">-0.00762228</td><td style=\"text-align: right;\"> 0.0474742 </td><td style=\"text-align: right;\">-0.0223307 </td><td style=\"text-align: right;\">0.115004 </td><td style=\"text-align: right;\"> 0.0864473  </td><td style=\"text-align: right;\"> 0.070582  </td><td style=\"text-align: right;\"> 0.0662224</td><td style=\"text-align: right;\">-0.00755582</td><td style=\"text-align: right;\"> 0.00218678</td><td style=\"text-align: right;\"> 0.0292147</td><td style=\"text-align: right;\"> 0.12444  </td><td style=\"text-align: right;\"> 0.0160181 </td><td style=\"text-align: right;\"> 0.0318405</td><td style=\"text-align: right;\"> 0.0418051</td><td style=\"text-align: right;\">-0.0305255 </td><td style=\"text-align: right;\">-0.0509009 </td><td style=\"text-align: right;\">-0.0283232 </td><td style=\"text-align: right;\">-0.0801327 </td><td style=\"text-align: right;\"> 0.0229666 </td><td style=\"text-align: right;\">-0.048194   </td><td style=\"text-align: right;\">-0.0551238</td><td style=\"text-align: right;\"> 0.0484954</td><td style=\"text-align: right;\">-0.048677  </td><td style=\"text-align: right;\">-0.0407712  </td><td style=\"text-align: right;\"> 0.0690952 </td><td style=\"text-align: right;\">-0.052171  </td><td style=\"text-align: right;\">-0.00598294</td><td style=\"text-align: right;\">-0.054974 </td><td style=\"text-align: right;\">-0.0648315 </td><td style=\"text-align: right;\"> 0.0237217 </td><td style=\"text-align: right;\"> 0.016627  </td><td style=\"text-align: right;\"> 0.105102   </td><td style=\"text-align: right;\"> 0.0557317 </td><td style=\"text-align: right;\">-0.010954 </td><td style=\"text-align: right;\">0.0672567  </td><td style=\"text-align: right;\"> 0.000413501</td><td style=\"text-align: right;\">-0.138114  </td><td style=\"text-align: right;\">-0.0582815 </td><td style=\"text-align: right;\"> 0.0932297 </td><td style=\"text-align: right;\">-0.0609083 </td><td style=\"text-align: right;\">-0.00888905</td><td style=\"text-align: right;\"> 0.0128746 </td><td style=\"text-align: right;\">-0.0492285</td><td style=\"text-align: right;\"> 0.0293345 </td><td style=\"text-align: right;\"> 0.0348763 </td><td style=\"text-align: right;\"> 0.0172724 </td><td style=\"text-align: right;\">-0.00351206</td><td style=\"text-align: right;\">-0.0431066 </td><td style=\"text-align: right;\"> 0.110294   </td><td style=\"text-align: right;\">-0.00185822</td><td style=\"text-align: right;\"> 0.00935102</td><td style=\"text-align: right;\"> 0.082052   </td><td style=\"text-align: right;\"> 0.0183819  </td><td style=\"text-align: right;\"> 0.0333778 </td><td style=\"text-align: right;\"> 0.061037  </td><td style=\"text-align: right;\"> 0.075945  </td><td style=\"text-align: right;\">-0.00610073</td><td style=\"text-align: right;\">-0.0101115 </td><td style=\"text-align: right;\">-0.0699047  </td><td style=\"text-align: right;\">-0.0553159 </td><td style=\"text-align: right;\"> 0.0856078</td><td style=\"text-align: right;\"> 0.00822585</td><td style=\"text-align: right;\"> 0.00475824</td><td style=\"text-align: right;\"> 0.0200666 </td><td style=\"text-align: right;\"> 0.0720785 </td><td style=\"text-align: right;\">-0.032531   </td><td style=\"text-align: right;\"> 0.0176377 </td><td style=\"text-align: right;\">-0.0824495 </td><td style=\"text-align: right;\">-0.0609831 </td><td style=\"text-align: right;\">-0.0489734  </td><td style=\"text-align: right;\">-0.00171271</td><td style=\"text-align: right;\">-0.0809925 </td><td style=\"text-align: right;\">-0.0194329</td><td style=\"text-align: right;\">-0.135717  </td><td style=\"text-align: right;\">-0.141247 </td><td style=\"text-align: right;\">-0.00519   </td><td style=\"text-align: right;\">-0.00574296</td><td style=\"text-align: right;\">-0.013797  </td><td style=\"text-align: right;\"> 0.0419388 </td></tr>\n",
       "<tr><td>B002Q8JOSI </td><td>A17UQD2RSSQH5X</td><td>My dogs tell me these treats are YUMMY</td><td style=\"text-align: right;\">      5</td><td style=\"text-align: right;\">                       1</td><td style=\"text-align: right;\">212536</td><td>in the dark                                     </td><td style=\"text-align: right;\">                     1</td><td style=\"text-align: right;\">1.31613e+09</td><td>My two Corgis were thoroughly spoiled by my late husband (I spent a year and a half dieting them down a combined total of 25 pounds!)&lt;br /&gt;&lt;br /&gt;They are accustomed to the finest of fare, and they absolutely love the Wellness brand of treats.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          </td><td style=\"text-align: right;\">               1</td><td style=\"text-align: right;\">-0.0309179</td><td style=\"text-align: right;\">-0.0810856</td><td style=\"text-align: right;\"> 0.0519798  </td><td style=\"text-align: right;\">-0.0791674</td><td style=\"text-align: right;\"> 0.0543204 </td><td style=\"text-align: right;\"> 0.0862787 </td><td style=\"text-align: right;\">0.0263453 </td><td style=\"text-align: right;\">-0.0177047 </td><td style=\"text-align: right;\"> 0.00519268</td><td style=\"text-align: right;\">-0.0254443</td><td style=\"text-align: right;\"> 0.0199463 </td><td style=\"text-align: right;\">-0.0211617 </td><td style=\"text-align: right;\">-0.0321043</td><td style=\"text-align: right;\">-0.0181255 </td><td style=\"text-align: right;\">-0.0453995 </td><td style=\"text-align: right;\"> 0.0461298 </td><td style=\"text-align: right;\">-0.00489213</td><td style=\"text-align: right;\">-0.0400766  </td><td style=\"text-align: right;\"> 0.0182538 </td><td style=\"text-align: right;\"> 0.0475926 </td><td style=\"text-align: right;\"> 0.0108357  </td><td style=\"text-align: right;\">-0.00174376</td><td style=\"text-align: right;\">-0.00191385</td><td style=\"text-align: right;\">-0.0103976 </td><td style=\"text-align: right;\">0.0257366</td><td style=\"text-align: right;\"> 0.000308164</td><td style=\"text-align: right;\"> 0.0514098 </td><td style=\"text-align: right;\"> 0.021942 </td><td style=\"text-align: right;\">-0.0972761 </td><td style=\"text-align: right;\"> 0.0400901 </td><td style=\"text-align: right;\"> 0.052936 </td><td style=\"text-align: right;\"> 0.0800468</td><td style=\"text-align: right;\"> 0.0487992 </td><td style=\"text-align: right;\">-0.0146669</td><td style=\"text-align: right;\">-0.0145052</td><td style=\"text-align: right;\">-0.026453  </td><td style=\"text-align: right;\">-0.0644132 </td><td style=\"text-align: right;\"> 0.0697721 </td><td style=\"text-align: right;\"> 0.00545899</td><td style=\"text-align: right;\"> 0.0806206 </td><td style=\"text-align: right;\"> 0.000417893</td><td style=\"text-align: right;\">-0.0398172</td><td style=\"text-align: right;\">-0.029349 </td><td style=\"text-align: right;\">-0.00733253</td><td style=\"text-align: right;\"> 0.00645248 </td><td style=\"text-align: right;\"> 0.0265826 </td><td style=\"text-align: right;\">-0.0377316 </td><td style=\"text-align: right;\"> 0.0286597 </td><td style=\"text-align: right;\"> 0.0733544</td><td style=\"text-align: right;\">-0.0362965 </td><td style=\"text-align: right;\"> 0.0326724 </td><td style=\"text-align: right;\">-0.0122447 </td><td style=\"text-align: right;\">-0.0961499  </td><td style=\"text-align: right;\">-0.0484678 </td><td style=\"text-align: right;\"> 0.0637648</td><td style=\"text-align: right;\">0.06201    </td><td style=\"text-align: right;\"> 0.0843366  </td><td style=\"text-align: right;\">-0.0230949 </td><td style=\"text-align: right;\"> 0.0470019 </td><td style=\"text-align: right;\"> 0.0501421 </td><td style=\"text-align: right;\"> 0.0600014 </td><td style=\"text-align: right;\">-0.0037747 </td><td style=\"text-align: right;\"> 0.00772799</td><td style=\"text-align: right;\">-0.0456762</td><td style=\"text-align: right;\"> 0.0614015 </td><td style=\"text-align: right;\"> 0.00105499</td><td style=\"text-align: right;\">-0.0104708 </td><td style=\"text-align: right;\"> 0.0606603 </td><td style=\"text-align: right;\">-0.0405618 </td><td style=\"text-align: right;\">-0.0618486  </td><td style=\"text-align: right;\">-0.1031    </td><td style=\"text-align: right;\">-0.0161886 </td><td style=\"text-align: right;\"> 0.000711265</td><td style=\"text-align: right;\"> 0.00361975 </td><td style=\"text-align: right;\"> 0.00369064</td><td style=\"text-align: right;\"> 0.0490131 </td><td style=\"text-align: right;\"> 0.0379513 </td><td style=\"text-align: right;\"> 0.0110999 </td><td style=\"text-align: right;\">-0.00371494</td><td style=\"text-align: right;\">-0.0825619  </td><td style=\"text-align: right;\">-0.0790808 </td><td style=\"text-align: right;\"> 0.0469108</td><td style=\"text-align: right;\">-0.022734  </td><td style=\"text-align: right;\"> 0.0603649 </td><td style=\"text-align: right;\">-0.114043  </td><td style=\"text-align: right;\">-0.00663388</td><td style=\"text-align: right;\"> 0.00917094 </td><td style=\"text-align: right;\"> 0.0377548 </td><td style=\"text-align: right;\">-0.0374798 </td><td style=\"text-align: right;\">-0.0447417 </td><td style=\"text-align: right;\">-0.0374864  </td><td style=\"text-align: right;\">-0.00273842</td><td style=\"text-align: right;\">-0.0627456 </td><td style=\"text-align: right;\">-0.102185 </td><td style=\"text-align: right;\">-0.0259653 </td><td style=\"text-align: right;\"> 0.027384 </td><td style=\"text-align: right;\"> 0.00249847</td><td style=\"text-align: right;\"> 0.0650841 </td><td style=\"text-align: right;\"> 0.0272965 </td><td style=\"text-align: right;\"> 0.0104615 </td></tr>\n",
       "<tr><td>B00176G870 </td><td>A2F2MZW8EOGH5J</td><td>Yummy to the tummy                    </td><td style=\"text-align: right;\">      5</td><td style=\"text-align: right;\">                       0</td><td style=\"text-align: right;\">115971</td><td>daemoncycler &quot;When you arrive at a fork in th...</td><td style=\"text-align: right;\">                     0</td><td style=\"text-align: right;\">1.33479e+09</td><td>We used to have drive down to the specialty pet food store for this product.  So glad we discovered Amazon.  As far as I can tell it is no more expensive &amp; in some cases less - Prime membership is awesome.  Loving Pets treats are some of the best according to my dog.  They do not develop that nasty smell like some dog treats do.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  </td><td style=\"text-align: right;\">               1</td><td style=\"text-align: right;\">-0.11551  </td><td style=\"text-align: right;\">-0.103209 </td><td style=\"text-align: right;\">-0.0217979  </td><td style=\"text-align: right;\">-0.10862  </td><td style=\"text-align: right;\"> 0.0496529 </td><td style=\"text-align: right;\"> 0.0959668 </td><td style=\"text-align: right;\">0.0904129 </td><td style=\"text-align: right;\">-0.0550773 </td><td style=\"text-align: right;\">-0.00213047</td><td style=\"text-align: right;\"> 0.0168331</td><td style=\"text-align: right;\"> 0.0725579 </td><td style=\"text-align: right;\"> 0.0424349 </td><td style=\"text-align: right;\">-0.0100934</td><td style=\"text-align: right;\">-0.0144199 </td><td style=\"text-align: right;\"> 0.0511903 </td><td style=\"text-align: right;\"> 0.0093318 </td><td style=\"text-align: right;\"> 0.0202627 </td><td style=\"text-align: right;\"> 0.04766    </td><td style=\"text-align: right;\"> 0.0375933 </td><td style=\"text-align: right;\"> 0.00826972</td><td style=\"text-align: right;\">-0.0370724  </td><td style=\"text-align: right;\"> 0.0220989 </td><td style=\"text-align: right;\">-0.036785  </td><td style=\"text-align: right;\">-0.0709478 </td><td style=\"text-align: right;\">0.044946 </td><td style=\"text-align: right;\"> 0.0648468  </td><td style=\"text-align: right;\">-0.0159494 </td><td style=\"text-align: right;\"> 0.0462118</td><td style=\"text-align: right;\">-0.0305758 </td><td style=\"text-align: right;\">-0.0245026 </td><td style=\"text-align: right;\"> 0.0178325</td><td style=\"text-align: right;\"> 0.0640569</td><td style=\"text-align: right;\"> 0.0943563 </td><td style=\"text-align: right;\"> 0.0656025</td><td style=\"text-align: right;\"> 0.10241  </td><td style=\"text-align: right;\"> 0.0854648 </td><td style=\"text-align: right;\">-0.0748227 </td><td style=\"text-align: right;\"> 0.0486059 </td><td style=\"text-align: right;\"> 0.0472177 </td><td style=\"text-align: right;\"> 0.0768321 </td><td style=\"text-align: right;\">-0.0444879  </td><td style=\"text-align: right;\">-0.0359536</td><td style=\"text-align: right;\">-0.0234699</td><td style=\"text-align: right;\">-0.0338359 </td><td style=\"text-align: right;\"> 0.0367042  </td><td style=\"text-align: right;\"> 0.0289177 </td><td style=\"text-align: right;\">-0.04527   </td><td style=\"text-align: right;\"> 0.0202352 </td><td style=\"text-align: right;\"> 0.119556 </td><td style=\"text-align: right;\"> 0.00887236</td><td style=\"text-align: right;\">-0.00283779</td><td style=\"text-align: right;\">-0.00529048</td><td style=\"text-align: right;\">-0.0224565  </td><td style=\"text-align: right;\"> 0.0298564 </td><td style=\"text-align: right;\"> 0.0364431</td><td style=\"text-align: right;\">0.0882008  </td><td style=\"text-align: right;\"> 0.006838   </td><td style=\"text-align: right;\">-0.0339969 </td><td style=\"text-align: right;\">-0.00935334</td><td style=\"text-align: right;\"> 0.0984409 </td><td style=\"text-align: right;\">-0.0589043 </td><td style=\"text-align: right;\">-0.0333364 </td><td style=\"text-align: right;\"> 0.00156149</td><td style=\"text-align: right;\">-0.080586 </td><td style=\"text-align: right;\"> 0.0529284 </td><td style=\"text-align: right;\">-0.015315  </td><td style=\"text-align: right;\">-0.1275    </td><td style=\"text-align: right;\">-0.022122  </td><td style=\"text-align: right;\">-0.0367261 </td><td style=\"text-align: right;\">-0.0544387  </td><td style=\"text-align: right;\"> 0.0218958 </td><td style=\"text-align: right;\">-0.0849173 </td><td style=\"text-align: right;\"> 0.0620345  </td><td style=\"text-align: right;\">-0.156168   </td><td style=\"text-align: right;\"> 0.0497484 </td><td style=\"text-align: right;\"> 0.0927496 </td><td style=\"text-align: right;\">-0.0404856 </td><td style=\"text-align: right;\"> 0.0248379 </td><td style=\"text-align: right;\">-0.0919703 </td><td style=\"text-align: right;\">-0.110382   </td><td style=\"text-align: right;\">-0.00449655</td><td style=\"text-align: right;\"> 0.0135124</td><td style=\"text-align: right;\">-0.00205705</td><td style=\"text-align: right;\"> 0.0367232 </td><td style=\"text-align: right;\">-0.0394602 </td><td style=\"text-align: right;\"> 0.0806931 </td><td style=\"text-align: right;\"> 0.0314342  </td><td style=\"text-align: right;\"> 0.00815701</td><td style=\"text-align: right;\">-0.0659654 </td><td style=\"text-align: right;\">-0.0452842 </td><td style=\"text-align: right;\">-0.0377819  </td><td style=\"text-align: right;\">-0.0407218 </td><td style=\"text-align: right;\">-0.111156  </td><td style=\"text-align: right;\">-0.0758596</td><td style=\"text-align: right;\">-0.132259  </td><td style=\"text-align: right;\">-0.0236822</td><td style=\"text-align: right;\"> 0.055836  </td><td style=\"text-align: right;\"> 0.0346567 </td><td style=\"text-align: right;\">-0.0639778 </td><td style=\"text-align: right;\"> 0.0541064 </td></tr>\n",
       "<tr><td>B001CHFUGY </td><td>A2M8VROSDPU4JT</td><td>Very good coffee                      </td><td style=\"text-align: right;\">      5</td><td style=\"text-align: right;\">                       1</td><td style=\"text-align: right;\">434484</td><td>Officefan &quot;Officefankt&quot;                         </td><td style=\"text-align: right;\">                     1</td><td style=\"text-align: right;\">1.27725e+09</td><td>I really liked this coffee, it was just as good as everyone claimed it was.  Strong, bold and flavorful!  I would recommend!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                </td><td style=\"text-align: right;\">               1</td><td style=\"text-align: right;\">-0.0271665</td><td style=\"text-align: right;\">-0.0715593</td><td style=\"text-align: right;\">-0.01502    </td><td style=\"text-align: right;\">-0.0219397</td><td style=\"text-align: right;\">-0.00653011</td><td style=\"text-align: right;\"> 0.00663928</td><td style=\"text-align: right;\">0.064415  </td><td style=\"text-align: right;\">-0.0118031 </td><td style=\"text-align: right;\">-0.056292  </td><td style=\"text-align: right;\">-0.0242536</td><td style=\"text-align: right;\"> 0.0137826 </td><td style=\"text-align: right;\"> 0.0137419 </td><td style=\"text-align: right;\"> 0.023325 </td><td style=\"text-align: right;\"> 0.0587478 </td><td style=\"text-align: right;\"> 0.0379234 </td><td style=\"text-align: right;\">-0.00643318</td><td style=\"text-align: right;\"> 0.0721844 </td><td style=\"text-align: right;\">-0.0149227  </td><td style=\"text-align: right;\"> 0.0244708 </td><td style=\"text-align: right;\"> 0.059416  </td><td style=\"text-align: right;\"> 0.000335146</td><td style=\"text-align: right;\">-0.0296565 </td><td style=\"text-align: right;\">-0.0195144 </td><td style=\"text-align: right;\">-0.0248383 </td><td style=\"text-align: right;\">0.0179456</td><td style=\"text-align: right;\">-0.0087398  </td><td style=\"text-align: right;\">-0.0430475 </td><td style=\"text-align: right;\"> 0.033994 </td><td style=\"text-align: right;\">-0.0316713 </td><td style=\"text-align: right;\">-0.00418097</td><td style=\"text-align: right;\"> 0.0658778</td><td style=\"text-align: right;\"> 0.0623851</td><td style=\"text-align: right;\">-0.00473747</td><td style=\"text-align: right;\"> 0.0591093</td><td style=\"text-align: right;\">-0.0241865</td><td style=\"text-align: right;\">-0.00316702</td><td style=\"text-align: right;\">-0.0579521 </td><td style=\"text-align: right;\">-0.0204084 </td><td style=\"text-align: right;\"> 0.027988  </td><td style=\"text-align: right;\"> 0.00784992</td><td style=\"text-align: right;\">-0.00580743 </td><td style=\"text-align: right;\"> 0.0270755</td><td style=\"text-align: right;\"> 0.0171478</td><td style=\"text-align: right;\"> 0.00696955</td><td style=\"text-align: right;\"> 0.00558229 </td><td style=\"text-align: right;\"> 0.0362168 </td><td style=\"text-align: right;\">-0.0106265 </td><td style=\"text-align: right;\"> 0.0600469 </td><td style=\"text-align: right;\"> 0.0431207</td><td style=\"text-align: right;\"> 0.0101809 </td><td style=\"text-align: right;\">-0.00464169</td><td style=\"text-align: right;\">-0.0358662 </td><td style=\"text-align: right;\">-0.0106336  </td><td style=\"text-align: right;\"> 0.0669514 </td><td style=\"text-align: right;\"> 0.042197 </td><td style=\"text-align: right;\">0.0309503  </td><td style=\"text-align: right;\"> 0.0346716  </td><td style=\"text-align: right;\">-0.0752697 </td><td style=\"text-align: right;\"> 0.0057241 </td><td style=\"text-align: right;\"> 0.0471554 </td><td style=\"text-align: right;\">-0.0112942 </td><td style=\"text-align: right;\">-0.00108046</td><td style=\"text-align: right;\"> 0.0417801 </td><td style=\"text-align: right;\">-0.0211523</td><td style=\"text-align: right;\"> 0.0323718 </td><td style=\"text-align: right;\"> 0.0311995 </td><td style=\"text-align: right;\">-0.0509989 </td><td style=\"text-align: right;\">-0.025337  </td><td style=\"text-align: right;\"> 0.00414944</td><td style=\"text-align: right;\"> 0.0255781  </td><td style=\"text-align: right;\"> 0.0131465 </td><td style=\"text-align: right;\">-0.0291046 </td><td style=\"text-align: right;\"> 0.0437848  </td><td style=\"text-align: right;\">-0.0598943  </td><td style=\"text-align: right;\">-0.0296691 </td><td style=\"text-align: right;\"> 0.0669362 </td><td style=\"text-align: right;\"> 0.00339839</td><td style=\"text-align: right;\"> 0.0367119 </td><td style=\"text-align: right;\">-0.0161436 </td><td style=\"text-align: right;\"> 0.000302148</td><td style=\"text-align: right;\"> 0.0362359 </td><td style=\"text-align: right;\"> 0.0007814</td><td style=\"text-align: right;\">-0.0177886 </td><td style=\"text-align: right;\"> 0.00220729</td><td style=\"text-align: right;\">-0.0121867 </td><td style=\"text-align: right;\"> 0.0590229 </td><td style=\"text-align: right;\"> 0.0652464  </td><td style=\"text-align: right;\"> 0.0486946 </td><td style=\"text-align: right;\">-0.0275727 </td><td style=\"text-align: right;\">-0.0269575 </td><td style=\"text-align: right;\">-0.0469296  </td><td style=\"text-align: right;\"> 0.0579083 </td><td style=\"text-align: right;\">-0.0410561 </td><td style=\"text-align: right;\">-0.0355459</td><td style=\"text-align: right;\">-0.062257  </td><td style=\"text-align: right;\">-0.0426228</td><td style=\"text-align: right;\"> 0.0321483 </td><td style=\"text-align: right;\"> 0.0136793 </td><td style=\"text-align: right;\">-0.0375067 </td><td style=\"text-align: right;\"> 0.046843  </td></tr>\n",
       "<tr><td>B0041CIR62 </td><td>A16I6WJUEBJ1C3</td><td>okay but not as healthy as it appears </td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">                       1</td><td style=\"text-align: right;\">138997</td><td>doctorsirena &quot;doctorsirena&quot;                     </td><td style=\"text-align: right;\">                     1</td><td style=\"text-align: right;\">1.34369e+09</td><td>I am always looking for healthier, whole grain versions of foods I enjoy.  Unfortunately, these Peacock brand noodles are yet another food masquerading as healthy.  The product title in big letters on the front says &quot;Brown Rice Vermicelli&quot;, making the consumer think &quot;this is made with brown rice, so it should be a healthy choice&quot;.  But the first indication that it is not is when looking at the fiber content on the nutrition facts - only 0.6g per 2oz serving.  Then onto the ingredients list to see why so low... contains brown rice, sago starch and water.  The sago starch comes from palms and must not have much (if any) fiber.&lt;br /&gt;&lt;br /&gt;The Annie Chun&#x27;s Maifun Brown Rice Noodles (sold on Amazon and in my local healthy grocer) has become one of my staples and is my frame of reference when comparing to the Peacock brand.  The Annie Chun&#x27;s product is made with 100% whole grain, with ingredients brown rice flour and water.  Per 2oz serving, it has 4g fiber and pretty much the same calories and other nutrients as the Peacock brand.&lt;br /&gt;&lt;br /&gt;If you do try this Peacock brand noodles and have not used rice noodles before, you will need to seek guidance elsewhere on preparation.  As others have pointed out, the Peacock package gives almost no directions on how to prepare the product, aside from a brief mention in the recipes (in the header text it does say that they are &quot;easy-to-cook&quot; but does not say how).  It also contains a very strange recipe for rice noodles: Aglio Olio style - this is an Italian recipe for noodles with olive oil/garlic/sprinkled with grated cheese that I think would not be very tasty.  The second recipe appears to be for a soup with veggie strips.  Neither recipe gives amounts or much direction.  In comparison, the Annie Chun&#x27;s package gives clear, specific directions on rice noodle preparation and two recipes.&lt;br /&gt;&lt;br /&gt;I use rice noodles = maifun = rice sticks = sometimes called vermicelli for making the Vietnamese salad &quot;bun tofu&quot;, to serve with stir-fried veggies or in lettuce rolls.  They can also be used in spring rolls/egg rolls.  When cooking with thin rice noodles, be careful not to oversoak/overcook/overmix or they tend to disintegrate.  Asian rice noodle vermicelli (maifun) are not the same as Italian vermicelli and are not readily interchangeable.  If making an Italian recipe, the best results would be expected from Italian pasta and not maifun.&lt;br /&gt;&lt;br /&gt;A few final notes...  Both Peacock and Annie Chun&#x27;s brown rice noodles are gluten free.  The Peacock is made in Singapore and the Annie Chun&#x27;s in Thailand.  The Peacock noodles do taste fine (kind of bland), but so do the Annie Chun&#x27;s.  At this time, they are both approximately the same price.  Peacock come in an plastic bag with some noodle crushage upon shipping; Annie Chun&#x27;s are perfect upon removal from their cellophane bag in a box.  Overall, I highly recommend the Annie Chun&#x27;s Maifun as a healthier option over the Peacock brand.  On a related note, the Annie Chun&#x27;s soba and brown rice pad thai noodles are also excellent.&lt;br /&gt;&lt;br /&gt;Rating for this product: 2.5 stars rounded down to 2 stars.</td><td style=\"text-align: right;\">               0</td><td style=\"text-align: right;\">-0.347388 </td><td style=\"text-align: right;\">-0.467799 </td><td style=\"text-align: right;\"> 0.294629   </td><td style=\"text-align: right;\">-0.0508477</td><td style=\"text-align: right;\">-0.549432  </td><td style=\"text-align: right;\"> 0.273022  </td><td style=\"text-align: right;\">0.0739884 </td><td style=\"text-align: right;\"> 0.106726  </td><td style=\"text-align: right;\">-0.139928  </td><td style=\"text-align: right;\"> 0.555422 </td><td style=\"text-align: right;\">-0.337149  </td><td style=\"text-align: right;\"> 0.477425  </td><td style=\"text-align: right;\">-0.135502 </td><td style=\"text-align: right;\"> 0.0657571 </td><td style=\"text-align: right;\"> 0.183615  </td><td style=\"text-align: right;\"> 0.10903   </td><td style=\"text-align: right;\"> 0.321535  </td><td style=\"text-align: right;\">-0.148126   </td><td style=\"text-align: right;\"> 0.560268  </td><td style=\"text-align: right;\"> 0.214594  </td><td style=\"text-align: right;\">-0.10708    </td><td style=\"text-align: right;\">-0.0787899 </td><td style=\"text-align: right;\"> 0.243522  </td><td style=\"text-align: right;\">-0.365625  </td><td style=\"text-align: right;\">0.577176 </td><td style=\"text-align: right;\"> 0.0365318  </td><td style=\"text-align: right;\"> 0.384215  </td><td style=\"text-align: right;\"> 0.62793  </td><td style=\"text-align: right;\">-0.153693  </td><td style=\"text-align: right;\">-0.294441  </td><td style=\"text-align: right;\"> 0.475701 </td><td style=\"text-align: right;\"> 0.583549 </td><td style=\"text-align: right;\">-0.472268  </td><td style=\"text-align: right;\">-0.190435 </td><td style=\"text-align: right;\"> 0.676963 </td><td style=\"text-align: right;\"> 0.0720167 </td><td style=\"text-align: right;\">-0.492958  </td><td style=\"text-align: right;\"> 0.191135  </td><td style=\"text-align: right;\">-0.0192992 </td><td style=\"text-align: right;\">-0.341615  </td><td style=\"text-align: right;\">-0.106094   </td><td style=\"text-align: right;\"> 0.439104 </td><td style=\"text-align: right;\"> 0.162949 </td><td style=\"text-align: right;\"> 0.0621371 </td><td style=\"text-align: right;\">-0.159077   </td><td style=\"text-align: right;\"> 0.253311  </td><td style=\"text-align: right;\"> 0.0693506 </td><td style=\"text-align: right;\"> 0.349589  </td><td style=\"text-align: right;\"> 0.162353 </td><td style=\"text-align: right;\"> 0.561213  </td><td style=\"text-align: right;\">-0.484451  </td><td style=\"text-align: right;\"> 0.034443  </td><td style=\"text-align: right;\"> 0.433276   </td><td style=\"text-align: right;\">-0.435644  </td><td style=\"text-align: right;\"> 0.0666417</td><td style=\"text-align: right;\">0.0542881  </td><td style=\"text-align: right;\"> 0.0995261  </td><td style=\"text-align: right;\"> 0.0581386 </td><td style=\"text-align: right;\">-0.15954   </td><td style=\"text-align: right;\">-0.0351755 </td><td style=\"text-align: right;\">-0.282065  </td><td style=\"text-align: right;\"> 0.180399  </td><td style=\"text-align: right;\"> 0.426229  </td><td style=\"text-align: right;\">-0.17142  </td><td style=\"text-align: right;\"> 0.03626   </td><td style=\"text-align: right;\">-0.174972  </td><td style=\"text-align: right;\">-0.141704  </td><td style=\"text-align: right;\"> 0.446467  </td><td style=\"text-align: right;\"> 0.113216  </td><td style=\"text-align: right;\"> 0.356234   </td><td style=\"text-align: right;\"> 0.164735  </td><td style=\"text-align: right;\">-0.403647  </td><td style=\"text-align: right;\">-0.120722   </td><td style=\"text-align: right;\"> 0.183498   </td><td style=\"text-align: right;\">-0.242832  </td><td style=\"text-align: right;\">-0.153102  </td><td style=\"text-align: right;\">-0.257618  </td><td style=\"text-align: right;\"> 0.109427  </td><td style=\"text-align: right;\"> 0.346764  </td><td style=\"text-align: right;\">-0.815041   </td><td style=\"text-align: right;\">-0.252172  </td><td style=\"text-align: right;\"> 0.217485 </td><td style=\"text-align: right;\">-0.263408  </td><td style=\"text-align: right;\">-0.0771506 </td><td style=\"text-align: right;\"> 0.27373   </td><td style=\"text-align: right;\"> 0.181654  </td><td style=\"text-align: right;\"> 0.455834   </td><td style=\"text-align: right;\">-0.25125   </td><td style=\"text-align: right;\"> 0.0352405 </td><td style=\"text-align: right;\"> 0.0559777 </td><td style=\"text-align: right;\"> 0.0276021  </td><td style=\"text-align: right;\"> 0.0799172 </td><td style=\"text-align: right;\"> 0.158467  </td><td style=\"text-align: right;\">-0.0466917</td><td style=\"text-align: right;\"> 0.164104  </td><td style=\"text-align: right;\">-0.17693  </td><td style=\"text-align: right;\">-0.117714  </td><td style=\"text-align: right;\"> 0.194627  </td><td style=\"text-align: right;\"> 0.175274  </td><td style=\"text-align: right;\">-0.260962  </td></tr>\n",
       "<tr><td>B001R3BQFW </td><td>AM50E42AFUVNL </td><td>Taste great.                          </td><td style=\"text-align: right;\">      5</td><td style=\"text-align: right;\">                       0</td><td style=\"text-align: right;\">126555</td><td>T. Higley &quot;Tina&quot;                                </td><td style=\"text-align: right;\">                     0</td><td style=\"text-align: right;\">1.32356e+09</td><td>I have tried many different drink mix, this is the best tasting by far. It does not have the after taste of the sweetener and I really like it, it is pretty strong, so I use a big water bottle (20 oz) for one tube, it still a little stronger than I like, but it is just my taste.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     </td><td style=\"text-align: right;\">               1</td><td style=\"text-align: right;\"> 0.0287507</td><td style=\"text-align: right;\">-0.0984241</td><td style=\"text-align: right;\">-0.0131094  </td><td style=\"text-align: right;\">-0.140434 </td><td style=\"text-align: right;\"> 0.032088  </td><td style=\"text-align: right;\">-0.00821121</td><td style=\"text-align: right;\">0.0519656 </td><td style=\"text-align: right;\">-0.0179963 </td><td style=\"text-align: right;\">-0.157456  </td><td style=\"text-align: right;\">-0.0529391</td><td style=\"text-align: right;\"> 0.0875333 </td><td style=\"text-align: right;\"> 0.101104  </td><td style=\"text-align: right;\"> 0.0294293</td><td style=\"text-align: right;\"> 0.0654107 </td><td style=\"text-align: right;\"> 0.020087  </td><td style=\"text-align: right;\"> 0.0285074 </td><td style=\"text-align: right;\"> 0.00359086</td><td style=\"text-align: right;\">-0.0614701  </td><td style=\"text-align: right;\">-0.0397451 </td><td style=\"text-align: right;\"> 0.0210309 </td><td style=\"text-align: right;\">-0.0891761  </td><td style=\"text-align: right;\">-0.00247209</td><td style=\"text-align: right;\">-0.0470436 </td><td style=\"text-align: right;\">-0.106461  </td><td style=\"text-align: right;\">0.0952522</td><td style=\"text-align: right;\"> 0.0953231  </td><td style=\"text-align: right;\"> 0.00201553</td><td style=\"text-align: right;\"> 0.103108 </td><td style=\"text-align: right;\">-0.135367  </td><td style=\"text-align: right;\"> 0.0444165 </td><td style=\"text-align: right;\"> 0.058865 </td><td style=\"text-align: right;\"> 0.124896 </td><td style=\"text-align: right;\"> 0.0373406 </td><td style=\"text-align: right;\">-0.0276751</td><td style=\"text-align: right;\"> 0.01242  </td><td style=\"text-align: right;\">-0.0345862 </td><td style=\"text-align: right;\">-0.100594  </td><td style=\"text-align: right;\"> 0.0253074 </td><td style=\"text-align: right;\">-0.0204754 </td><td style=\"text-align: right;\">-0.0226803 </td><td style=\"text-align: right;\">-0.0628609  </td><td style=\"text-align: right;\">-0.0265512</td><td style=\"text-align: right;\">-0.0240165</td><td style=\"text-align: right;\">-0.00729808</td><td style=\"text-align: right;\"> 0.0283721  </td><td style=\"text-align: right;\"> 0.0945689 </td><td style=\"text-align: right;\">-0.150412  </td><td style=\"text-align: right;\"> 0.23747   </td><td style=\"text-align: right;\"> 0.118083 </td><td style=\"text-align: right;\">-0.00427646</td><td style=\"text-align: right;\">-0.0189591 </td><td style=\"text-align: right;\">-0.023018  </td><td style=\"text-align: right;\"> 0.000834472</td><td style=\"text-align: right;\"> 0.0919989 </td><td style=\"text-align: right;\"> 0.0218403</td><td style=\"text-align: right;\">0.0787006  </td><td style=\"text-align: right;\"> 0.0577623  </td><td style=\"text-align: right;\">-0.091306  </td><td style=\"text-align: right;\">-0.153486  </td><td style=\"text-align: right;\"> 0.101591  </td><td style=\"text-align: right;\"> 0.0155503 </td><td style=\"text-align: right;\"> 0.00996294</td><td style=\"text-align: right;\"> 0.0955996 </td><td style=\"text-align: right;\">-0.172914 </td><td style=\"text-align: right;\"> 0.0443104 </td><td style=\"text-align: right;\"> 0.0361872 </td><td style=\"text-align: right;\">-0.095014  </td><td style=\"text-align: right;\"> 0.0324434 </td><td style=\"text-align: right;\"> 0.0275206 </td><td style=\"text-align: right;\">-0.0524526  </td><td style=\"text-align: right;\">-0.0616486 </td><td style=\"text-align: right;\">-0.097717  </td><td style=\"text-align: right;\"> 0.00936316 </td><td style=\"text-align: right;\">-0.0802194  </td><td style=\"text-align: right;\"> 0.0394673 </td><td style=\"text-align: right;\"> 0.0821349 </td><td style=\"text-align: right;\"> 0.00616072</td><td style=\"text-align: right;\"> 0.163339  </td><td style=\"text-align: right;\">-0.030864  </td><td style=\"text-align: right;\">-0.168387   </td><td style=\"text-align: right;\">-0.00379715</td><td style=\"text-align: right;\">-0.0483934</td><td style=\"text-align: right;\"> 0.0485483 </td><td style=\"text-align: right;\">-0.0299181 </td><td style=\"text-align: right;\">-0.0589156 </td><td style=\"text-align: right;\"> 0.0726873 </td><td style=\"text-align: right;\"> 0.187286   </td><td style=\"text-align: right;\"> 0.0563804 </td><td style=\"text-align: right;\">-0.00693958</td><td style=\"text-align: right;\">-0.087817  </td><td style=\"text-align: right;\">-0.0505065  </td><td style=\"text-align: right;\">-0.0444383 </td><td style=\"text-align: right;\">-0.0868177 </td><td style=\"text-align: right;\">-0.0928413</td><td style=\"text-align: right;\">-0.104858  </td><td style=\"text-align: right;\">-0.0653646</td><td style=\"text-align: right;\">-0.00770999</td><td style=\"text-align: right;\"> 0.0421915 </td><td style=\"text-align: right;\">-0.0660812 </td><td style=\"text-align: right;\"> 0.110634  </td></tr>\n",
       "<tr><td>B005HGAV8I </td><td>A2I5KDNOESGJ1H</td><td>variety galore                        </td><td style=\"text-align: right;\">      5</td><td style=\"text-align: right;\">                       1</td><td style=\"text-align: right;\">438837</td><td>TJ                                              </td><td style=\"text-align: right;\">                     1</td><td style=\"text-align: right;\">1.33402e+09</td><td>This is my favorite item to order for my Keurig. There are so many flavors, my finicky palate never gets bored!  The only downside is there are probably 5-6 decaf varieties.  I don&#x27;t drink decaf (I REQUIRE copious amounts of caffeine), so they sit on the shelf...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     </td><td style=\"text-align: right;\">               1</td><td style=\"text-align: right;\">-0.0328947</td><td style=\"text-align: right;\">-0.0831819</td><td style=\"text-align: right;\">-0.0657024  </td><td style=\"text-align: right;\"> 0.0483565</td><td style=\"text-align: right;\"> 0.00866472</td><td style=\"text-align: right;\"> 0.00432921</td><td style=\"text-align: right;\">0.00679293</td><td style=\"text-align: right;\"> 0.0463    </td><td style=\"text-align: right;\">-0.0232819 </td><td style=\"text-align: right;\">-0.0187351</td><td style=\"text-align: right;\">-0.0582966 </td><td style=\"text-align: right;\">-0.0361236 </td><td style=\"text-align: right;\"> 0.0557662</td><td style=\"text-align: right;\"> 0.00515392</td><td style=\"text-align: right;\"> 0.0165853 </td><td style=\"text-align: right;\"> 0.0382432 </td><td style=\"text-align: right;\"> 0.0404403 </td><td style=\"text-align: right;\">-0.039269   </td><td style=\"text-align: right;\">-0.00173118</td><td style=\"text-align: right;\">-0.0287118 </td><td style=\"text-align: right;\"> 0.0837229  </td><td style=\"text-align: right;\">-0.0285277 </td><td style=\"text-align: right;\">-0.0824551 </td><td style=\"text-align: right;\">-0.030766  </td><td style=\"text-align: right;\">0.035011 </td><td style=\"text-align: right;\"> 0.021678   </td><td style=\"text-align: right;\">-0.0169549 </td><td style=\"text-align: right;\">-0.079079 </td><td style=\"text-align: right;\"> 0.0211688 </td><td style=\"text-align: right;\"> 0.0370679 </td><td style=\"text-align: right;\">-0.0313245</td><td style=\"text-align: right;\">-0.0425382</td><td style=\"text-align: right;\">-0.0928767 </td><td style=\"text-align: right;\"> 0.0300581</td><td style=\"text-align: right;\">-0.0500493</td><td style=\"text-align: right;\"> 0.0209051 </td><td style=\"text-align: right;\">-0.00214003</td><td style=\"text-align: right;\">-0.0241631 </td><td style=\"text-align: right;\">-0.0239672 </td><td style=\"text-align: right;\"> 0.0522056 </td><td style=\"text-align: right;\"> 0.16119    </td><td style=\"text-align: right;\">-0.0537048</td><td style=\"text-align: right;\"> 0.139301 </td><td style=\"text-align: right;\">-0.0274696 </td><td style=\"text-align: right;\">-0.0455327  </td><td style=\"text-align: right;\">-0.0853248 </td><td style=\"text-align: right;\">-0.0425497 </td><td style=\"text-align: right;\">-0.0115379 </td><td style=\"text-align: right;\">-0.0650871</td><td style=\"text-align: right;\"> 0.0504527 </td><td style=\"text-align: right;\">-0.0385439 </td><td style=\"text-align: right;\">-0.00974706</td><td style=\"text-align: right;\">-0.0149668  </td><td style=\"text-align: right;\"> 0.100649  </td><td style=\"text-align: right;\"> 0.0312184</td><td style=\"text-align: right;\">0.000178409</td><td style=\"text-align: right;\">-0.0527072  </td><td style=\"text-align: right;\"> 0.00965293</td><td style=\"text-align: right;\">-0.0346073 </td><td style=\"text-align: right;\"> 0.00702845</td><td style=\"text-align: right;\">-0.00548485</td><td style=\"text-align: right;\"> 0.00684741</td><td style=\"text-align: right;\"> 0.0768864 </td><td style=\"text-align: right;\"> 0.0085376</td><td style=\"text-align: right;\">-0.0548922 </td><td style=\"text-align: right;\">-0.016292  </td><td style=\"text-align: right;\">-0.00399412</td><td style=\"text-align: right;\">-0.00953593</td><td style=\"text-align: right;\"> 0.0637293 </td><td style=\"text-align: right;\"> 0.108314   </td><td style=\"text-align: right;\">-0.0983645 </td><td style=\"text-align: right;\">-0.110933  </td><td style=\"text-align: right;\">-0.0113344  </td><td style=\"text-align: right;\"> 0.0616389  </td><td style=\"text-align: right;\">-0.0184045 </td><td style=\"text-align: right;\"> 0.0148369 </td><td style=\"text-align: right;\">-0.0367488 </td><td style=\"text-align: right;\"> 0.0348676 </td><td style=\"text-align: right;\"> 0.0233796 </td><td style=\"text-align: right;\">-0.0102339  </td><td style=\"text-align: right;\">-0.0673294 </td><td style=\"text-align: right;\"> 0.0316206</td><td style=\"text-align: right;\"> 0.0210643 </td><td style=\"text-align: right;\"> 0.0153875 </td><td style=\"text-align: right;\">-0.0285707 </td><td style=\"text-align: right;\">-0.0785177 </td><td style=\"text-align: right;\"> 0.000771454</td><td style=\"text-align: right;\">-0.0288366 </td><td style=\"text-align: right;\"> 0.05785   </td><td style=\"text-align: right;\"> 0.00874031</td><td style=\"text-align: right;\">-0.00115476 </td><td style=\"text-align: right;\">-0.0740344 </td><td style=\"text-align: right;\">-0.054432  </td><td style=\"text-align: right;\">-0.0373666</td><td style=\"text-align: right;\"> 0.0365672 </td><td style=\"text-align: right;\"> 0.0480539</td><td style=\"text-align: right;\">-0.0530344 </td><td style=\"text-align: right;\">-0.0718874 </td><td style=\"text-align: right;\"> 0.0502951 </td><td style=\"text-align: right;\"> 0.0419813 </td></tr>\n",
       "<tr><td>B000GFYRHQ </td><td>A3A7YUR6FS6ZCI</td><td>Bigelow Earl Grey Green Tea           </td><td style=\"text-align: right;\">      5</td><td style=\"text-align: right;\">                       0</td><td style=\"text-align: right;\">245379</td><td>Tea Lover                                       </td><td style=\"text-align: right;\">                     0</td><td style=\"text-align: right;\">1.17841e+09</td><td>Tastes like Earl Grey, but it&#x27;s green tea so it&#x27;s healthier.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                </td><td style=\"text-align: right;\">               1</td><td style=\"text-align: right;\">-0.010849 </td><td style=\"text-align: right;\">-0.0232738</td><td style=\"text-align: right;\"> 0.000933504</td><td style=\"text-align: right;\">-0.015408 </td><td style=\"text-align: right;\"> 0.0155034 </td><td style=\"text-align: right;\"> 0.00391342</td><td style=\"text-align: right;\">0.033525  </td><td style=\"text-align: right;\">-0.00831183</td><td style=\"text-align: right;\">-0.017343  </td><td style=\"text-align: right;\"> 0.0123838</td><td style=\"text-align: right;\">-0.00085218</td><td style=\"text-align: right;\">-0.00069983</td><td style=\"text-align: right;\">-0.0136244</td><td style=\"text-align: right;\">-0.0347349 </td><td style=\"text-align: right;\"> 0.021315  </td><td style=\"text-align: right;\"> 0.00773331</td><td style=\"text-align: right;\"> 0.042354  </td><td style=\"text-align: right;\">-0.000175536</td><td style=\"text-align: right;\">-0.0266608 </td><td style=\"text-align: right;\">-0.0157081 </td><td style=\"text-align: right;\">-0.0139647  </td><td style=\"text-align: right;\"> 0.0152316 </td><td style=\"text-align: right;\"> 0.00201933</td><td style=\"text-align: right;\">-0.00151071</td><td style=\"text-align: right;\">0.0398679</td><td style=\"text-align: right;\"> 0.0317084  </td><td style=\"text-align: right;\">-0.0392553 </td><td style=\"text-align: right;\"> 0.0140407</td><td style=\"text-align: right;\">-0.0350026 </td><td style=\"text-align: right;\">-0.0233569 </td><td style=\"text-align: right;\"> 0.0170909</td><td style=\"text-align: right;\"> 0.0800934</td><td style=\"text-align: right;\"> 0.0218795 </td><td style=\"text-align: right;\">-0.0053316</td><td style=\"text-align: right;\"> 0.0111653</td><td style=\"text-align: right;\"> 0.0143114 </td><td style=\"text-align: right;\">-0.0325279 </td><td style=\"text-align: right;\"> 0.00276845</td><td style=\"text-align: right;\"> 0.022113  </td><td style=\"text-align: right;\"> 0.00528838</td><td style=\"text-align: right;\">-0.0284193  </td><td style=\"text-align: right;\">-0.0107281</td><td style=\"text-align: right;\">-0.0098034</td><td style=\"text-align: right;\"> 0.00621621</td><td style=\"text-align: right;\"> 0.0083606  </td><td style=\"text-align: right;\"> 0.039228  </td><td style=\"text-align: right;\"> 0.00197698</td><td style=\"text-align: right;\"> 0.0490072 </td><td style=\"text-align: right;\"> 0.0175817</td><td style=\"text-align: right;\"> 0.010289  </td><td style=\"text-align: right;\">-0.032669  </td><td style=\"text-align: right;\">-0.00270637</td><td style=\"text-align: right;\">-0.0113041  </td><td style=\"text-align: right;\">-0.016245  </td><td style=\"text-align: right;\"> 0.0141794</td><td style=\"text-align: right;\">0.00558253 </td><td style=\"text-align: right;\"> 0.0180413  </td><td style=\"text-align: right;\">-0.060513  </td><td style=\"text-align: right;\">-0.0159885 </td><td style=\"text-align: right;\"> 0.0332582 </td><td style=\"text-align: right;\">-0.0105916 </td><td style=\"text-align: right;\">-0.0170461 </td><td style=\"text-align: right;\"> 0.0407877 </td><td style=\"text-align: right;\">-0.0586863</td><td style=\"text-align: right;\">-0.015464  </td><td style=\"text-align: right;\"> 0.00888863</td><td style=\"text-align: right;\">-0.0311525 </td><td style=\"text-align: right;\"> 0.0151116 </td><td style=\"text-align: right;\"> 0.0109512 </td><td style=\"text-align: right;\">-0.0105238  </td><td style=\"text-align: right;\"> 0.0342968 </td><td style=\"text-align: right;\">-0.0130999 </td><td style=\"text-align: right;\">-0.0322065  </td><td style=\"text-align: right;\"> 0.00361061 </td><td style=\"text-align: right;\"> 0.00107861</td><td style=\"text-align: right;\"> 0.00810959</td><td style=\"text-align: right;\">-0.0072322 </td><td style=\"text-align: right;\">-0.00489803</td><td style=\"text-align: right;\"> 0.00824841</td><td style=\"text-align: right;\">-0.0125807  </td><td style=\"text-align: right;\"> 0.0129282 </td><td style=\"text-align: right;\">-0.0189013</td><td style=\"text-align: right;\"> 0.0124712 </td><td style=\"text-align: right;\">-0.00703676</td><td style=\"text-align: right;\"> 0.0261281 </td><td style=\"text-align: right;\"> 0.0087335 </td><td style=\"text-align: right;\"> 0.0393472  </td><td style=\"text-align: right;\"> 0.00104361</td><td style=\"text-align: right;\">-0.0132111 </td><td style=\"text-align: right;\">-0.0153004 </td><td style=\"text-align: right;\">-0.0157904  </td><td style=\"text-align: right;\"> 0.0280714 </td><td style=\"text-align: right;\">-0.00172982</td><td style=\"text-align: right;\"> 0.0297739</td><td style=\"text-align: right;\">-0.00797018</td><td style=\"text-align: right;\"> 0.0093278</td><td style=\"text-align: right;\"> 0.0190008 </td><td style=\"text-align: right;\"> 0.0061181 </td><td style=\"text-align: right;\"> 0.0121204 </td><td style=\"text-align: right;\">-0.023878  </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate a vector for each review\n",
    "review_vecs = h2o.H2OFrame(vectorizer_model2.transform([pdf.fillna(\"\")['Text']]))\n",
    "# Add the review vectors to the original dataframe\n",
    "# Add aggregated word embeddings \n",
    "ext_reviews = reviews.cbind(review_vecs)\n",
    "ext_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: GBM with Review vectors\n",
    "<blockquote>\n",
    "    Now we can train a GBM like before, but include the review vectors. This should hopefully increase improvement! We'll log everything to mlflow so we can compare the results.\n",
    "    </i></br><footer>Splice Machine</footer></blockquote><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Code Block train_time... gbm Model Build progress: |███████████████████████████████████████████████| 100%\n",
      "Done.\n",
      "Code Block train_time:\n",
      "Ran in 7.378 secs\n",
      "Ran in 0.123 mins\n",
      "Saving artifact of size: 1598.677 KB to Splice Machine DB\n",
      "Saving artifact of size: 802.924 KB to Splice Machine DB\n",
      "Model Details\n",
      "=============\n",
      "H2OGradientBoostingEstimator :  Gradient Boosting Machine\n",
      "Model Key:  gbm_embeddings.hex\n",
      "\n",
      "\n",
      "Model Summary: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>number_of_trees</th>\n",
       "      <th>number_of_internal_trees</th>\n",
       "      <th>model_size_in_bytes</th>\n",
       "      <th>min_depth</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>mean_depth</th>\n",
       "      <th>min_leaves</th>\n",
       "      <th>max_leaves</th>\n",
       "      <th>mean_leaves</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>22988.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>31.92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     number_of_trees  number_of_internal_trees  model_size_in_bytes  \\\n",
       "0               50.0                      50.0              22988.0   \n",
       "\n",
       "   min_depth  max_depth  mean_depth  min_leaves  max_leaves  mean_leaves  \n",
       "0        5.0        5.0         5.0        31.0        32.0        31.92  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ModelMetricsBinomial: gbm\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 0.11572445963957394\n",
      "RMSE: 0.3401829796441526\n",
      "LogLoss: 0.3760365771055801\n",
      "Mean Per-Class Error: 0.2099203987872652\n",
      "AUC: 0.871821488073347\n",
      "AUCPR: 0.9559641584492758\n",
      "Gini: 0.7436429761466941\n",
      "\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.5982309112378869: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>Error</th>\n",
       "      <th>Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>6921.0</td>\n",
       "      <td>8298.0</td>\n",
       "      <td>0.5452</td>\n",
       "      <td>(8298.0/15219.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2126.0</td>\n",
       "      <td>52683.0</td>\n",
       "      <td>0.0388</td>\n",
       "      <td>(2126.0/54809.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Total</td>\n",
       "      <td>9047.0</td>\n",
       "      <td>60981.0</td>\n",
       "      <td>0.1489</td>\n",
       "      <td>(10424.0/70028.0)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0        1   Error                Rate\n",
       "0      0  6921.0   8298.0  0.5452    (8298.0/15219.0)\n",
       "1      1  2126.0  52683.0  0.0388    (2126.0/54809.0)\n",
       "2  Total  9047.0  60981.0  0.1489   (10424.0/70028.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Maximum Metrics: Maximum metrics at their respective thresholds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>threshold</th>\n",
       "      <th>value</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>max f1</td>\n",
       "      <td>0.598231</td>\n",
       "      <td>0.909975</td>\n",
       "      <td>249.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>max f2</td>\n",
       "      <td>0.413960</td>\n",
       "      <td>0.952631</td>\n",
       "      <td>319.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>max f0point5</td>\n",
       "      <td>0.738376</td>\n",
       "      <td>0.902438</td>\n",
       "      <td>175.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>max accuracy</td>\n",
       "      <td>0.629584</td>\n",
       "      <td>0.851973</td>\n",
       "      <td>235.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max precision</td>\n",
       "      <td>0.971780</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>max recall</td>\n",
       "      <td>0.118024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>392.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>max specificity</td>\n",
       "      <td>0.971780</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>max absolute_mcc</td>\n",
       "      <td>0.696002</td>\n",
       "      <td>0.538403</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>max min_per_class_accuracy</td>\n",
       "      <td>0.776881</td>\n",
       "      <td>0.789802</td>\n",
       "      <td>151.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>max mean_per_class_accuracy</td>\n",
       "      <td>0.778415</td>\n",
       "      <td>0.790080</td>\n",
       "      <td>150.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>max tns</td>\n",
       "      <td>0.971780</td>\n",
       "      <td>15219.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>max fns</td>\n",
       "      <td>0.971780</td>\n",
       "      <td>54786.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>max fps</td>\n",
       "      <td>0.059559</td>\n",
       "      <td>15219.000000</td>\n",
       "      <td>399.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>max tps</td>\n",
       "      <td>0.118024</td>\n",
       "      <td>54809.000000</td>\n",
       "      <td>392.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>max tnr</td>\n",
       "      <td>0.971780</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>max fnr</td>\n",
       "      <td>0.971780</td>\n",
       "      <td>0.999580</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>max fpr</td>\n",
       "      <td>0.059559</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>399.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>max tpr</td>\n",
       "      <td>0.118024</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>392.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         metric  threshold         value    idx\n",
       "0                        max f1   0.598231      0.909975  249.0\n",
       "1                        max f2   0.413960      0.952631  319.0\n",
       "2                  max f0point5   0.738376      0.902438  175.0\n",
       "3                  max accuracy   0.629584      0.851973  235.0\n",
       "4                 max precision   0.971780      1.000000    0.0\n",
       "5                    max recall   0.118024      1.000000  392.0\n",
       "6               max specificity   0.971780      1.000000    0.0\n",
       "7              max absolute_mcc   0.696002      0.538403  200.0\n",
       "8    max min_per_class_accuracy   0.776881      0.789802  151.0\n",
       "9   max mean_per_class_accuracy   0.778415      0.790080  150.0\n",
       "10                      max tns   0.971780  15219.000000    0.0\n",
       "11                      max fns   0.971780  54786.000000    0.0\n",
       "12                      max fps   0.059559  15219.000000  399.0\n",
       "13                      max tps   0.118024  54809.000000  392.0\n",
       "14                      max tnr   0.971780      1.000000    0.0\n",
       "15                      max fnr   0.971780      0.999580    0.0\n",
       "16                      max fpr   0.059559      1.000000  399.0\n",
       "17                      max tpr   0.118024      1.000000  392.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gains/Lift Table: Avg response rate: 78.27 %, avg score: 78.24 %\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>cumulative_data_fraction</th>\n",
       "      <th>lower_threshold</th>\n",
       "      <th>lift</th>\n",
       "      <th>cumulative_lift</th>\n",
       "      <th>response_rate</th>\n",
       "      <th>score</th>\n",
       "      <th>cumulative_response_rate</th>\n",
       "      <th>cumulative_score</th>\n",
       "      <th>capture_rate</th>\n",
       "      <th>cumulative_capture_rate</th>\n",
       "      <th>gain</th>\n",
       "      <th>cumulative_gain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>0.010010</td>\n",
       "      <td>0.953227</td>\n",
       "      <td>1.275851</td>\n",
       "      <td>1.275851</td>\n",
       "      <td>0.998573</td>\n",
       "      <td>0.958319</td>\n",
       "      <td>0.998573</td>\n",
       "      <td>0.958319</td>\n",
       "      <td>0.012772</td>\n",
       "      <td>0.012772</td>\n",
       "      <td>27.585073</td>\n",
       "      <td>27.585073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>0.020006</td>\n",
       "      <td>0.947908</td>\n",
       "      <td>1.272198</td>\n",
       "      <td>1.274025</td>\n",
       "      <td>0.995714</td>\n",
       "      <td>0.950350</td>\n",
       "      <td>0.997145</td>\n",
       "      <td>0.954338</td>\n",
       "      <td>0.012717</td>\n",
       "      <td>0.025489</td>\n",
       "      <td>27.219763</td>\n",
       "      <td>27.402549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>0.030002</td>\n",
       "      <td>0.943784</td>\n",
       "      <td>1.272198</td>\n",
       "      <td>1.273416</td>\n",
       "      <td>0.995714</td>\n",
       "      <td>0.945743</td>\n",
       "      <td>0.996668</td>\n",
       "      <td>0.951474</td>\n",
       "      <td>0.012717</td>\n",
       "      <td>0.038205</td>\n",
       "      <td>27.219763</td>\n",
       "      <td>27.341649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td>0.040013</td>\n",
       "      <td>0.940787</td>\n",
       "      <td>1.274028</td>\n",
       "      <td>1.273569</td>\n",
       "      <td>0.997147</td>\n",
       "      <td>0.942244</td>\n",
       "      <td>0.996788</td>\n",
       "      <td>0.949165</td>\n",
       "      <td>0.012753</td>\n",
       "      <td>0.050959</td>\n",
       "      <td>27.402809</td>\n",
       "      <td>27.356950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>0.050009</td>\n",
       "      <td>0.937885</td>\n",
       "      <td>1.272198</td>\n",
       "      <td>1.273295</td>\n",
       "      <td>0.995714</td>\n",
       "      <td>0.939285</td>\n",
       "      <td>0.996573</td>\n",
       "      <td>0.947190</td>\n",
       "      <td>0.012717</td>\n",
       "      <td>0.063676</td>\n",
       "      <td>27.219763</td>\n",
       "      <td>27.329528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td></td>\n",
       "      <td>6</td>\n",
       "      <td>0.100003</td>\n",
       "      <td>0.926013</td>\n",
       "      <td>1.265630</td>\n",
       "      <td>1.269463</td>\n",
       "      <td>0.990574</td>\n",
       "      <td>0.931762</td>\n",
       "      <td>0.993574</td>\n",
       "      <td>0.939477</td>\n",
       "      <td>0.063274</td>\n",
       "      <td>0.126950</td>\n",
       "      <td>26.563018</td>\n",
       "      <td>26.946328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "      <td>7</td>\n",
       "      <td>0.150011</td>\n",
       "      <td>0.916043</td>\n",
       "      <td>1.252499</td>\n",
       "      <td>1.263808</td>\n",
       "      <td>0.980297</td>\n",
       "      <td>0.920922</td>\n",
       "      <td>0.989148</td>\n",
       "      <td>0.933291</td>\n",
       "      <td>0.062636</td>\n",
       "      <td>0.189586</td>\n",
       "      <td>25.249934</td>\n",
       "      <td>26.380809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td></td>\n",
       "      <td>8</td>\n",
       "      <td>0.200006</td>\n",
       "      <td>0.906355</td>\n",
       "      <td>1.249208</td>\n",
       "      <td>1.260159</td>\n",
       "      <td>0.977721</td>\n",
       "      <td>0.911188</td>\n",
       "      <td>0.986292</td>\n",
       "      <td>0.927766</td>\n",
       "      <td>0.062453</td>\n",
       "      <td>0.252039</td>\n",
       "      <td>24.920764</td>\n",
       "      <td>26.015850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td></td>\n",
       "      <td>9</td>\n",
       "      <td>0.300009</td>\n",
       "      <td>0.886644</td>\n",
       "      <td>1.230785</td>\n",
       "      <td>1.250367</td>\n",
       "      <td>0.963301</td>\n",
       "      <td>0.896502</td>\n",
       "      <td>0.978628</td>\n",
       "      <td>0.917345</td>\n",
       "      <td>0.123082</td>\n",
       "      <td>0.375121</td>\n",
       "      <td>23.078460</td>\n",
       "      <td>25.036720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td></td>\n",
       "      <td>10</td>\n",
       "      <td>0.399997</td>\n",
       "      <td>0.864590</td>\n",
       "      <td>1.198845</td>\n",
       "      <td>1.237488</td>\n",
       "      <td>0.938303</td>\n",
       "      <td>0.875843</td>\n",
       "      <td>0.968548</td>\n",
       "      <td>0.906971</td>\n",
       "      <td>0.119871</td>\n",
       "      <td>0.494992</td>\n",
       "      <td>19.884520</td>\n",
       "      <td>23.748808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td></td>\n",
       "      <td>11</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.838401</td>\n",
       "      <td>1.166199</td>\n",
       "      <td>1.223230</td>\n",
       "      <td>0.912752</td>\n",
       "      <td>0.851916</td>\n",
       "      <td>0.957388</td>\n",
       "      <td>0.895959</td>\n",
       "      <td>0.116623</td>\n",
       "      <td>0.611615</td>\n",
       "      <td>16.619852</td>\n",
       "      <td>22.322976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td></td>\n",
       "      <td>12</td>\n",
       "      <td>0.600003</td>\n",
       "      <td>0.804269</td>\n",
       "      <td>1.110735</td>\n",
       "      <td>1.204480</td>\n",
       "      <td>0.869342</td>\n",
       "      <td>0.822240</td>\n",
       "      <td>0.942714</td>\n",
       "      <td>0.883673</td>\n",
       "      <td>0.111077</td>\n",
       "      <td>0.722692</td>\n",
       "      <td>11.073476</td>\n",
       "      <td>20.448015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td></td>\n",
       "      <td>13</td>\n",
       "      <td>0.699991</td>\n",
       "      <td>0.756964</td>\n",
       "      <td>1.021299</td>\n",
       "      <td>1.178314</td>\n",
       "      <td>0.799343</td>\n",
       "      <td>0.781953</td>\n",
       "      <td>0.922234</td>\n",
       "      <td>0.869143</td>\n",
       "      <td>0.102118</td>\n",
       "      <td>0.824810</td>\n",
       "      <td>2.129933</td>\n",
       "      <td>17.831413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td></td>\n",
       "      <td>14</td>\n",
       "      <td>0.799994</td>\n",
       "      <td>0.682286</td>\n",
       "      <td>0.880670</td>\n",
       "      <td>1.141107</td>\n",
       "      <td>0.689276</td>\n",
       "      <td>0.722463</td>\n",
       "      <td>0.893113</td>\n",
       "      <td>0.850807</td>\n",
       "      <td>0.088069</td>\n",
       "      <td>0.912879</td>\n",
       "      <td>-11.933038</td>\n",
       "      <td>14.110724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td></td>\n",
       "      <td>15</td>\n",
       "      <td>0.899997</td>\n",
       "      <td>0.545984</td>\n",
       "      <td>0.625427</td>\n",
       "      <td>1.083808</td>\n",
       "      <td>0.489504</td>\n",
       "      <td>0.623191</td>\n",
       "      <td>0.848267</td>\n",
       "      <td>0.825516</td>\n",
       "      <td>0.062544</td>\n",
       "      <td>0.975424</td>\n",
       "      <td>-37.457314</td>\n",
       "      <td>8.380760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td></td>\n",
       "      <td>16</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.051405</td>\n",
       "      <td>0.245756</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.192346</td>\n",
       "      <td>0.393961</td>\n",
       "      <td>0.782673</td>\n",
       "      <td>0.782359</td>\n",
       "      <td>0.024576</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-75.424446</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      group  cumulative_data_fraction  lower_threshold      lift  \\\n",
       "0         1                  0.010010         0.953227  1.275851   \n",
       "1         2                  0.020006         0.947908  1.272198   \n",
       "2         3                  0.030002         0.943784  1.272198   \n",
       "3         4                  0.040013         0.940787  1.274028   \n",
       "4         5                  0.050009         0.937885  1.272198   \n",
       "5         6                  0.100003         0.926013  1.265630   \n",
       "6         7                  0.150011         0.916043  1.252499   \n",
       "7         8                  0.200006         0.906355  1.249208   \n",
       "8         9                  0.300009         0.886644  1.230785   \n",
       "9        10                  0.399997         0.864590  1.198845   \n",
       "10       11                  0.500000         0.838401  1.166199   \n",
       "11       12                  0.600003         0.804269  1.110735   \n",
       "12       13                  0.699991         0.756964  1.021299   \n",
       "13       14                  0.799994         0.682286  0.880670   \n",
       "14       15                  0.899997         0.545984  0.625427   \n",
       "15       16                  1.000000         0.051405  0.245756   \n",
       "\n",
       "    cumulative_lift  response_rate     score  cumulative_response_rate  \\\n",
       "0          1.275851       0.998573  0.958319                  0.998573   \n",
       "1          1.274025       0.995714  0.950350                  0.997145   \n",
       "2          1.273416       0.995714  0.945743                  0.996668   \n",
       "3          1.273569       0.997147  0.942244                  0.996788   \n",
       "4          1.273295       0.995714  0.939285                  0.996573   \n",
       "5          1.269463       0.990574  0.931762                  0.993574   \n",
       "6          1.263808       0.980297  0.920922                  0.989148   \n",
       "7          1.260159       0.977721  0.911188                  0.986292   \n",
       "8          1.250367       0.963301  0.896502                  0.978628   \n",
       "9          1.237488       0.938303  0.875843                  0.968548   \n",
       "10         1.223230       0.912752  0.851916                  0.957388   \n",
       "11         1.204480       0.869342  0.822240                  0.942714   \n",
       "12         1.178314       0.799343  0.781953                  0.922234   \n",
       "13         1.141107       0.689276  0.722463                  0.893113   \n",
       "14         1.083808       0.489504  0.623191                  0.848267   \n",
       "15         1.000000       0.192346  0.393961                  0.782673   \n",
       "\n",
       "    cumulative_score  capture_rate  cumulative_capture_rate       gain  \\\n",
       "0           0.958319      0.012772                 0.012772  27.585073   \n",
       "1           0.954338      0.012717                 0.025489  27.219763   \n",
       "2           0.951474      0.012717                 0.038205  27.219763   \n",
       "3           0.949165      0.012753                 0.050959  27.402809   \n",
       "4           0.947190      0.012717                 0.063676  27.219763   \n",
       "5           0.939477      0.063274                 0.126950  26.563018   \n",
       "6           0.933291      0.062636                 0.189586  25.249934   \n",
       "7           0.927766      0.062453                 0.252039  24.920764   \n",
       "8           0.917345      0.123082                 0.375121  23.078460   \n",
       "9           0.906971      0.119871                 0.494992  19.884520   \n",
       "10          0.895959      0.116623                 0.611615  16.619852   \n",
       "11          0.883673      0.111077                 0.722692  11.073476   \n",
       "12          0.869143      0.102118                 0.824810   2.129933   \n",
       "13          0.850807      0.088069                 0.912879 -11.933038   \n",
       "14          0.825516      0.062544                 0.975424 -37.457314   \n",
       "15          0.782359      0.024576                 1.000000 -75.424446   \n",
       "\n",
       "    cumulative_gain  \n",
       "0         27.585073  \n",
       "1         27.402549  \n",
       "2         27.341649  \n",
       "3         27.356950  \n",
       "4         27.329528  \n",
       "5         26.946328  \n",
       "6         26.380809  \n",
       "7         26.015850  \n",
       "8         25.036720  \n",
       "9         23.748808  \n",
       "10        22.322976  \n",
       "11        20.448015  \n",
       "12        17.831413  \n",
       "13        14.110724  \n",
       "14         8.380760  \n",
       "15         0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ModelMetricsBinomial: gbm\n",
      "** Reported on validation data. **\n",
      "\n",
      "MSE: 0.12885289025826002\n",
      "RMSE: 0.3589608478069162\n",
      "LogLoss: 0.41002596498807614\n",
      "Mean Per-Class Error: 0.2501866177988008\n",
      "AUC: 0.8299493063417328\n",
      "AUCPR: 0.9362941943634113\n",
      "Gini: 0.6598986126834656\n",
      "\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.5462768125297364: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>Error</th>\n",
       "      <th>Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1071.0</td>\n",
       "      <td>2267.0</td>\n",
       "      <td>0.6791</td>\n",
       "      <td>(2267.0/3338.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>302.0</td>\n",
       "      <td>11233.0</td>\n",
       "      <td>0.0262</td>\n",
       "      <td>(302.0/11535.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Total</td>\n",
       "      <td>1373.0</td>\n",
       "      <td>13500.0</td>\n",
       "      <td>0.1727</td>\n",
       "      <td>(2569.0/14873.0)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0        1   Error               Rate\n",
       "0      0  1071.0   2267.0  0.6791    (2267.0/3338.0)\n",
       "1      1   302.0  11233.0  0.0262    (302.0/11535.0)\n",
       "2  Total  1373.0  13500.0  0.1727   (2569.0/14873.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Maximum Metrics: Maximum metrics at their respective thresholds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>threshold</th>\n",
       "      <th>value</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>max f1</td>\n",
       "      <td>0.546277</td>\n",
       "      <td>0.897384</td>\n",
       "      <td>271.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>max f2</td>\n",
       "      <td>0.373048</td>\n",
       "      <td>0.948300</td>\n",
       "      <td>333.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>max f0point5</td>\n",
       "      <td>0.744656</td>\n",
       "      <td>0.879953</td>\n",
       "      <td>170.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>max accuracy</td>\n",
       "      <td>0.578970</td>\n",
       "      <td>0.827405</td>\n",
       "      <td>257.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max precision</td>\n",
       "      <td>0.972891</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>max recall</td>\n",
       "      <td>0.151857</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>389.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>max specificity</td>\n",
       "      <td>0.972891</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>max absolute_mcc</td>\n",
       "      <td>0.701520</td>\n",
       "      <td>0.456664</td>\n",
       "      <td>195.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>max min_per_class_accuracy</td>\n",
       "      <td>0.785462</td>\n",
       "      <td>0.747753</td>\n",
       "      <td>144.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>max mean_per_class_accuracy</td>\n",
       "      <td>0.779835</td>\n",
       "      <td>0.749813</td>\n",
       "      <td>148.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>max tns</td>\n",
       "      <td>0.972891</td>\n",
       "      <td>3338.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>max fns</td>\n",
       "      <td>0.972891</td>\n",
       "      <td>11531.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>max fps</td>\n",
       "      <td>0.065755</td>\n",
       "      <td>3338.000000</td>\n",
       "      <td>399.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>max tps</td>\n",
       "      <td>0.151857</td>\n",
       "      <td>11535.000000</td>\n",
       "      <td>389.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>max tnr</td>\n",
       "      <td>0.972891</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>max fnr</td>\n",
       "      <td>0.972891</td>\n",
       "      <td>0.999653</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>max fpr</td>\n",
       "      <td>0.065755</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>399.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>max tpr</td>\n",
       "      <td>0.151857</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>389.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         metric  threshold         value    idx\n",
       "0                        max f1   0.546277      0.897384  271.0\n",
       "1                        max f2   0.373048      0.948300  333.0\n",
       "2                  max f0point5   0.744656      0.879953  170.0\n",
       "3                  max accuracy   0.578970      0.827405  257.0\n",
       "4                 max precision   0.972891      1.000000    0.0\n",
       "5                    max recall   0.151857      1.000000  389.0\n",
       "6               max specificity   0.972891      1.000000    0.0\n",
       "7              max absolute_mcc   0.701520      0.456664  195.0\n",
       "8    max min_per_class_accuracy   0.785462      0.747753  144.0\n",
       "9   max mean_per_class_accuracy   0.779835      0.749813  148.0\n",
       "10                      max tns   0.972891   3338.000000    0.0\n",
       "11                      max fns   0.972891  11531.000000    0.0\n",
       "12                      max fps   0.065755   3338.000000  399.0\n",
       "13                      max tps   0.151857  11535.000000  389.0\n",
       "14                      max tnr   0.972891      1.000000    0.0\n",
       "15                      max fnr   0.972891      0.999653    0.0\n",
       "16                      max fpr   0.065755      1.000000  399.0\n",
       "17                      max tpr   0.151857      1.000000  389.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gains/Lift Table: Avg response rate: 77.56 %, avg score: 78.22 %\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>group</th>\n",
       "      <th>cumulative_data_fraction</th>\n",
       "      <th>lower_threshold</th>\n",
       "      <th>lift</th>\n",
       "      <th>cumulative_lift</th>\n",
       "      <th>response_rate</th>\n",
       "      <th>score</th>\n",
       "      <th>cumulative_response_rate</th>\n",
       "      <th>cumulative_score</th>\n",
       "      <th>capture_rate</th>\n",
       "      <th>cumulative_capture_rate</th>\n",
       "      <th>gain</th>\n",
       "      <th>cumulative_gain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>0.010018</td>\n",
       "      <td>0.952700</td>\n",
       "      <td>1.272073</td>\n",
       "      <td>1.272073</td>\n",
       "      <td>0.986577</td>\n",
       "      <td>0.958248</td>\n",
       "      <td>0.986577</td>\n",
       "      <td>0.958248</td>\n",
       "      <td>0.012744</td>\n",
       "      <td>0.012744</td>\n",
       "      <td>27.207303</td>\n",
       "      <td>27.207303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>0.020036</td>\n",
       "      <td>0.947540</td>\n",
       "      <td>1.272073</td>\n",
       "      <td>1.272073</td>\n",
       "      <td>0.986577</td>\n",
       "      <td>0.949926</td>\n",
       "      <td>0.986577</td>\n",
       "      <td>0.954087</td>\n",
       "      <td>0.012744</td>\n",
       "      <td>0.025488</td>\n",
       "      <td>27.207303</td>\n",
       "      <td>27.207303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>0.030054</td>\n",
       "      <td>0.943292</td>\n",
       "      <td>1.246112</td>\n",
       "      <td>1.263419</td>\n",
       "      <td>0.966443</td>\n",
       "      <td>0.945395</td>\n",
       "      <td>0.979866</td>\n",
       "      <td>0.951189</td>\n",
       "      <td>0.012484</td>\n",
       "      <td>0.037971</td>\n",
       "      <td>24.611236</td>\n",
       "      <td>26.341947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "      <td>0.040005</td>\n",
       "      <td>0.939706</td>\n",
       "      <td>1.280668</td>\n",
       "      <td>1.267710</td>\n",
       "      <td>0.993243</td>\n",
       "      <td>0.941413</td>\n",
       "      <td>0.983193</td>\n",
       "      <td>0.948758</td>\n",
       "      <td>0.012744</td>\n",
       "      <td>0.050715</td>\n",
       "      <td>28.066812</td>\n",
       "      <td>26.770989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>0.050024</td>\n",
       "      <td>0.936787</td>\n",
       "      <td>1.272073</td>\n",
       "      <td>1.268584</td>\n",
       "      <td>0.986577</td>\n",
       "      <td>0.938289</td>\n",
       "      <td>0.983871</td>\n",
       "      <td>0.946661</td>\n",
       "      <td>0.012744</td>\n",
       "      <td>0.063459</td>\n",
       "      <td>27.207303</td>\n",
       "      <td>26.858369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td></td>\n",
       "      <td>6</td>\n",
       "      <td>0.100047</td>\n",
       "      <td>0.924646</td>\n",
       "      <td>1.258185</td>\n",
       "      <td>1.263385</td>\n",
       "      <td>0.975806</td>\n",
       "      <td>0.930209</td>\n",
       "      <td>0.979839</td>\n",
       "      <td>0.938435</td>\n",
       "      <td>0.062939</td>\n",
       "      <td>0.126398</td>\n",
       "      <td>25.818547</td>\n",
       "      <td>26.338458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "      <td>7</td>\n",
       "      <td>0.150003</td>\n",
       "      <td>0.914117</td>\n",
       "      <td>1.252937</td>\n",
       "      <td>1.259905</td>\n",
       "      <td>0.971736</td>\n",
       "      <td>0.919355</td>\n",
       "      <td>0.977140</td>\n",
       "      <td>0.932081</td>\n",
       "      <td>0.062592</td>\n",
       "      <td>0.188990</td>\n",
       "      <td>25.293737</td>\n",
       "      <td>25.990530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td></td>\n",
       "      <td>8</td>\n",
       "      <td>0.200027</td>\n",
       "      <td>0.904117</td>\n",
       "      <td>1.235656</td>\n",
       "      <td>1.253841</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>0.909135</td>\n",
       "      <td>0.972437</td>\n",
       "      <td>0.926342</td>\n",
       "      <td>0.061812</td>\n",
       "      <td>0.250802</td>\n",
       "      <td>23.565597</td>\n",
       "      <td>25.384093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td></td>\n",
       "      <td>9</td>\n",
       "      <td>0.300007</td>\n",
       "      <td>0.884060</td>\n",
       "      <td>1.214809</td>\n",
       "      <td>1.240833</td>\n",
       "      <td>0.942165</td>\n",
       "      <td>0.894176</td>\n",
       "      <td>0.962349</td>\n",
       "      <td>0.915623</td>\n",
       "      <td>0.121456</td>\n",
       "      <td>0.372258</td>\n",
       "      <td>21.480941</td>\n",
       "      <td>24.083334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td></td>\n",
       "      <td>10</td>\n",
       "      <td>0.399987</td>\n",
       "      <td>0.860780</td>\n",
       "      <td>1.184461</td>\n",
       "      <td>1.226743</td>\n",
       "      <td>0.918628</td>\n",
       "      <td>0.872616</td>\n",
       "      <td>0.951420</td>\n",
       "      <td>0.904873</td>\n",
       "      <td>0.118422</td>\n",
       "      <td>0.490681</td>\n",
       "      <td>18.446085</td>\n",
       "      <td>22.674258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td></td>\n",
       "      <td>11</td>\n",
       "      <td>0.500034</td>\n",
       "      <td>0.834360</td>\n",
       "      <td>1.118676</td>\n",
       "      <td>1.205121</td>\n",
       "      <td>0.867608</td>\n",
       "      <td>0.847582</td>\n",
       "      <td>0.934651</td>\n",
       "      <td>0.893410</td>\n",
       "      <td>0.111920</td>\n",
       "      <td>0.602601</td>\n",
       "      <td>11.867592</td>\n",
       "      <td>20.512053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td></td>\n",
       "      <td>12</td>\n",
       "      <td>0.600013</td>\n",
       "      <td>0.800432</td>\n",
       "      <td>1.090814</td>\n",
       "      <td>1.186074</td>\n",
       "      <td>0.845999</td>\n",
       "      <td>0.818178</td>\n",
       "      <td>0.919879</td>\n",
       "      <td>0.880874</td>\n",
       "      <td>0.109059</td>\n",
       "      <td>0.711660</td>\n",
       "      <td>9.081387</td>\n",
       "      <td>18.607369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td></td>\n",
       "      <td>13</td>\n",
       "      <td>0.699993</td>\n",
       "      <td>0.754865</td>\n",
       "      <td>0.997167</td>\n",
       "      <td>1.159092</td>\n",
       "      <td>0.773369</td>\n",
       "      <td>0.779153</td>\n",
       "      <td>0.898953</td>\n",
       "      <td>0.866346</td>\n",
       "      <td>0.099697</td>\n",
       "      <td>0.811357</td>\n",
       "      <td>-0.283311</td>\n",
       "      <td>15.909219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td></td>\n",
       "      <td>14</td>\n",
       "      <td>0.799973</td>\n",
       "      <td>0.684703</td>\n",
       "      <td>0.858431</td>\n",
       "      <td>1.121516</td>\n",
       "      <td>0.665770</td>\n",
       "      <td>0.720985</td>\n",
       "      <td>0.869810</td>\n",
       "      <td>0.848178</td>\n",
       "      <td>0.085826</td>\n",
       "      <td>0.897182</td>\n",
       "      <td>-14.156937</td>\n",
       "      <td>12.151581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td></td>\n",
       "      <td>15</td>\n",
       "      <td>0.899953</td>\n",
       "      <td>0.558221</td>\n",
       "      <td>0.706688</td>\n",
       "      <td>1.075431</td>\n",
       "      <td>0.548083</td>\n",
       "      <td>0.627830</td>\n",
       "      <td>0.834068</td>\n",
       "      <td>0.823699</td>\n",
       "      <td>0.070655</td>\n",
       "      <td>0.967837</td>\n",
       "      <td>-29.331216</td>\n",
       "      <td>7.543070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td></td>\n",
       "      <td>16</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.065755</td>\n",
       "      <td>0.321479</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.249328</td>\n",
       "      <td>0.408889</td>\n",
       "      <td>0.775566</td>\n",
       "      <td>0.782198</td>\n",
       "      <td>0.032163</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-67.852148</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      group  cumulative_data_fraction  lower_threshold      lift  \\\n",
       "0         1                  0.010018         0.952700  1.272073   \n",
       "1         2                  0.020036         0.947540  1.272073   \n",
       "2         3                  0.030054         0.943292  1.246112   \n",
       "3         4                  0.040005         0.939706  1.280668   \n",
       "4         5                  0.050024         0.936787  1.272073   \n",
       "5         6                  0.100047         0.924646  1.258185   \n",
       "6         7                  0.150003         0.914117  1.252937   \n",
       "7         8                  0.200027         0.904117  1.235656   \n",
       "8         9                  0.300007         0.884060  1.214809   \n",
       "9        10                  0.399987         0.860780  1.184461   \n",
       "10       11                  0.500034         0.834360  1.118676   \n",
       "11       12                  0.600013         0.800432  1.090814   \n",
       "12       13                  0.699993         0.754865  0.997167   \n",
       "13       14                  0.799973         0.684703  0.858431   \n",
       "14       15                  0.899953         0.558221  0.706688   \n",
       "15       16                  1.000000         0.065755  0.321479   \n",
       "\n",
       "    cumulative_lift  response_rate     score  cumulative_response_rate  \\\n",
       "0          1.272073       0.986577  0.958248                  0.986577   \n",
       "1          1.272073       0.986577  0.949926                  0.986577   \n",
       "2          1.263419       0.966443  0.945395                  0.979866   \n",
       "3          1.267710       0.993243  0.941413                  0.983193   \n",
       "4          1.268584       0.986577  0.938289                  0.983871   \n",
       "5          1.263385       0.975806  0.930209                  0.979839   \n",
       "6          1.259905       0.971736  0.919355                  0.977140   \n",
       "7          1.253841       0.958333  0.909135                  0.972437   \n",
       "8          1.240833       0.942165  0.894176                  0.962349   \n",
       "9          1.226743       0.918628  0.872616                  0.951420   \n",
       "10         1.205121       0.867608  0.847582                  0.934651   \n",
       "11         1.186074       0.845999  0.818178                  0.919879   \n",
       "12         1.159092       0.773369  0.779153                  0.898953   \n",
       "13         1.121516       0.665770  0.720985                  0.869810   \n",
       "14         1.075431       0.548083  0.627830                  0.834068   \n",
       "15         1.000000       0.249328  0.408889                  0.775566   \n",
       "\n",
       "    cumulative_score  capture_rate  cumulative_capture_rate       gain  \\\n",
       "0           0.958248      0.012744                 0.012744  27.207303   \n",
       "1           0.954087      0.012744                 0.025488  27.207303   \n",
       "2           0.951189      0.012484                 0.037971  24.611236   \n",
       "3           0.948758      0.012744                 0.050715  28.066812   \n",
       "4           0.946661      0.012744                 0.063459  27.207303   \n",
       "5           0.938435      0.062939                 0.126398  25.818547   \n",
       "6           0.932081      0.062592                 0.188990  25.293737   \n",
       "7           0.926342      0.061812                 0.250802  23.565597   \n",
       "8           0.915623      0.121456                 0.372258  21.480941   \n",
       "9           0.904873      0.118422                 0.490681  18.446085   \n",
       "10          0.893410      0.111920                 0.602601  11.867592   \n",
       "11          0.880874      0.109059                 0.711660   9.081387   \n",
       "12          0.866346      0.099697                 0.811357  -0.283311   \n",
       "13          0.848178      0.085826                 0.897182 -14.156937   \n",
       "14          0.823699      0.070655                 0.967837 -29.331216   \n",
       "15          0.782198      0.032163                 1.000000 -67.852148   \n",
       "\n",
       "    cumulative_gain  \n",
       "0         27.207303  \n",
       "1         27.207303  \n",
       "2         26.341947  \n",
       "3         26.770989  \n",
       "4         26.858369  \n",
       "5         26.338458  \n",
       "6         25.990530  \n",
       "7         25.384093  \n",
       "8         24.083334  \n",
       "9         22.674258  \n",
       "10        20.512053  \n",
       "11        18.607369  \n",
       "12        15.909219  \n",
       "13        12.151581  \n",
       "14         7.543070  \n",
       "15         0.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Scoring History: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>duration</th>\n",
       "      <th>number_of_trees</th>\n",
       "      <th>training_rmse</th>\n",
       "      <th>training_logloss</th>\n",
       "      <th>training_auc</th>\n",
       "      <th>training_pr_auc</th>\n",
       "      <th>training_lift</th>\n",
       "      <th>training_classification_error</th>\n",
       "      <th>validation_rmse</th>\n",
       "      <th>validation_logloss</th>\n",
       "      <th>validation_auc</th>\n",
       "      <th>validation_pr_auc</th>\n",
       "      <th>validation_lift</th>\n",
       "      <th>validation_classification_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>2020-06-01 23:22:13</td>\n",
       "      <td>0.224 sec</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.412427</td>\n",
       "      <td>0.523504</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.217327</td>\n",
       "      <td>0.417269</td>\n",
       "      <td>0.532610</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.224434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>2020-06-01 23:22:15</td>\n",
       "      <td>1.671 sec</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.379921</td>\n",
       "      <td>0.455273</td>\n",
       "      <td>0.792675</td>\n",
       "      <td>0.920583</td>\n",
       "      <td>1.252912</td>\n",
       "      <td>0.179185</td>\n",
       "      <td>0.388664</td>\n",
       "      <td>0.471733</td>\n",
       "      <td>0.764690</td>\n",
       "      <td>0.904185</td>\n",
       "      <td>1.264098</td>\n",
       "      <td>0.189605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>2020-06-01 23:22:16</td>\n",
       "      <td>2.928 sec</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.364688</td>\n",
       "      <td>0.424574</td>\n",
       "      <td>0.826823</td>\n",
       "      <td>0.936683</td>\n",
       "      <td>1.264915</td>\n",
       "      <td>0.168818</td>\n",
       "      <td>0.375871</td>\n",
       "      <td>0.445410</td>\n",
       "      <td>0.796275</td>\n",
       "      <td>0.919941</td>\n",
       "      <td>1.263419</td>\n",
       "      <td>0.182613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>2020-06-01 23:22:17</td>\n",
       "      <td>4.082 sec</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.354877</td>\n",
       "      <td>0.404805</td>\n",
       "      <td>0.845680</td>\n",
       "      <td>0.945132</td>\n",
       "      <td>1.274028</td>\n",
       "      <td>0.161007</td>\n",
       "      <td>0.368700</td>\n",
       "      <td>0.430483</td>\n",
       "      <td>0.810305</td>\n",
       "      <td>0.926910</td>\n",
       "      <td>1.263419</td>\n",
       "      <td>0.178646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>2020-06-01 23:22:18</td>\n",
       "      <td>5.288 sec</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.347027</td>\n",
       "      <td>0.389289</td>\n",
       "      <td>0.860372</td>\n",
       "      <td>0.951428</td>\n",
       "      <td>1.275851</td>\n",
       "      <td>0.154153</td>\n",
       "      <td>0.363524</td>\n",
       "      <td>0.419554</td>\n",
       "      <td>0.820919</td>\n",
       "      <td>0.931847</td>\n",
       "      <td>1.272073</td>\n",
       "      <td>0.177839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td></td>\n",
       "      <td>2020-06-01 23:22:19</td>\n",
       "      <td>6.559 sec</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.340183</td>\n",
       "      <td>0.376037</td>\n",
       "      <td>0.871821</td>\n",
       "      <td>0.955964</td>\n",
       "      <td>1.275851</td>\n",
       "      <td>0.148855</td>\n",
       "      <td>0.358961</td>\n",
       "      <td>0.410026</td>\n",
       "      <td>0.829949</td>\n",
       "      <td>0.936294</td>\n",
       "      <td>1.272073</td>\n",
       "      <td>0.172729</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               timestamp    duration  number_of_trees  training_rmse  \\\n",
       "0    2020-06-01 23:22:13   0.224 sec              0.0       0.412427   \n",
       "1    2020-06-01 23:22:15   1.671 sec             10.0       0.379921   \n",
       "2    2020-06-01 23:22:16   2.928 sec             20.0       0.364688   \n",
       "3    2020-06-01 23:22:17   4.082 sec             30.0       0.354877   \n",
       "4    2020-06-01 23:22:18   5.288 sec             40.0       0.347027   \n",
       "5    2020-06-01 23:22:19   6.559 sec             50.0       0.340183   \n",
       "\n",
       "   training_logloss  training_auc  training_pr_auc  training_lift  \\\n",
       "0          0.523504      0.500000         0.000000       1.000000   \n",
       "1          0.455273      0.792675         0.920583       1.252912   \n",
       "2          0.424574      0.826823         0.936683       1.264915   \n",
       "3          0.404805      0.845680         0.945132       1.274028   \n",
       "4          0.389289      0.860372         0.951428       1.275851   \n",
       "5          0.376037      0.871821         0.955964       1.275851   \n",
       "\n",
       "   training_classification_error  validation_rmse  validation_logloss  \\\n",
       "0                       0.217327         0.417269            0.532610   \n",
       "1                       0.179185         0.388664            0.471733   \n",
       "2                       0.168818         0.375871            0.445410   \n",
       "3                       0.161007         0.368700            0.430483   \n",
       "4                       0.154153         0.363524            0.419554   \n",
       "5                       0.148855         0.358961            0.410026   \n",
       "\n",
       "   validation_auc  validation_pr_auc  validation_lift  \\\n",
       "0        0.500000           0.000000         1.000000   \n",
       "1        0.764690           0.904185         1.264098   \n",
       "2        0.796275           0.919941         1.263419   \n",
       "3        0.810305           0.926910         1.263419   \n",
       "4        0.820919           0.931847         1.272073   \n",
       "5        0.829949           0.936294         1.272073   \n",
       "\n",
       "   validation_classification_error  \n",
       "0                         0.224434  \n",
       "1                         0.189605  \n",
       "2                         0.182613  \n",
       "3                         0.178646  \n",
       "4                         0.177839  \n",
       "5                         0.172729  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Variable Importances: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>relative_importance</th>\n",
       "      <th>scaled_importance</th>\n",
       "      <th>percentage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HelpfulnessDenominator</td>\n",
       "      <td>3240.504883</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.171022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HelpfulnessNumerator</td>\n",
       "      <td>2612.727539</td>\n",
       "      <td>0.806272</td>\n",
       "      <td>0.137890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C59</td>\n",
       "      <td>953.958557</td>\n",
       "      <td>0.294386</td>\n",
       "      <td>0.050346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Time</td>\n",
       "      <td>817.789734</td>\n",
       "      <td>0.252365</td>\n",
       "      <td>0.043160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C51</td>\n",
       "      <td>816.664490</td>\n",
       "      <td>0.252018</td>\n",
       "      <td>0.043101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>C98</td>\n",
       "      <td>786.702881</td>\n",
       "      <td>0.242772</td>\n",
       "      <td>0.041519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>C74</td>\n",
       "      <td>763.126404</td>\n",
       "      <td>0.235496</td>\n",
       "      <td>0.040275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>C4</td>\n",
       "      <td>755.322754</td>\n",
       "      <td>0.233088</td>\n",
       "      <td>0.039863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>C35</td>\n",
       "      <td>711.211182</td>\n",
       "      <td>0.219475</td>\n",
       "      <td>0.037535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>C21</td>\n",
       "      <td>672.725037</td>\n",
       "      <td>0.207599</td>\n",
       "      <td>0.035504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>C15</td>\n",
       "      <td>650.814758</td>\n",
       "      <td>0.200837</td>\n",
       "      <td>0.034348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>C12</td>\n",
       "      <td>594.585510</td>\n",
       "      <td>0.183485</td>\n",
       "      <td>0.031380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>C54</td>\n",
       "      <td>438.734833</td>\n",
       "      <td>0.135391</td>\n",
       "      <td>0.023155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>C80</td>\n",
       "      <td>437.210114</td>\n",
       "      <td>0.134920</td>\n",
       "      <td>0.023074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>C14</td>\n",
       "      <td>431.809296</td>\n",
       "      <td>0.133254</td>\n",
       "      <td>0.022789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>C7</td>\n",
       "      <td>368.783142</td>\n",
       "      <td>0.113804</td>\n",
       "      <td>0.019463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>C89</td>\n",
       "      <td>338.997986</td>\n",
       "      <td>0.104613</td>\n",
       "      <td>0.017891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>C69</td>\n",
       "      <td>238.513275</td>\n",
       "      <td>0.073604</td>\n",
       "      <td>0.012588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>C76</td>\n",
       "      <td>206.255753</td>\n",
       "      <td>0.063649</td>\n",
       "      <td>0.010885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>C68</td>\n",
       "      <td>187.803513</td>\n",
       "      <td>0.057955</td>\n",
       "      <td>0.009912</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  variable  relative_importance  scaled_importance  percentage\n",
       "0   HelpfulnessDenominator          3240.504883           1.000000    0.171022\n",
       "1     HelpfulnessNumerator          2612.727539           0.806272    0.137890\n",
       "2                      C59           953.958557           0.294386    0.050346\n",
       "3                     Time           817.789734           0.252365    0.043160\n",
       "4                      C51           816.664490           0.252018    0.043101\n",
       "5                      C98           786.702881           0.242772    0.041519\n",
       "6                      C74           763.126404           0.235496    0.040275\n",
       "7                       C4           755.322754           0.233088    0.039863\n",
       "8                      C35           711.211182           0.219475    0.037535\n",
       "9                      C21           672.725037           0.207599    0.035504\n",
       "10                     C15           650.814758           0.200837    0.034348\n",
       "11                     C12           594.585510           0.183485    0.031380\n",
       "12                     C54           438.734833           0.135391    0.023155\n",
       "13                     C80           437.210114           0.134920    0.023074\n",
       "14                     C14           431.809296           0.133254    0.022789\n",
       "15                      C7           368.783142           0.113804    0.019463\n",
       "16                     C89           338.997986           0.104613    0.017891\n",
       "17                     C69           238.513275           0.073604    0.012588\n",
       "18                     C76           206.255753           0.063649    0.010885\n",
       "19                     C68           187.803513           0.057955    0.009912"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "See the whole table with table.as_data_frame()\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from h2o.estimators import H2OGradientBoostingEstimator\n",
    "mlflow.end_run()\n",
    "mlflow.start_run(run_name='GBM with word vectors')\n",
    "RATIOS = [0.7,0.15]\n",
    "# Train Test Split\n",
    "ext_train,ext_test,ext_valid = ext_reviews.split_frame(ratios=RATIOS)\n",
    "# Log our ratios\n",
    "mlflow.lp('ratios',RATIOS)\n",
    "# Log what word vectors we're using\n",
    "mlflow.lp('word vectors', 'reviews')\n",
    "\n",
    "non_token_predictors = ['ProductId', 'UserId', 'HelpfulnessNumerator', 'HelpfulnessDenominator', 'Time']\n",
    "predictors = non_token_predictors + review_vecs.names\n",
    "response = 'PositiveReview'\n",
    "\n",
    "mlflow.lp('label', response)\n",
    "# There are a lot of predictors here (C1-C100 + features) so let's shorten that\n",
    "mlflow.lp('predictors', non_token_predictors + [f'C1-C{len(review_vecs.columns)}'])\n",
    "\n",
    "gbm_embeddings = H2OGradientBoostingEstimator(stopping_metric = \"AUC\", stopping_tolerance = 0.001,\n",
    "                                              stopping_rounds = 5, score_tree_interval = 10,\n",
    "                                              model_id = \"gbm_embeddings.hex\"\n",
    "                                             )\n",
    "with mlflow.timer('train_time'):\n",
    "    gbm_embeddings.train(x = predictors, y = response, \n",
    "                       training_frame = ext_train, validation_frame = ext_test\n",
    "                      )\n",
    "\n",
    "# Log the model params to mlflow\n",
    "mlflow.log_params(gbm_embeddings.get_params())\n",
    "# Log the model to MLFlow\n",
    "mlflow.log_model(gbm_embeddings, 'baseline_model')\n",
    "# Log the training notebook to MLFlow\n",
    "mlflow.log_artifact('MLManager H2O Demo.ipynb', 'training_notebook')\n",
    "gbm_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just like before, let's log all of our outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Summary: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>number_of_trees</th>\n",
       "      <th>number_of_internal_trees</th>\n",
       "      <th>model_size_in_bytes</th>\n",
       "      <th>min_depth</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>mean_depth</th>\n",
       "      <th>min_leaves</th>\n",
       "      <th>max_leaves</th>\n",
       "      <th>mean_leaves</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>22988.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>31.92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     number_of_trees  number_of_internal_trees  model_size_in_bytes  \\\n",
       "0               50.0                      50.0              22988.0   \n",
       "\n",
       "   min_depth  max_depth  mean_depth  min_leaves  max_leaves  mean_leaves  \n",
       "0        5.0        5.0         5.0        31.0        32.0        31.92  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on Validation Data: 0.83\n",
      "\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.5462768125297364: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>Error</th>\n",
       "      <th>Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1071.0</td>\n",
       "      <td>2267.0</td>\n",
       "      <td>0.6791</td>\n",
       "      <td>(2267.0/3338.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>302.0</td>\n",
       "      <td>11233.0</td>\n",
       "      <td>0.0262</td>\n",
       "      <td>(302.0/11535.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Total</td>\n",
       "      <td>1373.0</td>\n",
       "      <td>13500.0</td>\n",
       "      <td>0.1727</td>\n",
       "      <td>(2569.0/14873.0)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0        1   Error               Rate\n",
       "0      0  1071.0   2267.0  0.6791    (2267.0/3338.0)\n",
       "1      1   302.0  11233.0  0.0262    (302.0/11535.0)\n",
       "2  Total  1373.0  13500.0  0.1727   (2569.0/14873.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PartialDependencePlot progress: |█████████████████████████████████████████| 100%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline  \n",
    "from IPython.core.display import HTML\n",
    "\n",
    "# Print and Log Model params\n",
    "params = dict(zip(gbm_embeddings.summary().col_header[1:],\n",
    "                    gbm_embeddings.summary().cell_values[0][1:]))\n",
    "print(gbm_embeddings.summary())\n",
    "mlflow.log_params(params)\n",
    "\n",
    "\n",
    "#Plot and Log Scoring history\n",
    "gbm_embeddings.plot()\n",
    "print(\"AUC on Validation Data: \" + str(round(gbm_embeddings.auc(valid = True), 3)))\n",
    "# Log training and validation metrics over time\n",
    "for step, row in gbm_embeddings.scoring_history().iterrows():\n",
    "    row_dict = row.to_dict()\n",
    "    for r in row_dict:\n",
    "        if 'train' in r or 'valid' in r:\n",
    "            mlflow.log_metric(r, row_dict[r],step=step)\n",
    "\n",
    "\n",
    "# Print and Log Confusion Matrix\n",
    "print(gbm_embeddings.confusion_matrix(valid = True))\n",
    "mlflow.lm('fpr', gbm_embeddings.fpr(valid=True)[0][0])\n",
    "mlflow.lm('tpr', gbm_embeddings.tpr(valid=True)[0][0])\n",
    "mlflow.lm('fnr', gbm_embeddings.fnr(valid=True)[0][0])\n",
    "mlflow.lm('tnr', gbm_embeddings.fnr(valid=True)[0][0])\n",
    "mlflow.lm('F0point5', gbm_embeddings.F0point5(valid=True)[0][1])\n",
    "mlflow.lm('F1', gbm_embeddings.F1(valid=True)[0][1])\n",
    "mlflow.lm('F2', gbm_embeddings.F2(valid=True)[0][1])\n",
    "mlflow.lm('auc', gbm_embeddings.auc(valid = True))\n",
    "mlflow.lp('threshold', gbm_embeddings.F1(valid=True)[0][0]) # First element is the threshold\n",
    "\n",
    "\n",
    "# Plot and Log Variable Importance\n",
    "gbm_embeddings.varimp_plot()\n",
    "for var in gbm_embeddings.varimp():\n",
    "    mlflow.lm(f'varimp_{var[0]}',var[-1])\n",
    "    \n",
    "    \n",
    "# Partial Dependence Plot\n",
    "pdp_helpfulness = gbm_embeddings.partial_plot(ext_train, cols = [\"HelpfulnessNumerator\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "See your metrics plot <a href=/mlflow/#/metric/training_auc?runs=[\"755b1565c5b9\"]&experiment=4&plot_metric_keys=[\"training_logloss\",\"validation_logloss\",\"training_rmse\",\"validation_rmse\"]>here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_run = cur_run\n",
    "cur_run = mlflow.current_run_id()\n",
    "cur_exp = mlflow.current_exp_id()\n",
    "link = f'/mlflow/#/metric/training_auc?runs=[\"{cur_run}\"]&experiment={cur_exp}&plot_metric_keys=[\"training_logloss\",\\\"validation_logloss\\\",\\\"training_rmse\\\",\\\"validation_rmse\\\"]'\n",
    "HTML(f'See your metrics plot <a href={link}>here</a>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>duration</th>\n",
       "      <th>number_of_trees</th>\n",
       "      <th>training_rmse</th>\n",
       "      <th>training_logloss</th>\n",
       "      <th>training_auc</th>\n",
       "      <th>training_pr_auc</th>\n",
       "      <th>training_lift</th>\n",
       "      <th>training_classification_error</th>\n",
       "      <th>validation_rmse</th>\n",
       "      <th>validation_logloss</th>\n",
       "      <th>validation_auc</th>\n",
       "      <th>validation_pr_auc</th>\n",
       "      <th>validation_lift</th>\n",
       "      <th>validation_classification_error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>2020-05-29 17:16:38</td>\n",
       "      <td>0.275 sec</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.412581</td>\n",
       "      <td>0.523793</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.217553</td>\n",
       "      <td>0.413052</td>\n",
       "      <td>0.524673</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.218240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td>2020-05-29 17:16:40</td>\n",
       "      <td>1.585 sec</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.363967</td>\n",
       "      <td>0.422859</td>\n",
       "      <td>0.854668</td>\n",
       "      <td>0.948366</td>\n",
       "      <td>1.278041</td>\n",
       "      <td>0.163339</td>\n",
       "      <td>0.367460</td>\n",
       "      <td>0.429317</td>\n",
       "      <td>0.839656</td>\n",
       "      <td>0.943607</td>\n",
       "      <td>1.279165</td>\n",
       "      <td>0.171075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>2020-05-29 17:16:41</td>\n",
       "      <td>2.942 sec</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0.340196</td>\n",
       "      <td>0.377065</td>\n",
       "      <td>0.882425</td>\n",
       "      <td>0.960111</td>\n",
       "      <td>1.274395</td>\n",
       "      <td>0.146200</td>\n",
       "      <td>0.346782</td>\n",
       "      <td>0.388486</td>\n",
       "      <td>0.864674</td>\n",
       "      <td>0.954209</td>\n",
       "      <td>1.279165</td>\n",
       "      <td>0.154420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td>2020-05-29 17:16:42</td>\n",
       "      <td>4.218 sec</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0.325502</td>\n",
       "      <td>0.348799</td>\n",
       "      <td>0.897461</td>\n",
       "      <td>0.965599</td>\n",
       "      <td>1.274395</td>\n",
       "      <td>0.134185</td>\n",
       "      <td>0.335055</td>\n",
       "      <td>0.364778</td>\n",
       "      <td>0.877488</td>\n",
       "      <td>0.958772</td>\n",
       "      <td>1.279165</td>\n",
       "      <td>0.145960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td>2020-05-29 17:16:44</td>\n",
       "      <td>5.464 sec</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.315418</td>\n",
       "      <td>0.329413</td>\n",
       "      <td>0.907442</td>\n",
       "      <td>0.969016</td>\n",
       "      <td>1.278041</td>\n",
       "      <td>0.128006</td>\n",
       "      <td>0.327503</td>\n",
       "      <td>0.349258</td>\n",
       "      <td>0.885610</td>\n",
       "      <td>0.960976</td>\n",
       "      <td>1.279165</td>\n",
       "      <td>0.143561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td></td>\n",
       "      <td>2020-05-29 17:16:45</td>\n",
       "      <td>6.899 sec</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0.307178</td>\n",
       "      <td>0.313832</td>\n",
       "      <td>0.915321</td>\n",
       "      <td>0.971871</td>\n",
       "      <td>1.278041</td>\n",
       "      <td>0.120985</td>\n",
       "      <td>0.321558</td>\n",
       "      <td>0.337054</td>\n",
       "      <td>0.892199</td>\n",
       "      <td>0.963264</td>\n",
       "      <td>1.279165</td>\n",
       "      <td>0.142562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               timestamp    duration  number_of_trees  training_rmse  \\\n",
       "0    2020-05-29 17:16:38   0.275 sec              0.0       0.412581   \n",
       "1    2020-05-29 17:16:40   1.585 sec             10.0       0.363967   \n",
       "2    2020-05-29 17:16:41   2.942 sec             20.0       0.340196   \n",
       "3    2020-05-29 17:16:42   4.218 sec             30.0       0.325502   \n",
       "4    2020-05-29 17:16:44   5.464 sec             40.0       0.315418   \n",
       "5    2020-05-29 17:16:45   6.899 sec             50.0       0.307178   \n",
       "\n",
       "   training_logloss  training_auc  training_pr_auc  training_lift  \\\n",
       "0          0.523793      0.500000         0.000000       1.000000   \n",
       "1          0.422859      0.854668         0.948366       1.278041   \n",
       "2          0.377065      0.882425         0.960111       1.274395   \n",
       "3          0.348799      0.897461         0.965599       1.274395   \n",
       "4          0.329413      0.907442         0.969016       1.278041   \n",
       "5          0.313832      0.915321         0.971871       1.278041   \n",
       "\n",
       "   training_classification_error  validation_rmse  validation_logloss  \\\n",
       "0                       0.217553         0.413052            0.524673   \n",
       "1                       0.163339         0.367460            0.429317   \n",
       "2                       0.146200         0.346782            0.388486   \n",
       "3                       0.134185         0.335055            0.364778   \n",
       "4                       0.128006         0.327503            0.349258   \n",
       "5                       0.120985         0.321558            0.337054   \n",
       "\n",
       "   validation_auc  validation_pr_auc  validation_lift  \\\n",
       "0        0.500000           0.000000         1.000000   \n",
       "1        0.839656           0.943607         1.279165   \n",
       "2        0.864674           0.954209         1.279165   \n",
       "3        0.877488           0.958772         1.279165   \n",
       "4        0.885610           0.960976         1.279165   \n",
       "5        0.892199           0.963264         1.279165   \n",
       "\n",
       "   validation_classification_error  \n",
       "0                         0.218240  \n",
       "1                         0.171075  \n",
       "2                         0.154420  \n",
       "3                         0.145960  \n",
       "4                         0.143561  \n",
       "5                         0.142562  "
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm_embeddings.scoring_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Embeddings AUC: 0.772\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cur_run' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-f2d165310280>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# print(\"Baseline AUC: \" + str(round(gbm_baseline.auc(valid = True), 3)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"With Embeddings AUC: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgbm_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlink\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'/mlflow/#/metric/training_auc?runs=[\"{cur_run}\",\"{old_run}\"]&experiment={cur_exp}&plot_metric_keys=[\"training_auc\",\\\"validation_auc\\\",\\\"training_pr_auc\\\",\\\"validation_pr_auc\\\"]'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'See a metrics comparison <a href={link}>here</a>'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cur_run' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Baseline AUC: \" + str(round(gbm_baseline.auc(valid = True), 3)))\n",
    "print(\"With Embeddings AUC: \" + str(round(gbm_embeddings.auc(valid = True), 3)))\n",
    "link = f'/mlflow/#/metric/training_auc?runs=[\"{cur_run}\",\"{old_run}\"]&experiment={cur_exp}&plot_metric_keys=[\"training_auc\",\\\"validation_auc\\\",\\\"training_pr_auc\\\",\\\"validation_pr_auc\\\"]'\n",
    "HTML(f'See a metrics comparison <a href={link}>here</a>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That's some great imrpovement! So what's next?\n",
    "<blockquote>We included the customer reviews and developed a better model. We've logged everything to MLFlow for detailed comparisons. Now what?<br>\n",
    "    Let's deploy our models to production so we can utilize what we've built. First, we'll deploy our word vectorizer model, and then deploy our GBM model. Finally, we'll create a feed from the first model to the second, so we can see the final predictions!\n",
    "        </i></br><footer>Splice Machine</footer></blockquote><br>\n",
    "<img src=https://splice-demo.s3.amazonaws.com/H2O+Demo+Model+Diagram.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Deploy Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function _deploy_db in module splicemachine.mlflow_support.mlflow_support:\n",
      "\n",
      "_deploy_db(fittedModel, df, db_schema_name, db_table_name, primary_key, run_id: str = None, classes=None, sklearn_args={}, verbose=False, pred_threshold=None, replace=False) -> None\n",
      "    Function to deploy a trained (currently Spark, Sklearn or H2O) model to the Database.\n",
      "    This creates 2 tables: One with the features of the model, and one with the prediction and metadata.\n",
      "    They are linked with a column called MOMENT_ID\n",
      "    \n",
      "    :param fittedModel: (ML pipeline or model) The fitted pipeline to deploy\n",
      "    :param df: (Spark DF) The dataframe used to train the model\n",
      "                NOTE: this dataframe should NOT be transformed by the model. The columns in this df are the ones\n",
      "                that will be used to create the table.\n",
      "    :param db_schema_name: (str) the schema name to deploy to. If None, the currently set schema will be used.\n",
      "    :param db_table_name: (str) the table name to deploy to. If none, the run_id will be used for the table name(s)\n",
      "    :param primary_key: (List[Tuple[str, str]]) List of column + SQL datatype to use for the primary/composite key\n",
      "    :param run_id: (str) The active run_id\n",
      "    :param classes: (List[str]) The classes (prediction labels) for the model being deployed.\n",
      "                    NOTE: If not supplied, the table will have default column names for each class\n",
      "    :param sklearn_args: (dict{str: str}) Prediction options for sklearn models\n",
      "                        Available key value options:\n",
      "                        'predict_call': 'predict', 'predict_proba', or 'transform'\n",
      "                                                                       - Determines the function call for the model\n",
      "                                                                       If blank, predict will be used\n",
      "                                                                       (or transform if model doesn't have predict)\n",
      "                        'predict_args': 'return_std' or 'return_cov' - For Bayesian and Gaussian models\n",
      "                                                                         Only one can be specified\n",
      "                        If the model does not have the option specified, it will be ignored.\n",
      "    :param verbose: (bool) Whether or not to print out the queries being created. Helpful for debugging\n",
      "    :param pred_threshold: (double) A prediction threshold for *Keras* binary classification models\n",
      "                            If the model type isn't Keras, this parameter will be ignored\n",
      "                            NOTE: If the model type is Keras, the output layer has 1 node, and pred_threshold is None,\n",
      "                                  you will NOT receive a class prediction, only the output of the final layer (like model.predict()).\n",
      "                                  If you want a class prediction\n",
      "                                  for your binary classification problem, you MUST pass in a threshold.\n",
      "    :param replace: (bool) whether or not to replace a currently existing model. This param does not yet work\n",
      "    \n",
      "    \n",
      "    This function creates the following:\n",
      "    * Table (default called DATA_{run_id}) where run_id is the run_id of the mlflow run associated to that model.\n",
      "        This will have a column for each feature in the feature vector as well as a MOMENT_ID as primary key\n",
      "    * Table (default called DATA_{run_id}_PREDS) That will have the columns:\n",
      "        USER which is the current user who made the request\n",
      "        EVAL_TIME which is the CURRENT_TIMESTAMP\n",
      "        MOMENT_ID same as the DATA table to link predictions to rows in the table\n",
      "        PREDICTION. The prediction of the model. If the :classes: param is not filled in, this will be default values for classification models\n",
      "        A column for each class of the predictor with the value being the probability/confidence of the model if applicable\n",
      "    * A trigger that runs on (after) insertion to the data table that runs an INSERT into the prediction table,\n",
      "        calling the PREDICT function, passing in the row of data as well as the schema of the dataset, and the run_id of the model to run\n",
      "    * A trigger that runs on (after) insertion to the prediction table that calls an UPDATE to the row inserted,\n",
      "        parsing the prediction probabilities and filling in proper column values\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(mlflow.deploy_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model of size: 8014.339 KB to Splice Machine DB\n",
      "Deploying model b8738905c16c to table BEN.word_vec_model\n",
      "Creating data table ... \n",
      " CREATE TABLE BEN.word_vec_model (\n",
      "\tReview VARCHAR(5000),\tMOMENT_KEY INT,\n",
      "\tPRIMARY KEY(MOMENT_KEY)\n",
      ")\n",
      "\n",
      "Done.\n",
      "Creating prediction table ... \n",
      "CREATE TABLE BEN.word_vec_model_PREDS (\n",
      "        \tCUR_USER VARCHAR(50) DEFAULT CURRENT_USER,\n",
      "        \tEVAL_TIME TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
      "        \tRUN_ID VARCHAR(50) DEFAULT 'b8738905c16c',\n",
      "        \tMOMENT_KEY INT,\n",
      "\t\"C1_C0\" DOUBLE,\n",
      "\t\"C1_C1\" DOUBLE,\n",
      "\t\"C1_C2\" DOUBLE,\n",
      "\t\"C1_C3\" DOUBLE,\n",
      "\t\"C1_C4\" DOUBLE,\n",
      "\t\"C1_C5\" DOUBLE,\n",
      "\t\"C1_C6\" DOUBLE,\n",
      "\t\"C1_C7\" DOUBLE,\n",
      "\t\"C1_C8\" DOUBLE,\n",
      "\t\"C1_C9\" DOUBLE,\n",
      "\t\"C1_C10\" DOUBLE,\n",
      "\t\"C1_C11\" DOUBLE,\n",
      "\t\"C1_C12\" DOUBLE,\n",
      "\t\"C1_C13\" DOUBLE,\n",
      "\t\"C1_C14\" DOUBLE,\n",
      "\t\"C1_C15\" DOUBLE,\n",
      "\t\"C1_C16\" DOUBLE,\n",
      "\t\"C1_C17\" DOUBLE,\n",
      "\t\"C1_C18\" DOUBLE,\n",
      "\t\"C1_C19\" DOUBLE,\n",
      "\t\"C1_C20\" DOUBLE,\n",
      "\t\"C1_C21\" DOUBLE,\n",
      "\t\"C1_C22\" DOUBLE,\n",
      "\t\"C1_C23\" DOUBLE,\n",
      "\t\"C1_C24\" DOUBLE,\n",
      "\t\"C1_C25\" DOUBLE,\n",
      "\t\"C1_C26\" DOUBLE,\n",
      "\t\"C1_C27\" DOUBLE,\n",
      "\t\"C1_C28\" DOUBLE,\n",
      "\t\"C1_C29\" DOUBLE,\n",
      "\t\"C1_C30\" DOUBLE,\n",
      "\t\"C1_C31\" DOUBLE,\n",
      "\t\"C1_C32\" DOUBLE,\n",
      "\t\"C1_C33\" DOUBLE,\n",
      "\t\"C1_C34\" DOUBLE,\n",
      "\t\"C1_C35\" DOUBLE,\n",
      "\t\"C1_C36\" DOUBLE,\n",
      "\t\"C1_C37\" DOUBLE,\n",
      "\t\"C1_C38\" DOUBLE,\n",
      "\t\"C1_C39\" DOUBLE,\n",
      "\t\"C1_C40\" DOUBLE,\n",
      "\t\"C1_C41\" DOUBLE,\n",
      "\t\"C1_C42\" DOUBLE,\n",
      "\t\"C1_C43\" DOUBLE,\n",
      "\t\"C1_C44\" DOUBLE,\n",
      "\t\"C1_C45\" DOUBLE,\n",
      "\t\"C1_C46\" DOUBLE,\n",
      "\t\"C1_C47\" DOUBLE,\n",
      "\t\"C1_C48\" DOUBLE,\n",
      "\t\"C1_C49\" DOUBLE,\n",
      "\t\"C1_C50\" DOUBLE,\n",
      "\t\"C1_C51\" DOUBLE,\n",
      "\t\"C1_C52\" DOUBLE,\n",
      "\t\"C1_C53\" DOUBLE,\n",
      "\t\"C1_C54\" DOUBLE,\n",
      "\t\"C1_C55\" DOUBLE,\n",
      "\t\"C1_C56\" DOUBLE,\n",
      "\t\"C1_C57\" DOUBLE,\n",
      "\t\"C1_C58\" DOUBLE,\n",
      "\t\"C1_C59\" DOUBLE,\n",
      "\t\"C1_C60\" DOUBLE,\n",
      "\t\"C1_C61\" DOUBLE,\n",
      "\t\"C1_C62\" DOUBLE,\n",
      "\t\"C1_C63\" DOUBLE,\n",
      "\t\"C1_C64\" DOUBLE,\n",
      "\t\"C1_C65\" DOUBLE,\n",
      "\t\"C1_C66\" DOUBLE,\n",
      "\t\"C1_C67\" DOUBLE,\n",
      "\t\"C1_C68\" DOUBLE,\n",
      "\t\"C1_C69\" DOUBLE,\n",
      "\t\"C1_C70\" DOUBLE,\n",
      "\t\"C1_C71\" DOUBLE,\n",
      "\t\"C1_C72\" DOUBLE,\n",
      "\t\"C1_C73\" DOUBLE,\n",
      "\t\"C1_C74\" DOUBLE,\n",
      "\t\"C1_C75\" DOUBLE,\n",
      "\t\"C1_C76\" DOUBLE,\n",
      "\t\"C1_C77\" DOUBLE,\n",
      "\t\"C1_C78\" DOUBLE,\n",
      "\t\"C1_C79\" DOUBLE,\n",
      "\t\"C1_C80\" DOUBLE,\n",
      "\t\"C1_C81\" DOUBLE,\n",
      "\t\"C1_C82\" DOUBLE,\n",
      "\t\"C1_C83\" DOUBLE,\n",
      "\t\"C1_C84\" DOUBLE,\n",
      "\t\"C1_C85\" DOUBLE,\n",
      "\t\"C1_C86\" DOUBLE,\n",
      "\t\"C1_C87\" DOUBLE,\n",
      "\t\"C1_C88\" DOUBLE,\n",
      "\t\"C1_C89\" DOUBLE,\n",
      "\t\"C1_C90\" DOUBLE,\n",
      "\t\"C1_C91\" DOUBLE,\n",
      "\t\"C1_C92\" DOUBLE,\n",
      "\t\"C1_C93\" DOUBLE,\n",
      "\t\"C1_C94\" DOUBLE,\n",
      "\t\"C1_C95\" DOUBLE,\n",
      "\t\"C1_C96\" DOUBLE,\n",
      "\t\"C1_C97\" DOUBLE,\n",
      "\t\"C1_C98\" DOUBLE,\n",
      "\t\"C1_C99\" DOUBLE,\n",
      "\tPRIMARY KEY(MOMENT_KEY)\n",
      ")\n",
      "\n",
      "Done.\n",
      "Creating model prediction trigger ... \n",
      "CREATE TRIGGER runModel_BEN_word_vec_model_b8738905c16c\n",
      " \tAFTER INSERT\n",
      " \tON BEN.word_vec_model\n",
      " \tREFERENCING NEW AS NEWROW\n",
      " \tFOR EACH ROW\n",
      " \t\tINSERT INTO BEN.word_vec_model_PREDS(MOMENT_KEY,\"C1_C0\",\"C1_C1\",\"C1_C2\",\"C1_C3\",\"C1_C4\",\"C1_C5\",\"C1_C6\",\"C1_C7\",\"C1_C8\",\"C1_C9\",\"C1_C10\",\"C1_C11\",\"C1_C12\",\"C1_C13\",\"C1_C14\",\"C1_C15\",\"C1_C16\",\"C1_C17\",\"C1_C18\",\"C1_C19\",\"C1_C20\",\"C1_C21\",\"C1_C22\",\"C1_C23\",\"C1_C24\",\"C1_C25\",\"C1_C26\",\"C1_C27\",\"C1_C28\",\"C1_C29\",\"C1_C30\",\"C1_C31\",\"C1_C32\",\"C1_C33\",\"C1_C34\",\"C1_C35\",\"C1_C36\",\"C1_C37\",\"C1_C38\",\"C1_C39\",\"C1_C40\",\"C1_C41\",\"C1_C42\",\"C1_C43\",\"C1_C44\",\"C1_C45\",\"C1_C46\",\"C1_C47\",\"C1_C48\",\"C1_C49\",\"C1_C50\",\"C1_C51\",\"C1_C52\",\"C1_C53\",\"C1_C54\",\"C1_C55\",\"C1_C56\",\"C1_C57\",\"C1_C58\",\"C1_C59\",\"C1_C60\",\"C1_C61\",\"C1_C62\",\"C1_C63\",\"C1_C64\",\"C1_C65\",\"C1_C66\",\"C1_C67\",\"C1_C68\",\"C1_C69\",\"C1_C70\",\"C1_C71\",\"C1_C72\",\"C1_C73\",\"C1_C74\",\"C1_C75\",\"C1_C76\",\"C1_C77\",\"C1_C78\",\"C1_C79\",\"C1_C80\",\"C1_C81\",\"C1_C82\",\"C1_C83\",\"C1_C84\",\"C1_C85\",\"C1_C86\",\"C1_C87\",\"C1_C88\",\"C1_C89\",\"C1_C90\",\"C1_C91\",\"C1_C92\",\"C1_C93\",\"C1_C94\",\"C1_C95\",\"C1_C96\",\"C1_C97\",\"C1_C98\",\"C1_C99\") SELECT \tNEWROW.MOMENT_KEY, b.\"C1_C0\",b.\"C1_C1\",b.\"C1_C2\",b.\"C1_C3\",b.\"C1_C4\",b.\"C1_C5\",b.\"C1_C6\",b.\"C1_C7\",b.\"C1_C8\",b.\"C1_C9\",b.\"C1_C10\",b.\"C1_C11\",b.\"C1_C12\",b.\"C1_C13\",b.\"C1_C14\",b.\"C1_C15\",b.\"C1_C16\",b.\"C1_C17\",b.\"C1_C18\",b.\"C1_C19\",b.\"C1_C20\",b.\"C1_C21\",b.\"C1_C22\",b.\"C1_C23\",b.\"C1_C24\",b.\"C1_C25\",b.\"C1_C26\",b.\"C1_C27\",b.\"C1_C28\",b.\"C1_C29\",b.\"C1_C30\",b.\"C1_C31\",b.\"C1_C32\",b.\"C1_C33\",b.\"C1_C34\",b.\"C1_C35\",b.\"C1_C36\",b.\"C1_C37\",b.\"C1_C38\",b.\"C1_C39\",b.\"C1_C40\",b.\"C1_C41\",b.\"C1_C42\",b.\"C1_C43\",b.\"C1_C44\",b.\"C1_C45\",b.\"C1_C46\",b.\"C1_C47\",b.\"C1_C48\",b.\"C1_C49\",b.\"C1_C50\",b.\"C1_C51\",b.\"C1_C52\",b.\"C1_C53\",b.\"C1_C54\",b.\"C1_C55\",b.\"C1_C56\",b.\"C1_C57\",b.\"C1_C58\",b.\"C1_C59\",b.\"C1_C60\",b.\"C1_C61\",b.\"C1_C62\",b.\"C1_C63\",b.\"C1_C64\",b.\"C1_C65\",b.\"C1_C66\",b.\"C1_C67\",b.\"C1_C68\",b.\"C1_C69\",b.\"C1_C70\",b.\"C1_C71\",b.\"C1_C72\",b.\"C1_C73\",b.\"C1_C74\",b.\"C1_C75\",b.\"C1_C76\",b.\"C1_C77\",b.\"C1_C78\",b.\"C1_C79\",b.\"C1_C80\",b.\"C1_C81\",b.\"C1_C82\",b.\"C1_C83\",b.\"C1_C84\",b.\"C1_C85\",b.\"C1_C86\",b.\"C1_C87\",b.\"C1_C88\",b.\"C1_C89\",b.\"C1_C90\",b.\"C1_C91\",b.\"C1_C92\",b.\"C1_C93\",b.\"C1_C94\",b.\"C1_C95\",b.\"C1_C96\",b.\"C1_C97\",b.\"C1_C98\",b.\"C1_C99\" FROM new com.splicemachine.mlrunner.MLRunner('key_value', 'b8738905c16c', TRIM(CAST(NEWROW.Review as CHAR(41))), 'Review VARCHAR(5000)') as b (\"C1_C0\" DOUBLE,\"C1_C1\" DOUBLE,\"C1_C2\" DOUBLE,\"C1_C3\" DOUBLE,\"C1_C4\" DOUBLE,\"C1_C5\" DOUBLE,\"C1_C6\" DOUBLE,\"C1_C7\" DOUBLE,\"C1_C8\" DOUBLE,\"C1_C9\" DOUBLE,\"C1_C10\" DOUBLE,\"C1_C11\" DOUBLE,\"C1_C12\" DOUBLE,\"C1_C13\" DOUBLE,\"C1_C14\" DOUBLE,\"C1_C15\" DOUBLE,\"C1_C16\" DOUBLE,\"C1_C17\" DOUBLE,\"C1_C18\" DOUBLE,\"C1_C19\" DOUBLE,\"C1_C20\" DOUBLE,\"C1_C21\" DOUBLE,\"C1_C22\" DOUBLE,\"C1_C23\" DOUBLE,\"C1_C24\" DOUBLE,\"C1_C25\" DOUBLE,\"C1_C26\" DOUBLE,\"C1_C27\" DOUBLE,\"C1_C28\" DOUBLE,\"C1_C29\" DOUBLE,\"C1_C30\" DOUBLE,\"C1_C31\" DOUBLE,\"C1_C32\" DOUBLE,\"C1_C33\" DOUBLE,\"C1_C34\" DOUBLE,\"C1_C35\" DOUBLE,\"C1_C36\" DOUBLE,\"C1_C37\" DOUBLE,\"C1_C38\" DOUBLE,\"C1_C39\" DOUBLE,\"C1_C40\" DOUBLE,\"C1_C41\" DOUBLE,\"C1_C42\" DOUBLE,\"C1_C43\" DOUBLE,\"C1_C44\" DOUBLE,\"C1_C45\" DOUBLE,\"C1_C46\" DOUBLE,\"C1_C47\" DOUBLE,\"C1_C48\" DOUBLE,\"C1_C49\" DOUBLE,\"C1_C50\" DOUBLE,\"C1_C51\" DOUBLE,\"C1_C52\" DOUBLE,\"C1_C53\" DOUBLE,\"C1_C54\" DOUBLE,\"C1_C55\" DOUBLE,\"C1_C56\" DOUBLE,\"C1_C57\" DOUBLE,\"C1_C58\" DOUBLE,\"C1_C59\" DOUBLE,\"C1_C60\" DOUBLE,\"C1_C61\" DOUBLE,\"C1_C62\" DOUBLE,\"C1_C63\" DOUBLE,\"C1_C64\" DOUBLE,\"C1_C65\" DOUBLE,\"C1_C66\" DOUBLE,\"C1_C67\" DOUBLE,\"C1_C68\" DOUBLE,\"C1_C69\" DOUBLE,\"C1_C70\" DOUBLE,\"C1_C71\" DOUBLE,\"C1_C72\" DOUBLE,\"C1_C73\" DOUBLE,\"C1_C74\" DOUBLE,\"C1_C75\" DOUBLE,\"C1_C76\" DOUBLE,\"C1_C77\" DOUBLE,\"C1_C78\" DOUBLE,\"C1_C79\" DOUBLE,\"C1_C80\" DOUBLE,\"C1_C81\" DOUBLE,\"C1_C82\" DOUBLE,\"C1_C83\" DOUBLE,\"C1_C84\" DOUBLE,\"C1_C85\" DOUBLE,\"C1_C86\" DOUBLE,\"C1_C87\" DOUBLE,\"C1_C88\" DOUBLE,\"C1_C89\" DOUBLE,\"C1_C90\" DOUBLE,\"C1_C91\" DOUBLE,\"C1_C92\" DOUBLE,\"C1_C93\" DOUBLE,\"C1_C94\" DOUBLE,\"C1_C95\" DOUBLE,\"C1_C96\" DOUBLE,\"C1_C97\" DOUBLE,\"C1_C98\" DOUBLE,\"C1_C99\" DOUBLE)\n",
      "\n",
      "Done.\n",
      "Model Deployed.\n"
     ]
    }
   ],
   "source": [
    "# Get the run_id from the name. Note that multiple runs can have the same name, so this returns a list\n",
    "run_id = mlflow.get_run_ids_by_name('review_tokenizer')[0]\n",
    "# Get the model from that run\n",
    "w2v_model = mlflow.load_model(run_id=run_id, name='review_vectorizer_model')\n",
    "deploy_df = hc.asSparkFrame(reviews[['Text']]).withColumnRenamed('Text', 'Review')\n",
    "schema = 'REPLACE_ME_DBSCHEMA'\n",
    "schema='BEN'\n",
    "mlflow.deploy_db(w2v_model, deploy_df, schema, 'word_vec_model', [('MOMENT_KEY', 'INT')], run_id=run_id, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sweet! Let's try it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>Review                                                      </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>Tastes like Earl Grey, but it&#x27;s green tea so it&#x27;s healthier.</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: right;\">        C1</th><th style=\"text-align: right;\">        C2</th><th style=\"text-align: right;\">        C3</th><th style=\"text-align: right;\">       C4</th><th style=\"text-align: right;\">         C5</th><th style=\"text-align: right;\">         C6</th><th style=\"text-align: right;\">        C7</th><th style=\"text-align: right;\">        C8</th><th style=\"text-align: right;\">        C9</th><th style=\"text-align: right;\">       C10</th><th style=\"text-align: right;\">       C11</th><th style=\"text-align: right;\">       C12</th><th style=\"text-align: right;\">        C13</th><th style=\"text-align: right;\">        C14</th><th style=\"text-align: right;\">       C15</th><th style=\"text-align: right;\">     C16</th><th style=\"text-align: right;\">       C17</th><th style=\"text-align: right;\">        C18</th><th style=\"text-align: right;\">      C19</th><th style=\"text-align: right;\">        C20</th><th style=\"text-align: right;\">       C21</th><th style=\"text-align: right;\">       C22</th><th style=\"text-align: right;\">      C23</th><th style=\"text-align: right;\">         C24</th><th style=\"text-align: right;\">      C25</th><th style=\"text-align: right;\">       C26</th><th style=\"text-align: right;\">        C27</th><th style=\"text-align: right;\">       C28</th><th style=\"text-align: right;\">        C29</th><th style=\"text-align: right;\">        C30</th><th style=\"text-align: right;\">       C31</th><th style=\"text-align: right;\">         C32</th><th style=\"text-align: right;\">       C33</th><th style=\"text-align: right;\">        C34</th><th style=\"text-align: right;\">       C35</th><th style=\"text-align: right;\">      C36</th><th style=\"text-align: right;\">      C37</th><th style=\"text-align: right;\">       C38</th><th style=\"text-align: right;\">       C39</th><th style=\"text-align: right;\">        C40</th><th style=\"text-align: right;\">       C41</th><th style=\"text-align: right;\">       C42</th><th style=\"text-align: right;\">       C43</th><th style=\"text-align: right;\">        C44</th><th style=\"text-align: right;\">        C45</th><th style=\"text-align: right;\">         C46</th><th style=\"text-align: right;\">       C47</th><th style=\"text-align: right;\">      C48</th><th style=\"text-align: right;\">        C49</th><th style=\"text-align: right;\">       C50</th><th style=\"text-align: right;\">       C51</th><th style=\"text-align: right;\">        C52</th><th style=\"text-align: right;\">        C53</th><th style=\"text-align: right;\">        C54</th><th style=\"text-align: right;\">        C55</th><th style=\"text-align: right;\">       C56</th><th style=\"text-align: right;\">         C57</th><th style=\"text-align: right;\">      C58</th><th style=\"text-align: right;\">       C59</th><th style=\"text-align: right;\">        C60</th><th style=\"text-align: right;\">       C61</th><th style=\"text-align: right;\">       C62</th><th style=\"text-align: right;\">       C63</th><th style=\"text-align: right;\">        C64</th><th style=\"text-align: right;\">        C65</th><th style=\"text-align: right;\">        C66</th><th style=\"text-align: right;\">       C67</th><th style=\"text-align: right;\">      C68</th><th style=\"text-align: right;\">       C69</th><th style=\"text-align: right;\">       C70</th><th style=\"text-align: right;\">        C71</th><th style=\"text-align: right;\">        C72</th><th style=\"text-align: right;\">       C73</th><th style=\"text-align: right;\">       C74</th><th style=\"text-align: right;\">       C75</th><th style=\"text-align: right;\">       C76</th><th style=\"text-align: right;\">        C77</th><th style=\"text-align: right;\">       C78</th><th style=\"text-align: right;\">       C79</th><th style=\"text-align: right;\">       C80</th><th style=\"text-align: right;\">        C81</th><th style=\"text-align: right;\">       C82</th><th style=\"text-align: right;\">         C83</th><th style=\"text-align: right;\">        C84</th><th style=\"text-align: right;\">       C85</th><th style=\"text-align: right;\">        C86</th><th style=\"text-align: right;\">       C87</th><th style=\"text-align: right;\">       C88</th><th style=\"text-align: right;\">       C89</th><th style=\"text-align: right;\">        C90</th><th style=\"text-align: right;\">        C91</th><th style=\"text-align: right;\">       C92</th><th style=\"text-align: right;\">        C93</th><th style=\"text-align: right;\">       C94</th><th style=\"text-align: right;\">        C95</th><th style=\"text-align: right;\">        C96</th><th style=\"text-align: right;\">       C97</th><th style=\"text-align: right;\">        C98</th><th style=\"text-align: right;\">        C99</th><th style=\"text-align: right;\">       C100</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: right;\"> 0.0118207</td><td style=\"text-align: right;\">-0.101889 </td><td style=\"text-align: right;\"> 0.0147137</td><td style=\"text-align: right;\">0.278492 </td><td style=\"text-align: right;\"> 0.0469628 </td><td style=\"text-align: right;\">-0.172297  </td><td style=\"text-align: right;\">-0.0129039</td><td style=\"text-align: right;\"> 0.0830429</td><td style=\"text-align: right;\">-0.0914811</td><td style=\"text-align: right;\"> 0.0574676</td><td style=\"text-align: right;\">-0.0639514</td><td style=\"text-align: right;\"> 0.120431 </td><td style=\"text-align: right;\">-0.0962916 </td><td style=\"text-align: right;\">-0.0178387 </td><td style=\"text-align: right;\"> 0.0759381</td><td style=\"text-align: right;\">0.179793</td><td style=\"text-align: right;\">-0.0157825</td><td style=\"text-align: right;\">-0.12134   </td><td style=\"text-align: right;\">0.0527563</td><td style=\"text-align: right;\"> 0.0573318 </td><td style=\"text-align: right;\"> 0.0509253</td><td style=\"text-align: right;\"> 0.120528 </td><td style=\"text-align: right;\">0.15535  </td><td style=\"text-align: right;\">-0.0792156  </td><td style=\"text-align: right;\">0.0379387</td><td style=\"text-align: right;\">-0.0660397</td><td style=\"text-align: right;\">-0.00495673</td><td style=\"text-align: right;\">-0.079089 </td><td style=\"text-align: right;\">-0.0337948 </td><td style=\"text-align: right;\"> 0.0854287 </td><td style=\"text-align: right;\">-0.270305 </td><td style=\"text-align: right;\"> 0.14011    </td><td style=\"text-align: right;\"> 0.0664857</td><td style=\"text-align: right;\">-0.0903058 </td><td style=\"text-align: right;\">-0.0534608</td><td style=\"text-align: right;\">0.139986 </td><td style=\"text-align: right;\">0.114537 </td><td style=\"text-align: right;\">-0.0631098</td><td style=\"text-align: right;\">-0.145617 </td><td style=\"text-align: right;\"> 0.00200458</td><td style=\"text-align: right;\"> 0.195605 </td><td style=\"text-align: right;\">-0.12095  </td><td style=\"text-align: right;\"> 0.0142132</td><td style=\"text-align: right;\"> 0.00382048</td><td style=\"text-align: right;\"> 0.00467813</td><td style=\"text-align: right;\">-0.0647183  </td><td style=\"text-align: right;\"> 0.12261  </td><td style=\"text-align: right;\">0.10863  </td><td style=\"text-align: right;\">-0.179427  </td><td style=\"text-align: right;\"> 0.100265 </td><td style=\"text-align: right;\">-0.0961187</td><td style=\"text-align: right;\"> 0.108058  </td><td style=\"text-align: right;\"> 0.0614284 </td><td style=\"text-align: right;\">-0.113856  </td><td style=\"text-align: right;\">-0.0481913 </td><td style=\"text-align: right;\">-0.0260456</td><td style=\"text-align: right;\"> 0.0346338  </td><td style=\"text-align: right;\">0.127268 </td><td style=\"text-align: right;\"> 0.112572 </td><td style=\"text-align: right;\">-0.0689213 </td><td style=\"text-align: right;\"> 0.032635 </td><td style=\"text-align: right;\"> 0.1602   </td><td style=\"text-align: right;\">-0.0245575</td><td style=\"text-align: right;\">-0.0889716 </td><td style=\"text-align: right;\"> 0.0344523 </td><td style=\"text-align: right;\"> 0.0388336 </td><td style=\"text-align: right;\">-0.0977627</td><td style=\"text-align: right;\">0.0664074</td><td style=\"text-align: right;\">-0.0214509</td><td style=\"text-align: right;\">-0.0922424</td><td style=\"text-align: right;\">-0.0759032 </td><td style=\"text-align: right;\"> 0.168529  </td><td style=\"text-align: right;\"> 0.0484152</td><td style=\"text-align: right;\">0.0341554 </td><td style=\"text-align: right;\">-0.0530976</td><td style=\"text-align: right;\"> 0.0814525</td><td style=\"text-align: right;\"> 0.114328  </td><td style=\"text-align: right;\"> 0.118376 </td><td style=\"text-align: right;\"> 0.0836662</td><td style=\"text-align: right;\">-0.155847 </td><td style=\"text-align: right;\"> 0.0524612 </td><td style=\"text-align: right;\">-0.0582578</td><td style=\"text-align: right;\"> 0.0343186  </td><td style=\"text-align: right;\">-0.0191061 </td><td style=\"text-align: right;\"> 0.0982101</td><td style=\"text-align: right;\">-0.0475758 </td><td style=\"text-align: right;\">0.00883958</td><td style=\"text-align: right;\"> 0.159842 </td><td style=\"text-align: right;\">-0.0601515</td><td style=\"text-align: right;\"> 0.0405056 </td><td style=\"text-align: right;\">-0.00583725</td><td style=\"text-align: right;\">-0.113468 </td><td style=\"text-align: right;\"> 0.0123878 </td><td style=\"text-align: right;\"> 0.0548682</td><td style=\"text-align: right;\"> 0.0231149 </td><td style=\"text-align: right;\">-0.153038  </td><td style=\"text-align: right;\">-0.0830139</td><td style=\"text-align: right;\"> 0.0517786 </td><td style=\"text-align: right;\">-0.167759  </td><td style=\"text-align: right;\"> 0.0590884 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">-0.0647607</td><td style=\"text-align: right;\">-0.0727755</td><td style=\"text-align: right;\">-0.115707 </td><td style=\"text-align: right;\">0.0486232</td><td style=\"text-align: right;\"> 0.00228006</td><td style=\"text-align: right;\">-0.0894267 </td><td style=\"text-align: right;\">-0.0550715</td><td style=\"text-align: right;\"> 0.0182499</td><td style=\"text-align: right;\"> 0.104742 </td><td style=\"text-align: right;\"> 0.0961806</td><td style=\"text-align: right;\">-0.0779413</td><td style=\"text-align: right;\">-0.0978953</td><td style=\"text-align: right;\"> 0.115459  </td><td style=\"text-align: right;\">-0.153234  </td><td style=\"text-align: right;\"> 0.0496184</td><td style=\"text-align: right;\">0.168469</td><td style=\"text-align: right;\">-0.0180559</td><td style=\"text-align: right;\"> 0.122231  </td><td style=\"text-align: right;\">0.0145231</td><td style=\"text-align: right;\"> 0.0172325 </td><td style=\"text-align: right;\">-0.0637962</td><td style=\"text-align: right;\">-0.126619 </td><td style=\"text-align: right;\">0.15836  </td><td style=\"text-align: right;\">-0.0885775  </td><td style=\"text-align: right;\">0.0203511</td><td style=\"text-align: right;\">-0.0701156</td><td style=\"text-align: right;\">-0.040133  </td><td style=\"text-align: right;\">-0.304234 </td><td style=\"text-align: right;\">-0.0746839 </td><td style=\"text-align: right;\">-0.0674719 </td><td style=\"text-align: right;\">-0.103309 </td><td style=\"text-align: right;\"> 0.0273377  </td><td style=\"text-align: right;\"> 0.108569 </td><td style=\"text-align: right;\"> 0.0180806 </td><td style=\"text-align: right;\">-0.0350313</td><td style=\"text-align: right;\">0.21245  </td><td style=\"text-align: right;\">0.0424701</td><td style=\"text-align: right;\">-0.126426 </td><td style=\"text-align: right;\">-0.0514539</td><td style=\"text-align: right;\">-0.112131  </td><td style=\"text-align: right;\"> 0.0785268</td><td style=\"text-align: right;\">-0.0475902</td><td style=\"text-align: right;\">-0.166364 </td><td style=\"text-align: right;\"> 0.253536  </td><td style=\"text-align: right;\">-0.0662786 </td><td style=\"text-align: right;\"> 0.133733   </td><td style=\"text-align: right;\"> 0.139492 </td><td style=\"text-align: right;\">0.144991 </td><td style=\"text-align: right;\"> 0.00557501</td><td style=\"text-align: right;\">-0.079479 </td><td style=\"text-align: right;\">-0.0282375</td><td style=\"text-align: right;\">-0.0368711 </td><td style=\"text-align: right;\"> 0.0286063 </td><td style=\"text-align: right;\">-0.0782206 </td><td style=\"text-align: right;\"> 0.00642933</td><td style=\"text-align: right;\"> 0.11829  </td><td style=\"text-align: right;\"> 0.00523307 </td><td style=\"text-align: right;\">0.0984682</td><td style=\"text-align: right;\">-0.0332558</td><td style=\"text-align: right;\"> 0.0394351 </td><td style=\"text-align: right;\"> 0.0516561</td><td style=\"text-align: right;\"> 0.191958 </td><td style=\"text-align: right;\">-0.0593793</td><td style=\"text-align: right;\">-0.0620899 </td><td style=\"text-align: right;\"> 0.00285376</td><td style=\"text-align: right;\"> 0.0289805 </td><td style=\"text-align: right;\">-0.0604133</td><td style=\"text-align: right;\">0.0530364</td><td style=\"text-align: right;\">-0.0330077</td><td style=\"text-align: right;\"> 0.0675788</td><td style=\"text-align: right;\">-0.0419845 </td><td style=\"text-align: right;\"> 0.157696  </td><td style=\"text-align: right;\"> 0.0653407</td><td style=\"text-align: right;\">0.161192  </td><td style=\"text-align: right;\"> 0.0112189</td><td style=\"text-align: right;\"> 0.111598 </td><td style=\"text-align: right;\"> 0.0453798 </td><td style=\"text-align: right;\"> 0.194989 </td><td style=\"text-align: right;\"> 0.138888 </td><td style=\"text-align: right;\"> 0.0231592</td><td style=\"text-align: right;\"> 0.00690089</td><td style=\"text-align: right;\">-0.166217 </td><td style=\"text-align: right;\">-0.0599659  </td><td style=\"text-align: right;\">-0.102449  </td><td style=\"text-align: right;\"> 0.0882302</td><td style=\"text-align: right;\">-0.00776763</td><td style=\"text-align: right;\">0.113629  </td><td style=\"text-align: right;\"> 0.0108894</td><td style=\"text-align: right;\">-0.222703 </td><td style=\"text-align: right;\">-0.15212   </td><td style=\"text-align: right;\">-0.0933445 </td><td style=\"text-align: right;\">-0.0339013</td><td style=\"text-align: right;\"> 0.0440877 </td><td style=\"text-align: right;\">-0.116198 </td><td style=\"text-align: right;\"> 0.00132958</td><td style=\"text-align: right;\">-0.00732167</td><td style=\"text-align: right;\">-0.157039 </td><td style=\"text-align: right;\">-0.039421  </td><td style=\"text-align: right;\">-0.0166677 </td><td style=\"text-align: right;\">-0.0640155 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">-0.0385096</td><td style=\"text-align: right;\">-0.164311 </td><td style=\"text-align: right;\"> 0.0749115</td><td style=\"text-align: right;\">0.105914 </td><td style=\"text-align: right;\"> 0.0364867 </td><td style=\"text-align: right;\">-0.14865   </td><td style=\"text-align: right;\"> 0.0147054</td><td style=\"text-align: right;\"> 0.0432582</td><td style=\"text-align: right;\"> 0.0389016</td><td style=\"text-align: right;\"> 0.0396217</td><td style=\"text-align: right;\">-0.0317641</td><td style=\"text-align: right;\"> 0.095205 </td><td style=\"text-align: right;\">-0.0875548 </td><td style=\"text-align: right;\">-0.00580839</td><td style=\"text-align: right;\"> 0.0605815</td><td style=\"text-align: right;\">0.184019</td><td style=\"text-align: right;\">-0.0828741</td><td style=\"text-align: right;\">-0.075011  </td><td style=\"text-align: right;\">0.0940388</td><td style=\"text-align: right;\"> 0.0624332 </td><td style=\"text-align: right;\">-0.0170392</td><td style=\"text-align: right;\">-0.0428544</td><td style=\"text-align: right;\">0.127299 </td><td style=\"text-align: right;\"> 0.0350129  </td><td style=\"text-align: right;\">0.0544682</td><td style=\"text-align: right;\">-0.0712631</td><td style=\"text-align: right;\"> 0.0524339 </td><td style=\"text-align: right;\">-0.114599 </td><td style=\"text-align: right;\"> 0.0339514 </td><td style=\"text-align: right;\">-0.0160842 </td><td style=\"text-align: right;\">-0.206147 </td><td style=\"text-align: right;\">-0.0418927  </td><td style=\"text-align: right;\">-0.0193752</td><td style=\"text-align: right;\"> 0.0589492 </td><td style=\"text-align: right;\">-0.0550368</td><td style=\"text-align: right;\">0.160735 </td><td style=\"text-align: right;\">0.0416373</td><td style=\"text-align: right;\">-0.106153 </td><td style=\"text-align: right;\">-0.0952572</td><td style=\"text-align: right;\">-0.0907418 </td><td style=\"text-align: right;\"> 0.140228 </td><td style=\"text-align: right;\">-0.141172 </td><td style=\"text-align: right;\">-0.0906205</td><td style=\"text-align: right;\"> 0.172362  </td><td style=\"text-align: right;\">-0.0140963 </td><td style=\"text-align: right;\">-0.0684992  </td><td style=\"text-align: right;\"> 0.0649892</td><td style=\"text-align: right;\">0.218862 </td><td style=\"text-align: right;\">-0.112363  </td><td style=\"text-align: right;\"> 0.0910496</td><td style=\"text-align: right;\">-0.0594554</td><td style=\"text-align: right;\"> 0.0785124 </td><td style=\"text-align: right;\"> 0.0352732 </td><td style=\"text-align: right;\"> 0.00169446</td><td style=\"text-align: right;\"> 0.0356621 </td><td style=\"text-align: right;\"> 0.126364 </td><td style=\"text-align: right;\">-0.0982795  </td><td style=\"text-align: right;\">0.136849 </td><td style=\"text-align: right;\"> 0.0250553</td><td style=\"text-align: right;\">-0.132051  </td><td style=\"text-align: right;\"> 0.0240202</td><td style=\"text-align: right;\"> 0.0611908</td><td style=\"text-align: right;\">-0.0153794</td><td style=\"text-align: right;\">-0.0686614 </td><td style=\"text-align: right;\">-0.0755786 </td><td style=\"text-align: right;\"> 0.070956  </td><td style=\"text-align: right;\">-0.0514114</td><td style=\"text-align: right;\">0.101125 </td><td style=\"text-align: right;\">-0.0541772</td><td style=\"text-align: right;\"> 0.0603693</td><td style=\"text-align: right;\">-0.151588  </td><td style=\"text-align: right;\"> 0.118073  </td><td style=\"text-align: right;\"> 0.0864352</td><td style=\"text-align: right;\">0.0697684 </td><td style=\"text-align: right;\"> 0.0771088</td><td style=\"text-align: right;\"> 0.127461 </td><td style=\"text-align: right;\">-0.00255191</td><td style=\"text-align: right;\"> 0.171573 </td><td style=\"text-align: right;\"> 0.0162494</td><td style=\"text-align: right;\">-0.0422113</td><td style=\"text-align: right;\">-0.0200063 </td><td style=\"text-align: right;\">-0.016754 </td><td style=\"text-align: right;\">-0.0927813  </td><td style=\"text-align: right;\">-0.0477321 </td><td style=\"text-align: right;\">-0.0520219</td><td style=\"text-align: right;\">-0.123997  </td><td style=\"text-align: right;\">0.147805  </td><td style=\"text-align: right;\"> 0.0895179</td><td style=\"text-align: right;\">-0.0564334</td><td style=\"text-align: right;\"> 0.0667794 </td><td style=\"text-align: right;\">-0.0441478 </td><td style=\"text-align: right;\">-0.263763 </td><td style=\"text-align: right;\"> 0.0178793 </td><td style=\"text-align: right;\">-0.0385753</td><td style=\"text-align: right;\"> 0.0908309 </td><td style=\"text-align: right;\">-0.213796  </td><td style=\"text-align: right;\">-0.0448226</td><td style=\"text-align: right;\"> 0.0337263 </td><td style=\"text-align: right;\"> 0.0551377 </td><td style=\"text-align: right;\">-0.00646082</td></tr>\n",
       "<tr><td style=\"text-align: right;\">-0.0335025</td><td style=\"text-align: right;\">-0.104515 </td><td style=\"text-align: right;\"> 0.0670558</td><td style=\"text-align: right;\">0.125524 </td><td style=\"text-align: right;\"> 0.121117  </td><td style=\"text-align: right;\">-0.132281  </td><td style=\"text-align: right;\">-0.0278141</td><td style=\"text-align: right;\"> 0.0481801</td><td style=\"text-align: right;\"> 0.0214209</td><td style=\"text-align: right;\"> 0.0399683</td><td style=\"text-align: right;\">-0.0675355</td><td style=\"text-align: right;\"> 0.0613814</td><td style=\"text-align: right;\">-0.0557912 </td><td style=\"text-align: right;\">-0.0336307 </td><td style=\"text-align: right;\"> 0.0842112</td><td style=\"text-align: right;\">0.186743</td><td style=\"text-align: right;\">-0.087822 </td><td style=\"text-align: right;\"> 0.00219227</td><td style=\"text-align: right;\">0.108149 </td><td style=\"text-align: right;\">-0.00918456</td><td style=\"text-align: right;\">-0.0524869</td><td style=\"text-align: right;\">-0.0394393</td><td style=\"text-align: right;\">0.0391789</td><td style=\"text-align: right;\"> 0.0445749  </td><td style=\"text-align: right;\">0.0390993</td><td style=\"text-align: right;\">-0.0879633</td><td style=\"text-align: right;\"> 0.00584351</td><td style=\"text-align: right;\">-0.144511 </td><td style=\"text-align: right;\"> 0.0167408 </td><td style=\"text-align: right;\">-0.13431   </td><td style=\"text-align: right;\">-0.0807045</td><td style=\"text-align: right;\"> 0.0671659  </td><td style=\"text-align: right;\"> 0.0427141</td><td style=\"text-align: right;\"> 0.041196  </td><td style=\"text-align: right;\">-0.0704532</td><td style=\"text-align: right;\">0.181694 </td><td style=\"text-align: right;\">0.0836733</td><td style=\"text-align: right;\">-0.0584921</td><td style=\"text-align: right;\">-0.0831199</td><td style=\"text-align: right;\"> 0.0848009 </td><td style=\"text-align: right;\"> 0.169781 </td><td style=\"text-align: right;\">-0.0153495</td><td style=\"text-align: right;\">-0.190452 </td><td style=\"text-align: right;\"> 0.0853043 </td><td style=\"text-align: right;\"> 0.0573439 </td><td style=\"text-align: right;\">-0.0407362  </td><td style=\"text-align: right;\"> 0.107085 </td><td style=\"text-align: right;\">0.0810725</td><td style=\"text-align: right;\">-0.200234  </td><td style=\"text-align: right;\"> 0.179503 </td><td style=\"text-align: right;\">-0.0225273</td><td style=\"text-align: right;\"> 0.0183807 </td><td style=\"text-align: right;\"> 0.0216287 </td><td style=\"text-align: right;\"> 0.0350069 </td><td style=\"text-align: right;\"> 0.0458474 </td><td style=\"text-align: right;\"> 0.136206 </td><td style=\"text-align: right;\"> 0.0118192  </td><td style=\"text-align: right;\">0.0873194</td><td style=\"text-align: right;\"> 0.0177314</td><td style=\"text-align: right;\">-0.0423478 </td><td style=\"text-align: right;\">-0.0389214</td><td style=\"text-align: right;\"> 0.0677268</td><td style=\"text-align: right;\">-0.0431368</td><td style=\"text-align: right;\">-0.00251263</td><td style=\"text-align: right;\"> 0.0645829 </td><td style=\"text-align: right;\">-0.0395505 </td><td style=\"text-align: right;\"> 0.0327319</td><td style=\"text-align: right;\">0.072015 </td><td style=\"text-align: right;\"> 0.0300226</td><td style=\"text-align: right;\"> 0.0785264</td><td style=\"text-align: right;\">-0.0905882 </td><td style=\"text-align: right;\"> 0.0845776 </td><td style=\"text-align: right;\"> 0.127451 </td><td style=\"text-align: right;\">0.0224333 </td><td style=\"text-align: right;\"> 0.050228 </td><td style=\"text-align: right;\"> 0.108584 </td><td style=\"text-align: right;\">-0.0275074 </td><td style=\"text-align: right;\"> 0.215823 </td><td style=\"text-align: right;\"> 0.107455 </td><td style=\"text-align: right;\">-0.0603542</td><td style=\"text-align: right;\"> 0.0351557 </td><td style=\"text-align: right;\">-0.0219788</td><td style=\"text-align: right;\">-0.126746   </td><td style=\"text-align: right;\">-0.0453881 </td><td style=\"text-align: right;\"> 0.0899957</td><td style=\"text-align: right;\">-0.0165066 </td><td style=\"text-align: right;\">0.0925419 </td><td style=\"text-align: right;\">-0.0128623</td><td style=\"text-align: right;\">-0.102198 </td><td style=\"text-align: right;\">-0.00859935</td><td style=\"text-align: right;\">-0.058625  </td><td style=\"text-align: right;\">-0.289004 </td><td style=\"text-align: right;\">-0.0106296 </td><td style=\"text-align: right;\">-0.0464101</td><td style=\"text-align: right;\"> 0.105933  </td><td style=\"text-align: right;\">-0.110108  </td><td style=\"text-align: right;\">-0.0585328</td><td style=\"text-align: right;\">-0.0127461 </td><td style=\"text-align: right;\"> 0.00460146</td><td style=\"text-align: right;\"> 0.00230653</td></tr>\n",
       "<tr><td style=\"text-align: right;\">-0.0201451</td><td style=\"text-align: right;\">-0.132165 </td><td style=\"text-align: right;\"> 0.0379038</td><td style=\"text-align: right;\">0.161137 </td><td style=\"text-align: right;\"> 0.124739  </td><td style=\"text-align: right;\">-0.178314  </td><td style=\"text-align: right;\">-0.0275578</td><td style=\"text-align: right;\"> 0.0744305</td><td style=\"text-align: right;\"> 0.0447734</td><td style=\"text-align: right;\">-0.0425596</td><td style=\"text-align: right;\"> 0.0165299</td><td style=\"text-align: right;\"> 0.145382 </td><td style=\"text-align: right;\">-0.0457867 </td><td style=\"text-align: right;\">-0.0420987 </td><td style=\"text-align: right;\"> 0.137567 </td><td style=\"text-align: right;\">0.119278</td><td style=\"text-align: right;\">-0.11449  </td><td style=\"text-align: right;\">-0.0836703 </td><td style=\"text-align: right;\">0.164161 </td><td style=\"text-align: right;\"> 0.0346721 </td><td style=\"text-align: right;\"> 0.0557758</td><td style=\"text-align: right;\">-0.0353933</td><td style=\"text-align: right;\">0.17646  </td><td style=\"text-align: right;\">-0.0539324  </td><td style=\"text-align: right;\">0.0196024</td><td style=\"text-align: right;\">-0.0559806</td><td style=\"text-align: right;\"> 0.0231409 </td><td style=\"text-align: right;\">-0.108781 </td><td style=\"text-align: right;\">-0.0593631 </td><td style=\"text-align: right;\">-0.0490342 </td><td style=\"text-align: right;\">-0.212415 </td><td style=\"text-align: right;\"> 0.0274417  </td><td style=\"text-align: right;\">-0.046688 </td><td style=\"text-align: right;\">-0.0630046 </td><td style=\"text-align: right;\">-0.0759929</td><td style=\"text-align: right;\">0.15379  </td><td style=\"text-align: right;\">0.0883736</td><td style=\"text-align: right;\">-0.0141757</td><td style=\"text-align: right;\">-0.0614761</td><td style=\"text-align: right;\"> 0.0105543 </td><td style=\"text-align: right;\"> 0.138841 </td><td style=\"text-align: right;\">-0.0907222</td><td style=\"text-align: right;\">-0.149053 </td><td style=\"text-align: right;\"> 0.0686344 </td><td style=\"text-align: right;\">-0.00758567</td><td style=\"text-align: right;\"> 0.000467301</td><td style=\"text-align: right;\"> 0.101195 </td><td style=\"text-align: right;\">0.171427 </td><td style=\"text-align: right;\">-0.17147   </td><td style=\"text-align: right;\"> 0.0747841</td><td style=\"text-align: right;\">-0.0688282</td><td style=\"text-align: right;\"> 0.123618  </td><td style=\"text-align: right;\"> 0.0871568 </td><td style=\"text-align: right;\">-0.0806954 </td><td style=\"text-align: right;\"> 0.0701221 </td><td style=\"text-align: right;\">-0.0372177</td><td style=\"text-align: right;\"> 0.0681081  </td><td style=\"text-align: right;\">0.1486   </td><td style=\"text-align: right;\"> 0.099561 </td><td style=\"text-align: right;\"> 0.0636671 </td><td style=\"text-align: right;\"> 0.0701539</td><td style=\"text-align: right;\"> 0.147183 </td><td style=\"text-align: right;\">-0.0509202</td><td style=\"text-align: right;\">-0.0172091 </td><td style=\"text-align: right;\"> 0.0473874 </td><td style=\"text-align: right;\"> 0.0857132 </td><td style=\"text-align: right;\">-0.047345 </td><td style=\"text-align: right;\">0.137962 </td><td style=\"text-align: right;\">-0.0638686</td><td style=\"text-align: right;\">-0.0367208</td><td style=\"text-align: right;\">-0.145565  </td><td style=\"text-align: right;\"> 0.163988  </td><td style=\"text-align: right;\"> 0.0627691</td><td style=\"text-align: right;\">0.0367048 </td><td style=\"text-align: right;\">-0.0394994</td><td style=\"text-align: right;\"> 0.0967054</td><td style=\"text-align: right;\"> 0.030205  </td><td style=\"text-align: right;\"> 0.157469 </td><td style=\"text-align: right;\"> 0.0740559</td><td style=\"text-align: right;\">-0.0329426</td><td style=\"text-align: right;\"> 0.0543468 </td><td style=\"text-align: right;\"> 0.0171674</td><td style=\"text-align: right;\">-0.0890092  </td><td style=\"text-align: right;\">-0.0360687 </td><td style=\"text-align: right;\"> 0.0424948</td><td style=\"text-align: right;\">-0.0229394 </td><td style=\"text-align: right;\">0.17668   </td><td style=\"text-align: right;\"> 0.067795 </td><td style=\"text-align: right;\">-0.0300711</td><td style=\"text-align: right;\">-0.0254723 </td><td style=\"text-align: right;\">-0.0622449 </td><td style=\"text-align: right;\">-0.242192 </td><td style=\"text-align: right;\">-0.017879  </td><td style=\"text-align: right;\">-0.10616  </td><td style=\"text-align: right;\"> 0.0546131 </td><td style=\"text-align: right;\">-0.161157  </td><td style=\"text-align: right;\">-0.115351 </td><td style=\"text-align: right;\"> 0.123939  </td><td style=\"text-align: right;\">-0.0864101 </td><td style=\"text-align: right;\"> 0.0883715 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">-0.214375 </td><td style=\"text-align: right;\">-0.0369557</td><td style=\"text-align: right;\">-0.101649 </td><td style=\"text-align: right;\">0.219768 </td><td style=\"text-align: right;\"> 0.0442532 </td><td style=\"text-align: right;\">-0.0170526 </td><td style=\"text-align: right;\">-0.11648  </td><td style=\"text-align: right;\"> 0.126972 </td><td style=\"text-align: right;\"> 0.197733 </td><td style=\"text-align: right;\"> 0.101063 </td><td style=\"text-align: right;\"> 0.135763 </td><td style=\"text-align: right;\">-0.0163704</td><td style=\"text-align: right;\"> 0.156413  </td><td style=\"text-align: right;\">-0.111991  </td><td style=\"text-align: right;\">-0.0803099</td><td style=\"text-align: right;\">0.205641</td><td style=\"text-align: right;\">-0.0581775</td><td style=\"text-align: right;\"> 0.131295  </td><td style=\"text-align: right;\">0.15432  </td><td style=\"text-align: right;\"> 0.163607  </td><td style=\"text-align: right;\">-0.046011 </td><td style=\"text-align: right;\">-0.172902 </td><td style=\"text-align: right;\">0.158536 </td><td style=\"text-align: right;\"> 0.0242689  </td><td style=\"text-align: right;\">0.114504 </td><td style=\"text-align: right;\">-0.160656 </td><td style=\"text-align: right;\">-0.143985  </td><td style=\"text-align: right;\">-0.122883 </td><td style=\"text-align: right;\">-0.0575605 </td><td style=\"text-align: right;\">-0.23525   </td><td style=\"text-align: right;\">-0.0999923</td><td style=\"text-align: right;\"> 0.0764702  </td><td style=\"text-align: right;\"> 0.0782552</td><td style=\"text-align: right;\">-0.0861496 </td><td style=\"text-align: right;\"> 0.0464735</td><td style=\"text-align: right;\">0.121648 </td><td style=\"text-align: right;\">0.131931 </td><td style=\"text-align: right;\">-0.136769 </td><td style=\"text-align: right;\">-0.180226 </td><td style=\"text-align: right;\">-0.00352418</td><td style=\"text-align: right;\"> 0.0587089</td><td style=\"text-align: right;\">-0.0295637</td><td style=\"text-align: right;\">-0.185958 </td><td style=\"text-align: right;\"> 0.117099  </td><td style=\"text-align: right;\"> 0.0319455 </td><td style=\"text-align: right;\">-0.0144965  </td><td style=\"text-align: right;\"> 0.0958378</td><td style=\"text-align: right;\">0.167461 </td><td style=\"text-align: right;\"> 0.0451791 </td><td style=\"text-align: right;\">-0.104298 </td><td style=\"text-align: right;\">-0.105662 </td><td style=\"text-align: right;\">-0.0157025 </td><td style=\"text-align: right;\"> 0.133378  </td><td style=\"text-align: right;\">-0.00208287</td><td style=\"text-align: right;\"> 0.0573898 </td><td style=\"text-align: right;\"> 0.148256 </td><td style=\"text-align: right;\"> 0.0580269  </td><td style=\"text-align: right;\">0.126037 </td><td style=\"text-align: right;\">-0.0436015</td><td style=\"text-align: right;\">-0.0891411 </td><td style=\"text-align: right;\">-0.149922 </td><td style=\"text-align: right;\">-0.0155653</td><td style=\"text-align: right;\">-0.0691431</td><td style=\"text-align: right;\"> 0.0369772 </td><td style=\"text-align: right;\"> 0.0261047 </td><td style=\"text-align: right;\"> 0.030653  </td><td style=\"text-align: right;\">-0.0814072</td><td style=\"text-align: right;\">0.0576432</td><td style=\"text-align: right;\"> 0.0169297</td><td style=\"text-align: right;\"> 0.0700941</td><td style=\"text-align: right;\">-0.102137  </td><td style=\"text-align: right;\">-0.00144821</td><td style=\"text-align: right;\"> 0.142381 </td><td style=\"text-align: right;\">0.0538387 </td><td style=\"text-align: right;\"> 0.122036 </td><td style=\"text-align: right;\"> 0.183889 </td><td style=\"text-align: right;\"> 0.103282  </td><td style=\"text-align: right;\"> 0.20281  </td><td style=\"text-align: right;\">-0.0831994</td><td style=\"text-align: right;\"> 0.0495038</td><td style=\"text-align: right;\">-0.0367568 </td><td style=\"text-align: right;\"> 0.0443851</td><td style=\"text-align: right;\"> 0.00556307 </td><td style=\"text-align: right;\">-0.0492791 </td><td style=\"text-align: right;\">-0.125605 </td><td style=\"text-align: right;\"> 0.021378  </td><td style=\"text-align: right;\">0.047709  </td><td style=\"text-align: right;\"> 0.188007 </td><td style=\"text-align: right;\"> 0.0488827</td><td style=\"text-align: right;\">-0.0578316 </td><td style=\"text-align: right;\"> 0.0528324 </td><td style=\"text-align: right;\">-0.109295 </td><td style=\"text-align: right;\"> 0.0497787 </td><td style=\"text-align: right;\">-0.0638945</td><td style=\"text-align: right;\">-0.0914897 </td><td style=\"text-align: right;\">-0.120594  </td><td style=\"text-align: right;\">-0.157838 </td><td style=\"text-align: right;\">-0.114866  </td><td style=\"text-align: right;\">-0.0680224 </td><td style=\"text-align: right;\">-0.0389557 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">-0.0216595</td><td style=\"text-align: right;\">-0.0860846</td><td style=\"text-align: right;\">-0.0521659</td><td style=\"text-align: right;\">0.127222 </td><td style=\"text-align: right;\"> 0.0805164 </td><td style=\"text-align: right;\">-0.0687312 </td><td style=\"text-align: right;\">-0.0416746</td><td style=\"text-align: right;\"> 0.0276679</td><td style=\"text-align: right;\"> 0.0555427</td><td style=\"text-align: right;\"> 0.0161992</td><td style=\"text-align: right;\">-0.1786   </td><td style=\"text-align: right;\"> 0.0442721</td><td style=\"text-align: right;\">-0.00771938</td><td style=\"text-align: right;\">-0.131468  </td><td style=\"text-align: right;\">-0.0724022</td><td style=\"text-align: right;\">0.116649</td><td style=\"text-align: right;\"> 0.0231421</td><td style=\"text-align: right;\"> 0.0466121 </td><td style=\"text-align: right;\">0.0813585</td><td style=\"text-align: right;\"> 0.0264658 </td><td style=\"text-align: right;\">-0.0433392</td><td style=\"text-align: right;\">-0.0666173</td><td style=\"text-align: right;\">0.142332 </td><td style=\"text-align: right;\"> 0.0232238  </td><td style=\"text-align: right;\">0.0645313</td><td style=\"text-align: right;\">-0.166218 </td><td style=\"text-align: right;\">-0.036077  </td><td style=\"text-align: right;\">-0.151196 </td><td style=\"text-align: right;\">-0.00563044</td><td style=\"text-align: right;\"> 0.00734563</td><td style=\"text-align: right;\">-0.110929 </td><td style=\"text-align: right;\">-0.000840302</td><td style=\"text-align: right;\"> 0.0864319</td><td style=\"text-align: right;\">-0.0096056 </td><td style=\"text-align: right;\"> 0.0318864</td><td style=\"text-align: right;\">0.0852141</td><td style=\"text-align: right;\">0.161717 </td><td style=\"text-align: right;\">-0.119659 </td><td style=\"text-align: right;\">-0.0806639</td><td style=\"text-align: right;\">-0.00661783</td><td style=\"text-align: right;\"> 0.181565 </td><td style=\"text-align: right;\">-0.224121 </td><td style=\"text-align: right;\"> 0.018073 </td><td style=\"text-align: right;\"> 0.219789  </td><td style=\"text-align: right;\"> 0.0170883 </td><td style=\"text-align: right;\"> 0.0974341  </td><td style=\"text-align: right;\">-0.0228969</td><td style=\"text-align: right;\">0.0718762</td><td style=\"text-align: right;\">-0.158417  </td><td style=\"text-align: right;\"> 0.051986 </td><td style=\"text-align: right;\"> 0.010496 </td><td style=\"text-align: right;\"> 0.031218  </td><td style=\"text-align: right;\">-0.075082  </td><td style=\"text-align: right;\">-0.0960302 </td><td style=\"text-align: right;\">-0.0845548 </td><td style=\"text-align: right;\"> 0.0788758</td><td style=\"text-align: right;\"> 0.000478667</td><td style=\"text-align: right;\">0.175059 </td><td style=\"text-align: right;\"> 0.0958483</td><td style=\"text-align: right;\"> 0.0243113 </td><td style=\"text-align: right;\">-0.127263 </td><td style=\"text-align: right;\">-0.0199746</td><td style=\"text-align: right;\">-0.0444245</td><td style=\"text-align: right;\"> 0.0452839 </td><td style=\"text-align: right;\"> 0.0381728 </td><td style=\"text-align: right;\">-0.00664754</td><td style=\"text-align: right;\">-0.141949 </td><td style=\"text-align: right;\">0.0411201</td><td style=\"text-align: right;\">-0.0683749</td><td style=\"text-align: right;\">-0.0159984</td><td style=\"text-align: right;\">-0.130445  </td><td style=\"text-align: right;\"> 0.0980892 </td><td style=\"text-align: right;\"> 0.11186  </td><td style=\"text-align: right;\">0.0769736 </td><td style=\"text-align: right;\">-0.0755395</td><td style=\"text-align: right;\">-0.0548621</td><td style=\"text-align: right;\"> 0.130341  </td><td style=\"text-align: right;\"> 0.226099 </td><td style=\"text-align: right;\"> 0.155404 </td><td style=\"text-align: right;\">-0.0411062</td><td style=\"text-align: right;\"> 0.221949  </td><td style=\"text-align: right;\"> 0.0798396</td><td style=\"text-align: right;\">-0.000585934</td><td style=\"text-align: right;\">-0.0451517 </td><td style=\"text-align: right;\"> 0.107744 </td><td style=\"text-align: right;\">-0.0904976 </td><td style=\"text-align: right;\">0.103362  </td><td style=\"text-align: right;\"> 0.096351 </td><td style=\"text-align: right;\">-0.107486 </td><td style=\"text-align: right;\">-0.0772551 </td><td style=\"text-align: right;\">-0.0136998 </td><td style=\"text-align: right;\">-0.0307121</td><td style=\"text-align: right;\"> 0.0458204 </td><td style=\"text-align: right;\">-0.0781022</td><td style=\"text-align: right;\"> 0.0654209 </td><td style=\"text-align: right;\">-0.0509875 </td><td style=\"text-align: right;\">-0.116846 </td><td style=\"text-align: right;\">-0.0907132 </td><td style=\"text-align: right;\">-0.100661  </td><td style=\"text-align: right;\">-0.00077185</td></tr>\n",
       "<tr><td style=\"text-align: right;\">-0.0152875</td><td style=\"text-align: right;\">-0.0715452</td><td style=\"text-align: right;\">-0.0596505</td><td style=\"text-align: right;\">0.0210277</td><td style=\"text-align: right;\">-0.0280086 </td><td style=\"text-align: right;\">-0.00420187</td><td style=\"text-align: right;\">-0.146765 </td><td style=\"text-align: right;\">-0.012545 </td><td style=\"text-align: right;\"> 0.150861 </td><td style=\"text-align: right;\"> 0.0924331</td><td style=\"text-align: right;\"> 0.0398208</td><td style=\"text-align: right;\">-0.0178212</td><td style=\"text-align: right;\"> 0.0357146 </td><td style=\"text-align: right;\">-0.0820587 </td><td style=\"text-align: right;\">-0.0333361</td><td style=\"text-align: right;\">0.128814</td><td style=\"text-align: right;\"> 0.0120944</td><td style=\"text-align: right;\"> 0.0290463 </td><td style=\"text-align: right;\">0.125865 </td><td style=\"text-align: right;\"> 0.0967999 </td><td style=\"text-align: right;\">-0.0355486</td><td style=\"text-align: right;\">-0.0708388</td><td style=\"text-align: right;\">0.123957 </td><td style=\"text-align: right;\">-0.00348934 </td><td style=\"text-align: right;\">0.0897278</td><td style=\"text-align: right;\">-0.108525 </td><td style=\"text-align: right;\">-0.0914897 </td><td style=\"text-align: right;\">-0.279995 </td><td style=\"text-align: right;\">-0.02007   </td><td style=\"text-align: right;\">-0.0919259 </td><td style=\"text-align: right;\">-0.159563 </td><td style=\"text-align: right;\"> 0.0868672  </td><td style=\"text-align: right;\"> 0.0588285</td><td style=\"text-align: right;\"> 0.0171826 </td><td style=\"text-align: right;\"> 0.0185981</td><td style=\"text-align: right;\">0.186392 </td><td style=\"text-align: right;\">0.0420061</td><td style=\"text-align: right;\">-0.23516  </td><td style=\"text-align: right;\">-0.187293 </td><td style=\"text-align: right;\"> 0.0465837 </td><td style=\"text-align: right;\"> 0.10126  </td><td style=\"text-align: right;\">-0.111872 </td><td style=\"text-align: right;\">-0.144822 </td><td style=\"text-align: right;\"> 0.101581  </td><td style=\"text-align: right;\">-0.117462  </td><td style=\"text-align: right;\"> 0.0796994  </td><td style=\"text-align: right;\"> 0.115214 </td><td style=\"text-align: right;\">0.229584 </td><td style=\"text-align: right;\">-0.0713672 </td><td style=\"text-align: right;\">-0.0390147</td><td style=\"text-align: right;\"> 0.0512523</td><td style=\"text-align: right;\"> 0.00191428</td><td style=\"text-align: right;\"> 0.0536491 </td><td style=\"text-align: right;\">-0.0486795 </td><td style=\"text-align: right;\"> 0.0666075 </td><td style=\"text-align: right;\"> 0.102646 </td><td style=\"text-align: right;\"> 0.00214635 </td><td style=\"text-align: right;\">0.138902 </td><td style=\"text-align: right;\"> 0.0214773</td><td style=\"text-align: right;\"> 0.00136467</td><td style=\"text-align: right;\">-0.061166 </td><td style=\"text-align: right;\"> 0.0963146</td><td style=\"text-align: right;\">-0.0312219</td><td style=\"text-align: right;\">-0.0377728 </td><td style=\"text-align: right;\"> 0.0110977 </td><td style=\"text-align: right;\"> 0.0267467 </td><td style=\"text-align: right;\">-0.0658187</td><td style=\"text-align: right;\">0.0593206</td><td style=\"text-align: right;\">-0.0343376</td><td style=\"text-align: right;\"> 0.135023 </td><td style=\"text-align: right;\">-0.0621164 </td><td style=\"text-align: right;\"> 0.106411  </td><td style=\"text-align: right;\"> 0.0584986</td><td style=\"text-align: right;\">0.046391  </td><td style=\"text-align: right;\"> 0.0460339</td><td style=\"text-align: right;\"> 0.0906557</td><td style=\"text-align: right;\"> 0.00114776</td><td style=\"text-align: right;\"> 0.158475 </td><td style=\"text-align: right;\"> 0.0708439</td><td style=\"text-align: right;\">-0.0360095</td><td style=\"text-align: right;\">-0.0325766 </td><td style=\"text-align: right;\">-0.0717995</td><td style=\"text-align: right;\">-0.0448237  </td><td style=\"text-align: right;\">-0.0939923 </td><td style=\"text-align: right;\">-0.0255299</td><td style=\"text-align: right;\">-0.0232647 </td><td style=\"text-align: right;\">0.0337385 </td><td style=\"text-align: right;\"> 0.104819 </td><td style=\"text-align: right;\">-0.0848314</td><td style=\"text-align: right;\">-0.132427  </td><td style=\"text-align: right;\"> 0.0592799 </td><td style=\"text-align: right;\">-0.0278515</td><td style=\"text-align: right;\">-0.00470951</td><td style=\"text-align: right;\">-0.0997909</td><td style=\"text-align: right;\"> 0.0221211 </td><td style=\"text-align: right;\">-0.114544  </td><td style=\"text-align: right;\">-0.187636 </td><td style=\"text-align: right;\">-0.094655  </td><td style=\"text-align: right;\"> 0.00655682</td><td style=\"text-align: right;\">-0.0753624 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">-0.0444855</td><td style=\"text-align: right;\">-0.136749 </td><td style=\"text-align: right;\">-0.064766 </td><td style=\"text-align: right;\">0.0865771</td><td style=\"text-align: right;\"> 0.0429173 </td><td style=\"text-align: right;\">-0.0376827 </td><td style=\"text-align: right;\">-0.0927733</td><td style=\"text-align: right;\"> 0.141897 </td><td style=\"text-align: right;\"> 0.157546 </td><td style=\"text-align: right;\"> 0.0613846</td><td style=\"text-align: right;\"> 0.063049 </td><td style=\"text-align: right;\"> 0.0612632</td><td style=\"text-align: right;\">-0.0817861 </td><td style=\"text-align: right;\"> 0.035267  </td><td style=\"text-align: right;\"> 0.0314649</td><td style=\"text-align: right;\">0.180172</td><td style=\"text-align: right;\">-0.035238 </td><td style=\"text-align: right;\"> 0.145447  </td><td style=\"text-align: right;\">0.112999 </td><td style=\"text-align: right;\"> 0.0847628 </td><td style=\"text-align: right;\">-0.0511798</td><td style=\"text-align: right;\">-0.169082 </td><td style=\"text-align: right;\">0.0878044</td><td style=\"text-align: right;\"> 0.0226005  </td><td style=\"text-align: right;\">0.130172 </td><td style=\"text-align: right;\">-0.110729 </td><td style=\"text-align: right;\">-0.122074  </td><td style=\"text-align: right;\">-0.0896175</td><td style=\"text-align: right;\"> 0.00558331</td><td style=\"text-align: right;\">-0.0909231 </td><td style=\"text-align: right;\">-0.151759 </td><td style=\"text-align: right;\"> 0.0922034  </td><td style=\"text-align: right;\"> 0.027838 </td><td style=\"text-align: right;\">-0.0651061 </td><td style=\"text-align: right;\">-0.0216926</td><td style=\"text-align: right;\">0.109536 </td><td style=\"text-align: right;\">0.100399 </td><td style=\"text-align: right;\">-0.211377 </td><td style=\"text-align: right;\">-0.122195 </td><td style=\"text-align: right;\"> 0.0359905 </td><td style=\"text-align: right;\"> 0.113204 </td><td style=\"text-align: right;\"> 0.024932 </td><td style=\"text-align: right;\">-0.156284 </td><td style=\"text-align: right;\"> 0.085134  </td><td style=\"text-align: right;\"> 0.0364976 </td><td style=\"text-align: right;\"> 0.0174787  </td><td style=\"text-align: right;\"> 0.0664203</td><td style=\"text-align: right;\">0.150909 </td><td style=\"text-align: right;\">-0.0884863 </td><td style=\"text-align: right;\"> 0.0497732</td><td style=\"text-align: right;\">-0.0815174</td><td style=\"text-align: right;\"> 0.0573616 </td><td style=\"text-align: right;\">-0.0223498 </td><td style=\"text-align: right;\"> 0.00331112</td><td style=\"text-align: right;\"> 0.118039  </td><td style=\"text-align: right;\"> 0.0732774</td><td style=\"text-align: right;\"> 0.0301757  </td><td style=\"text-align: right;\">0.244341 </td><td style=\"text-align: right;\"> 0.066726 </td><td style=\"text-align: right;\">-0.0630507 </td><td style=\"text-align: right;\">-0.0542544</td><td style=\"text-align: right;\"> 0.0753806</td><td style=\"text-align: right;\">-0.0143171</td><td style=\"text-align: right;\">-0.0220478 </td><td style=\"text-align: right;\"> 0.00918858</td><td style=\"text-align: right;\"> 0.00505272</td><td style=\"text-align: right;\">-0.0906953</td><td style=\"text-align: right;\">0.120604 </td><td style=\"text-align: right;\"> 0.024875 </td><td style=\"text-align: right;\"> 0.0801327</td><td style=\"text-align: right;\"> 0.00900708</td><td style=\"text-align: right;\"> 0.106778  </td><td style=\"text-align: right;\"> 0.0905721</td><td style=\"text-align: right;\">0.0544582 </td><td style=\"text-align: right;\"> 0.074747 </td><td style=\"text-align: right;\"> 0.0795821</td><td style=\"text-align: right;\"> 0.00600424</td><td style=\"text-align: right;\"> 0.113041 </td><td style=\"text-align: right;\"> 0.0149877</td><td style=\"text-align: right;\">-0.13618  </td><td style=\"text-align: right;\"> 0.00315993</td><td style=\"text-align: right;\">-0.0886219</td><td style=\"text-align: right;\">-0.0907169  </td><td style=\"text-align: right;\"> 0.029071  </td><td style=\"text-align: right;\">-0.0397595</td><td style=\"text-align: right;\">-0.0318108 </td><td style=\"text-align: right;\">0.139571  </td><td style=\"text-align: right;\"> 0.137431 </td><td style=\"text-align: right;\">-0.0147875</td><td style=\"text-align: right;\"> 0.0184729 </td><td style=\"text-align: right;\"> 0.0470235 </td><td style=\"text-align: right;\">-0.0800962</td><td style=\"text-align: right;\"> 0.104865  </td><td style=\"text-align: right;\">-0.0679469</td><td style=\"text-align: right;\"> 0.062309  </td><td style=\"text-align: right;\">-0.204208  </td><td style=\"text-align: right;\">-0.0638533</td><td style=\"text-align: right;\">-0.00313411</td><td style=\"text-align: right;\"> 0.0227715 </td><td style=\"text-align: right;\">-0.0697703 </td></tr>\n",
       "<tr><td style=\"text-align: right;\">-0.151232 </td><td style=\"text-align: right;\">-0.128398 </td><td style=\"text-align: right;\">-0.0458661</td><td style=\"text-align: right;\">0.209259 </td><td style=\"text-align: right;\"> 0.121817  </td><td style=\"text-align: right;\"> 0.0958705 </td><td style=\"text-align: right;\">-0.552895 </td><td style=\"text-align: right;\"> 0.167645 </td><td style=\"text-align: right;\"> 0.0418051</td><td style=\"text-align: right;\"> 0.121377 </td><td style=\"text-align: right;\"> 0.0955454</td><td style=\"text-align: right;\">-0.211727 </td><td style=\"text-align: right;\">-0.0103224 </td><td style=\"text-align: right;\"> 0.0164757 </td><td style=\"text-align: right;\">-0.0475631</td><td style=\"text-align: right;\">0.262533</td><td style=\"text-align: right;\">-0.156327 </td><td style=\"text-align: right;\"> 0.377485  </td><td style=\"text-align: right;\">0.246717 </td><td style=\"text-align: right;\">-0.0135573 </td><td style=\"text-align: right;\">-0.177406 </td><td style=\"text-align: right;\">-0.183147 </td><td style=\"text-align: right;\">0.180468 </td><td style=\"text-align: right;\">-0.000253381</td><td style=\"text-align: right;\">0.265976 </td><td style=\"text-align: right;\">-0.160893 </td><td style=\"text-align: right;\">-0.200018  </td><td style=\"text-align: right;\">-0.261309 </td><td style=\"text-align: right;\"> 0.0500338 </td><td style=\"text-align: right;\">-0.0399509 </td><td style=\"text-align: right;\">-0.0765872</td><td style=\"text-align: right;\">-0.0506921  </td><td style=\"text-align: right;\"> 0.169115 </td><td style=\"text-align: right;\"> 0.00725004</td><td style=\"text-align: right;\">-0.297368 </td><td style=\"text-align: right;\">0.301351 </td><td style=\"text-align: right;\">0.299986 </td><td style=\"text-align: right;\">-0.455418 </td><td style=\"text-align: right;\">-0.179032 </td><td style=\"text-align: right;\">-0.0106398 </td><td style=\"text-align: right;\">-0.0170218</td><td style=\"text-align: right;\"> 0.0471121</td><td style=\"text-align: right;\">-0.212905 </td><td style=\"text-align: right;\">-0.0643439 </td><td style=\"text-align: right;\">-0.220091  </td><td style=\"text-align: right;\">-0.0173816  </td><td style=\"text-align: right;\"> 0.156557 </td><td style=\"text-align: right;\">0.343897 </td><td style=\"text-align: right;\"> 0.114946  </td><td style=\"text-align: right;\">-0.138344 </td><td style=\"text-align: right;\">-0.237115 </td><td style=\"text-align: right;\">-0.0706474 </td><td style=\"text-align: right;\">-0.00870185</td><td style=\"text-align: right;\"> 0.00491258</td><td style=\"text-align: right;\">-0.00255729</td><td style=\"text-align: right;\">-0.0589396</td><td style=\"text-align: right;\"> 0.207085   </td><td style=\"text-align: right;\">0.0937893</td><td style=\"text-align: right;\"> 0.104862 </td><td style=\"text-align: right;\">-0.105031  </td><td style=\"text-align: right;\">-0.0036231</td><td style=\"text-align: right;\">-0.0776935</td><td style=\"text-align: right;\"> 0.259848 </td><td style=\"text-align: right;\"> 0.217472  </td><td style=\"text-align: right;\">-0.194927  </td><td style=\"text-align: right;\"> 0.0453228 </td><td style=\"text-align: right;\">-0.13975  </td><td style=\"text-align: right;\">0.115349 </td><td style=\"text-align: right;\"> 0.133061 </td><td style=\"text-align: right;\">-0.05156  </td><td style=\"text-align: right;\"> 0.247622  </td><td style=\"text-align: right;\">-0.159117  </td><td style=\"text-align: right;\">-0.0349687</td><td style=\"text-align: right;\">0.00249731</td><td style=\"text-align: right;\">-0.134824 </td><td style=\"text-align: right;\"> 0.100211 </td><td style=\"text-align: right;\"> 0.0741915 </td><td style=\"text-align: right;\">-0.0620691</td><td style=\"text-align: right;\"> 0.182817 </td><td style=\"text-align: right;\"> 0.101812 </td><td style=\"text-align: right;\"> 0.136909  </td><td style=\"text-align: right;\">-0.0102396</td><td style=\"text-align: right;\">-0.23326    </td><td style=\"text-align: right;\"> 0.00444215</td><td style=\"text-align: right;\"> 0.153172 </td><td style=\"text-align: right;\"> 0.0549877 </td><td style=\"text-align: right;\">0.114905  </td><td style=\"text-align: right;\"> 0.0954119</td><td style=\"text-align: right;\">-0.103502 </td><td style=\"text-align: right;\">-0.0435195 </td><td style=\"text-align: right;\">-0.245606  </td><td style=\"text-align: right;\">-0.107886 </td><td style=\"text-align: right;\">-0.0321344 </td><td style=\"text-align: right;\"> 0.022238 </td><td style=\"text-align: right;\">-0.0756358 </td><td style=\"text-align: right;\"> 0.0391836 </td><td style=\"text-align: right;\">-0.155411 </td><td style=\"text-align: right;\"> 0.079341  </td><td style=\"text-align: right;\"> 0.330519  </td><td style=\"text-align: right;\">-0.215143  </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = reviews['Text'][9,:]\n",
    "row.set_names(['Review'])\n",
    "print(row)\n",
    "w2v_model.transform(words, aggregate_method=\"AVERAGE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>ProductId  </th><th>UserId        </th><th>Summary                               </th><th style=\"text-align: right;\">  Score</th><th style=\"text-align: right;\">  HelpfulnessDenominator</th><th style=\"text-align: right;\">    Id</th><th>ProfileName                                     </th><th style=\"text-align: right;\">  HelpfulnessNumerator</th><th style=\"text-align: right;\">       Time</th><th>Text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </th><th style=\"text-align: right;\">  PositiveReview</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>B00141QYSQ </td><td>A1YS02UZZGRDCT</td><td>Do Not Buy                            </td><td style=\"text-align: right;\">      1</td><td style=\"text-align: right;\">                       2</td><td style=\"text-align: right;\"> 41471</td><td>Evan Eberhardt                                  </td><td style=\"text-align: right;\">                     2</td><td style=\"text-align: right;\">1.34836e+09</td><td>These are made in China (do not buy ANY pet food from China).  Dogswell has been using propylene glycol to soften their treats (what are they thinkng?).  Do not purchase or support this company in any way until they clean up their act.  And for whatever reason Amazon doesn&#x27;t allow returns of this item, so I had to toss mine out.  Bad business all around on this one.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            </td><td style=\"text-align: right;\">               0</td></tr>\n",
       "<tr><td>B0089SPEO2 </td><td>A3JOYNYL458QHP</td><td>Less lemon and less zing              </td><td style=\"text-align: right;\">      3</td><td style=\"text-align: right;\">                       0</td><td style=\"text-align: right;\"> 28582</td><td>coleridge                                       </td><td style=\"text-align: right;\">                     0</td><td style=\"text-align: right;\">1.32391e+09</td><td>Everything is ok, except it just isn&#x27;t as good as it is in the bags.  Just considerably more bland -- less lemon and less zing.  Boring.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </td><td style=\"text-align: right;\">               0</td></tr>\n",
       "<tr><td>B001PMCDK2 </td><td>A14TTMM0Z03Y2W</td><td>my cat goes crazy for these!          </td><td style=\"text-align: right;\">      5</td><td style=\"text-align: right;\">                       0</td><td style=\"text-align: right;\">389965</td><td>Lindsay S. Bradford                             </td><td style=\"text-align: right;\">                     0</td><td style=\"text-align: right;\">1.3106e+09 </td><td>Best cat treat ever. There isn&#x27;t anything comparable to the love my cat has for these treats, he snubs away any other kind now.&lt;br /&gt;I know he likes to manipulate me with his cattiness but these treats are my way of manipulating him to come sit on my lap and have some chill time. :)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 </td><td style=\"text-align: right;\">               1</td></tr>\n",
       "<tr><td>B002Q8JOSI </td><td>A17UQD2RSSQH5X</td><td>My dogs tell me these treats are YUMMY</td><td style=\"text-align: right;\">      5</td><td style=\"text-align: right;\">                       1</td><td style=\"text-align: right;\">212536</td><td>in the dark                                     </td><td style=\"text-align: right;\">                     1</td><td style=\"text-align: right;\">1.31613e+09</td><td>My two Corgis were thoroughly spoiled by my late husband (I spent a year and a half dieting them down a combined total of 25 pounds!)&lt;br /&gt;&lt;br /&gt;They are accustomed to the finest of fare, and they absolutely love the Wellness brand of treats.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          </td><td style=\"text-align: right;\">               1</td></tr>\n",
       "<tr><td>B00176G870 </td><td>A2F2MZW8EOGH5J</td><td>Yummy to the tummy                    </td><td style=\"text-align: right;\">      5</td><td style=\"text-align: right;\">                       0</td><td style=\"text-align: right;\">115971</td><td>daemoncycler &quot;When you arrive at a fork in th...</td><td style=\"text-align: right;\">                     0</td><td style=\"text-align: right;\">1.33479e+09</td><td>We used to have drive down to the specialty pet food store for this product.  So glad we discovered Amazon.  As far as I can tell it is no more expensive &amp; in some cases less - Prime membership is awesome.  Loving Pets treats are some of the best according to my dog.  They do not develop that nasty smell like some dog treats do.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  </td><td style=\"text-align: right;\">               1</td></tr>\n",
       "<tr><td>B001CHFUGY </td><td>A2M8VROSDPU4JT</td><td>Very good coffee                      </td><td style=\"text-align: right;\">      5</td><td style=\"text-align: right;\">                       1</td><td style=\"text-align: right;\">434484</td><td>Officefan &quot;Officefankt&quot;                         </td><td style=\"text-align: right;\">                     1</td><td style=\"text-align: right;\">1.27725e+09</td><td>I really liked this coffee, it was just as good as everyone claimed it was.  Strong, bold and flavorful!  I would recommend!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                </td><td style=\"text-align: right;\">               1</td></tr>\n",
       "<tr><td>B0041CIR62 </td><td>A16I6WJUEBJ1C3</td><td>okay but not as healthy as it appears </td><td style=\"text-align: right;\">      2</td><td style=\"text-align: right;\">                       1</td><td style=\"text-align: right;\">138997</td><td>doctorsirena &quot;doctorsirena&quot;                     </td><td style=\"text-align: right;\">                     1</td><td style=\"text-align: right;\">1.34369e+09</td><td>I am always looking for healthier, whole grain versions of foods I enjoy.  Unfortunately, these Peacock brand noodles are yet another food masquerading as healthy.  The product title in big letters on the front says &quot;Brown Rice Vermicelli&quot;, making the consumer think &quot;this is made with brown rice, so it should be a healthy choice&quot;.  But the first indication that it is not is when looking at the fiber content on the nutrition facts - only 0.6g per 2oz serving.  Then onto the ingredients list to see why so low... contains brown rice, sago starch and water.  The sago starch comes from palms and must not have much (if any) fiber.&lt;br /&gt;&lt;br /&gt;The Annie Chun&#x27;s Maifun Brown Rice Noodles (sold on Amazon and in my local healthy grocer) has become one of my staples and is my frame of reference when comparing to the Peacock brand.  The Annie Chun&#x27;s product is made with 100% whole grain, with ingredients brown rice flour and water.  Per 2oz serving, it has 4g fiber and pretty much the same calories and other nutrients as the Peacock brand.&lt;br /&gt;&lt;br /&gt;If you do try this Peacock brand noodles and have not used rice noodles before, you will need to seek guidance elsewhere on preparation.  As others have pointed out, the Peacock package gives almost no directions on how to prepare the product, aside from a brief mention in the recipes (in the header text it does say that they are &quot;easy-to-cook&quot; but does not say how).  It also contains a very strange recipe for rice noodles: Aglio Olio style - this is an Italian recipe for noodles with olive oil/garlic/sprinkled with grated cheese that I think would not be very tasty.  The second recipe appears to be for a soup with veggie strips.  Neither recipe gives amounts or much direction.  In comparison, the Annie Chun&#x27;s package gives clear, specific directions on rice noodle preparation and two recipes.&lt;br /&gt;&lt;br /&gt;I use rice noodles = maifun = rice sticks = sometimes called vermicelli for making the Vietnamese salad &quot;bun tofu&quot;, to serve with stir-fried veggies or in lettuce rolls.  They can also be used in spring rolls/egg rolls.  When cooking with thin rice noodles, be careful not to oversoak/overcook/overmix or they tend to disintegrate.  Asian rice noodle vermicelli (maifun) are not the same as Italian vermicelli and are not readily interchangeable.  If making an Italian recipe, the best results would be expected from Italian pasta and not maifun.&lt;br /&gt;&lt;br /&gt;A few final notes...  Both Peacock and Annie Chun&#x27;s brown rice noodles are gluten free.  The Peacock is made in Singapore and the Annie Chun&#x27;s in Thailand.  The Peacock noodles do taste fine (kind of bland), but so do the Annie Chun&#x27;s.  At this time, they are both approximately the same price.  Peacock come in an plastic bag with some noodle crushage upon shipping; Annie Chun&#x27;s are perfect upon removal from their cellophane bag in a box.  Overall, I highly recommend the Annie Chun&#x27;s Maifun as a healthier option over the Peacock brand.  On a related note, the Annie Chun&#x27;s soba and brown rice pad thai noodles are also excellent.&lt;br /&gt;&lt;br /&gt;Rating for this product: 2.5 stars rounded down to 2 stars.</td><td style=\"text-align: right;\">               0</td></tr>\n",
       "<tr><td>B001R3BQFW </td><td>AM50E42AFUVNL </td><td>Taste great.                          </td><td style=\"text-align: right;\">      5</td><td style=\"text-align: right;\">                       0</td><td style=\"text-align: right;\">126555</td><td>T. Higley &quot;Tina&quot;                                </td><td style=\"text-align: right;\">                     0</td><td style=\"text-align: right;\">1.32356e+09</td><td>I have tried many different drink mix, this is the best tasting by far. It does not have the after taste of the sweetener and I really like it, it is pretty strong, so I use a big water bottle (20 oz) for one tube, it still a little stronger than I like, but it is just my taste.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     </td><td style=\"text-align: right;\">               1</td></tr>\n",
       "<tr><td>B005HGAV8I </td><td>A2I5KDNOESGJ1H</td><td>variety galore                        </td><td style=\"text-align: right;\">      5</td><td style=\"text-align: right;\">                       1</td><td style=\"text-align: right;\">438837</td><td>TJ                                              </td><td style=\"text-align: right;\">                     1</td><td style=\"text-align: right;\">1.33402e+09</td><td>This is my favorite item to order for my Keurig. There are so many flavors, my finicky palate never gets bored!  The only downside is there are probably 5-6 decaf varieties.  I don&#x27;t drink decaf (I REQUIRE copious amounts of caffeine), so they sit on the shelf...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     </td><td style=\"text-align: right;\">               1</td></tr>\n",
       "<tr><td>B000GFYRHQ </td><td>A3A7YUR6FS6ZCI</td><td>Bigelow Earl Grey Green Tea           </td><td style=\"text-align: right;\">      5</td><td style=\"text-align: right;\">                       0</td><td style=\"text-align: right;\">245379</td><td>Tea Lover                                       </td><td style=\"text-align: right;\">                     0</td><td style=\"text-align: right;\">1.17841e+09</td><td>Tastes like Earl Grey, but it&#x27;s green tea so it&#x27;s healthier.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                </td><td style=\"text-align: right;\">               1</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>C1        </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>everything</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[32,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th style=\"text-align: right;\">  C1</th><th style=\"text-align: right;\">  C2</th><th style=\"text-align: right;\">  C3</th><th style=\"text-align: right;\">  C4</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td style=\"text-align: right;\">   2</td><td style=\"text-align: right;\">   4</td><td style=\"text-align: right;\">   2</td><td style=\"text-align: right;\">   2</td></tr>\n",
       "<tr><td style=\"text-align: right;\">   2</td><td style=\"text-align: right;\">   2</td><td style=\"text-align: right;\">   2</td><td style=\"text-align: right;\">   2</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from h2o.assembly import *\n",
    "from h2o.transforms.preprocessing import *\n",
    "\n",
    "python_list1 = [[4,4,4,4],[4,4,4,4]]\n",
    "python_list2 = [[2,1,2,2], [2,2,2,2]]\n",
    "\n",
    "frame1 = h2o.H2OFrame(python_obj=python_list1)\n",
    "frame2 = h2o.H2OFrame(python_obj=python_list2)\n",
    "\n",
    "# assembly = H2OAssembly(steps=[]\n",
    "\n",
    "H2OAssembly.divide(frame1, frame2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class H2OWord2vecEstimator in module h2o.estimators.word2vec:\n",
      "\n",
      "class H2OWord2vecEstimator(h2o.estimators.estimator_base.H2OEstimator)\n",
      " |  H2OWord2vecEstimator(**kwargs)\n",
      " |  \n",
      " |  Word2Vec\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      H2OWord2vecEstimator\n",
      " |      h2o.estimators.estimator_base.H2OEstimator\n",
      " |      h2o.model.model_base.ModelBase\n",
      " |      h2o.model.model_base.ModelBase\n",
      " |      h2o.base.Keyed\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, **kwargs)\n",
      " |      Construct a new model instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  from_external(external=<class 'h2o.frame.H2OFrame'>)\n",
      " |      Creates new H2OWord2vecEstimator based on an external model.\n",
      " |      \n",
      " |      :param external: H2OFrame with an external model\n",
      " |      :return: H2OWord2vecEstimator instance representing the external model\n",
      " |      \n",
      " |      :examples:\n",
      " |      \n",
      " |      >>> words = h2o.create_frame(rows=10, cols=1,\n",
      " |      ...                          string_fraction=1.0,\n",
      " |      ...                          missing_fraction=0.0)\n",
      " |      >>> embeddings = h2o.create_frame(rows=10, cols=100,\n",
      " |      ...                               real_fraction=1.0,\n",
      " |      ...                               missing_fraction=0.0)\n",
      " |      >>> word_embeddings = words.cbind(embeddings)\n",
      " |      >>> w2v_model = H2OWord2vecEstimator.from_external(external=word_embeddings)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  epochs\n",
      " |      Number of training iterations to run\n",
      " |      \n",
      " |      Type: ``int``  (default: ``5``).\n",
      " |      \n",
      " |      :examples:\n",
      " |      \n",
      " |      >>> job_titles = h2o.import_file((\"https://s3.amazonaws.com/h2o-public-test-data/smalldata/craigslistJobTitles.csv\"), \n",
      " |      ...                               col_names = [\"category\", \"jobtitle\"], \n",
      " |      ...                               col_types = [\"string\", \"string\"], \n",
      " |      ...                               header = 1)\n",
      " |      >>> words = job_titles.tokenize(\" \")\n",
      " |      >>> w2v_model = H2OWord2vecEstimator(sent_sample_rate = 0.0, epochs = 10)\n",
      " |      >>> w2v_model.train(training_frame=words)\n",
      " |      >>> synonyms = w2v_model.find_synonyms(\"teacher\", count = 5)\n",
      " |      >>> print(synonyms)\n",
      " |      >>>\n",
      " |      >>> w2v_model2 = H2OWord2vecEstimator(sent_sample_rate = 0.0, epochs = 1)\n",
      " |      >>> w2v_model2.train(training_frame=words)\n",
      " |      >>> synonyms2 = w2v_model2.find_synonyms(\"teacher\", 3)\n",
      " |      >>> print(synonyms2)\n",
      " |  \n",
      " |  export_checkpoints_dir\n",
      " |      Automatically export generated models to this directory.\n",
      " |      \n",
      " |      Type: ``str``.\n",
      " |      \n",
      " |      :examples:\n",
      " |      \n",
      " |      >>> import tempfile\n",
      " |      >>> from os import listdir\n",
      " |      >>> job_titles = h2o.import_file((\"https://s3.amazonaws.com/h2o-public-test-data/smalldata/craigslistJobTitles.csv\"), \n",
      " |      ...                               col_names = [\"category\", \"jobtitle\"], \n",
      " |      ...                               col_types = [\"string\", \"string\"], \n",
      " |      ...                               header = 1)\n",
      " |      >>> checkpoints_dir = tempfile.mkdtemp()\n",
      " |      >>> words = job_titles.tokenize(\" \")\n",
      " |      >>> w2v_model = H2OWord2vecEstimator(epochs=1,\n",
      " |      ...                                  max_runtime_secs=10,\n",
      " |      ...                                  export_checkpoints_dir=checkpoints_dir)\n",
      " |      >>> w2v_model.train(training_frame=words)\n",
      " |      >>> len(listdir(checkpoints_dir))\n",
      " |  \n",
      " |  init_learning_rate\n",
      " |      Set the starting learning rate\n",
      " |      \n",
      " |      Type: ``float``  (default: ``0.025``).\n",
      " |      \n",
      " |      :examples:\n",
      " |      \n",
      " |      >>> job_titles = h2o.import_file((\"https://s3.amazonaws.com/h2o-public-test-data/smalldata/craigslistJobTitles.csv\"), \n",
      " |      ...                               col_names = [\"category\", \"jobtitle\"], \n",
      " |      ...                               col_types = [\"string\", \"string\"], \n",
      " |      ...                               header = 1)\n",
      " |      >>> words = job_titles.tokenize(\" \")\n",
      " |      >>> w2v_model = H2OWord2vecEstimator(epochs=3, init_learning_rate=0.05)\n",
      " |      >>> w2v_model.train(training_frame=words)\n",
      " |      >>> synonyms = w2v_model.find_synonyms(\"assistant\", 3)\n",
      " |      >>> print(synonyms)\n",
      " |  \n",
      " |  max_runtime_secs\n",
      " |      Maximum allowed runtime in seconds for model training. Use 0 to disable.\n",
      " |      \n",
      " |      Type: ``float``  (default: ``0``).\n",
      " |      \n",
      " |      :examples:\n",
      " |      \n",
      " |      >>> job_titles = h2o.import_file((\"https://s3.amazonaws.com/h2o-public-test-data/smalldata/craigslistJobTitles.csv\"), \n",
      " |      ...                               col_names = [\"category\", \"jobtitle\"], \n",
      " |      ...                               col_types = [\"string\", \"string\"], \n",
      " |      ...                               header = 1)\n",
      " |      >>> words = job_titles.tokenize(\" \")\n",
      " |      >>> w2v_model = H2OWord2vecEstimator(epochs=1, max_runtime_secs=10)\n",
      " |      >>> w2v_model.train(training_frame=words)\n",
      " |      >>> synonyms = w2v_model.find_synonyms(\"tutor\", 3)\n",
      " |      >>> print(synonyms)\n",
      " |  \n",
      " |  min_word_freq\n",
      " |      This will discard words that appear less than <int> times\n",
      " |      \n",
      " |      Type: ``int``  (default: ``5``).\n",
      " |      \n",
      " |      :examples:\n",
      " |      \n",
      " |      >>> job_titles = h2o.import_file((\"https://s3.amazonaws.com/h2o-public-test-data/smalldata/craigslistJobTitles.csv\"), \n",
      " |      ...                               col_names = [\"category\", \"jobtitle\"], \n",
      " |      ...                               col_types = [\"string\", \"string\"], \n",
      " |      ...                               header = 1)\n",
      " |      >>> words = job_titles.tokenize(\" \")\n",
      " |      >>> w2v_model = H2OWord2vecEstimator(epochs=1, min_word_freq=4)\n",
      " |      >>> w2v_model.train(training_frame=words)\n",
      " |      >>> synonyms = w2v_model.find_synonyms(\"teacher\", 3)\n",
      " |      >>> print(synonyms)\n",
      " |  \n",
      " |  norm_model\n",
      " |      Use Hierarchical Softmax\n",
      " |      \n",
      " |      One of: ``\"hsm\"``  (default: ``\"hsm\"``).\n",
      " |      \n",
      " |      :examples:\n",
      " |      \n",
      " |      >>> job_titles = h2o.import_file((\"https://s3.amazonaws.com/h2o-public-test-data/smalldata/craigslistJobTitles.csv\"), \n",
      " |      ...                               col_names = [\"category\", \"jobtitle\"], \n",
      " |      ...                               col_types = [\"string\", \"string\"], \n",
      " |      ...                               header = 1)\n",
      " |      >>> words = job_titles.tokenize(\" \")\n",
      " |      >>> w2v_model = H2OWord2vecEstimator(epochs=1, norm_model=\"hsm\")\n",
      " |      >>> w2v_model.train(training_frame=words)\n",
      " |      >>> synonyms = w2v_model.find_synonyms(\"teacher\", 3)\n",
      " |      >>> print(synonyms)\n",
      " |  \n",
      " |  pre_trained\n",
      " |      Id of a data frame that contains a pre-trained (external) word2vec model\n",
      " |      \n",
      " |      Type: ``H2OFrame``.\n",
      " |      \n",
      " |      :examples:\n",
      " |      \n",
      " |      >>> words = h2o.create_frame(rows=1000,cols=1,\n",
      " |      ...                          string_fraction=1.0,\n",
      " |      ...                          missing_fraction=0.0)\n",
      " |      >>> embeddings = h2o.create_frame(rows=1000,cols=100,\n",
      " |      ...                               real_fraction=1.0,\n",
      " |      ...                               missing_fraction=0.0)\n",
      " |      >>> word_embeddings = words.cbind(embeddings)\n",
      " |      >>> w2v_model = H2OWord2vecEstimator(pre_trained=word_embeddings)\n",
      " |      >>> w2v_model.train(training_frame=word_embeddings)\n",
      " |      >>> model_id = w2v_model.model_id\n",
      " |      >>> model = h2o.get_model(model_id)\n",
      " |  \n",
      " |  sent_sample_rate\n",
      " |      Set threshold for occurrence of words. Those that appear with higher frequency in the training data\n",
      " |      will be randomly down-sampled; useful range is (0, 1e-5)\n",
      " |      \n",
      " |      Type: ``float``  (default: ``0.001``).\n",
      " |      \n",
      " |      :examples:\n",
      " |      \n",
      " |      >>> job_titles = h2o.import_file((\"https://s3.amazonaws.com/h2o-public-test-data/smalldata/craigslistJobTitles.csv\"), \n",
      " |      ...                               col_names = [\"category\", \"jobtitle\"], \n",
      " |      ...                               col_types = [\"string\", \"string\"], \n",
      " |      ...                               header = 1)\n",
      " |      >>> words = job_titles.tokenize(\" \")\n",
      " |      >>> w2v_model = H2OWord2vecEstimator(epochs=1, sent_sample_rate=0.01)\n",
      " |      >>> w2v_model.train(training_frame=words)\n",
      " |      >>> synonyms = w2v_model.find_synonyms(\"teacher\", 3)\n",
      " |      >>> print(synonyms)\n",
      " |  \n",
      " |  training_frame\n",
      " |      Id of the training data frame.\n",
      " |      \n",
      " |      Type: ``H2OFrame``.\n",
      " |      \n",
      " |      :examples:\n",
      " |      \n",
      " |      >>> job_titles = h2o.import_file((\"https://s3.amazonaws.com/h2o-public-test-data/smalldata/craigslistJobTitles.csv\"), \n",
      " |      ...                               col_names = [\"category\", \"jobtitle\"], \n",
      " |      ...                               col_types = [\"string\", \"string\"], \n",
      " |      ...                               header = 1)\n",
      " |      >>> words = job_titles.tokenize(\" \")\n",
      " |      >>> w2v_model = H2OWord2vecEstimator()\n",
      " |      >>> w2v_model.train(training_frame=words)\n",
      " |      >>> synonyms = w2v_model.find_synonyms(\"tutor\", 3)\n",
      " |      >>> print(synonyms)\n",
      " |  \n",
      " |  vec_size\n",
      " |      Set size of word vectors\n",
      " |      \n",
      " |      Type: ``int``  (default: ``100``).\n",
      " |      \n",
      " |      :examples:\n",
      " |      \n",
      " |      >>> job_titles = h2o.import_file((\"https://s3.amazonaws.com/h2o-public-test-data/smalldata/craigslistJobTitles.csv\"), \n",
      " |      ...                               col_names = [\"category\", \"jobtitle\"], \n",
      " |      ...                               col_types = [\"string\", \"string\"], \n",
      " |      ...                               header = 1)\n",
      " |      >>> words = job_titles.tokenize(\" \")\n",
      " |      >>> w2v_model = H2OWord2vecEstimator(epochs=3, vec_size=50)\n",
      " |      >>> w2v_model.train(training_frame=words)\n",
      " |      >>> synonyms = w2v_model.find_synonyms(\"tutor\", 3)\n",
      " |      >>> print(synonyms)\n",
      " |  \n",
      " |  window_size\n",
      " |      Set max skip length between words\n",
      " |      \n",
      " |      Type: ``int``  (default: ``5``).\n",
      " |      \n",
      " |      :examples:\n",
      " |      \n",
      " |      >>> job_titles = h2o.import_file((\"https://s3.amazonaws.com/h2o-public-test-data/smalldata/craigslistJobTitles.csv\"), \n",
      " |      ...                               col_names = [\"category\", \"jobtitle\"], \n",
      " |      ...                               col_types = [\"string\", \"string\"], \n",
      " |      ...                               header = 1)\n",
      " |      >>> words = job_titles.tokenize(\" \")\n",
      " |      >>> w2v_model = H2OWord2vecEstimator(epochs=3, window_size=2)\n",
      " |      >>> w2v_model.train(training_frame=words)\n",
      " |      >>> synonyms = w2v_model.find_synonyms(\"teacher\", 3)\n",
      " |      >>> print(synonyms)\n",
      " |  \n",
      " |  word_model\n",
      " |      Use the Skip-Gram model\n",
      " |      \n",
      " |      One of: ``\"skip_gram\"``  (default: ``\"skip_gram\"``).\n",
      " |      \n",
      " |      :examples:\n",
      " |      \n",
      " |      >>> job_titles = h2o.import_file((\"https://s3.amazonaws.com/h2o-public-test-data/smalldata/craigslistJobTitles.csv\"), \n",
      " |      ...                               col_names = [\"category\", \"jobtitle\"], \n",
      " |      ...                               col_types = [\"string\", \"string\"], \n",
      " |      ...                               header = 1)\n",
      " |      >>> words = job_titles.tokenize(\" \")\n",
      " |      >>> w2v_model = H2OWord2vecEstimator(epochs=3, word_model=\"skip_gram\")\n",
      " |      >>> w2v_model.train(training_frame=words)\n",
      " |      >>> synonyms = w2v_model.find_synonyms(\"assistant\", 3)\n",
      " |      >>> print(synonyms)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  algo = 'word2vec'\n",
      " |  \n",
      " |  param_names = {'epochs', 'export_checkpoints_dir', 'init_learning_rate...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from h2o.estimators.estimator_base.H2OEstimator:\n",
      " |  \n",
      " |  convert_H2OXGBoostParams_2_XGBoostParams(self)\n",
      " |      In order to use convert_H2OXGBoostParams_2_XGBoostParams and convert_H2OFrame_2_DMatrix, you must import\n",
      " |      the following toolboxes: xgboost, pandas, numpy and scipy.sparse.\n",
      " |      \n",
      " |      Given an H2OXGBoost model, this method will generate the corresponding parameters that should be used by\n",
      " |      native XGBoost in order to give exactly the same result, assuming that the same dataset\n",
      " |      (derived from h2oFrame) is used to train the native XGBoost model.\n",
      " |      \n",
      " |      Follow the steps below to compare H2OXGBoost and native XGBoost:\n",
      " |      \n",
      " |       1. Train the H2OXGBoost model with H2OFrame trainFile and generate a prediction:\n",
      " |      \n",
      " |        - h2oModelD = H2OXGBoostEstimator(\\*\\*h2oParamsD) # parameters specified as a dict()\n",
      " |        - h2oModelD.train(x=myX, y=y, training_frame=trainFile) # train with H2OFrame trainFile\n",
      " |        - h2oPredict = h2oPredictD = h2oModelD.predict(trainFile)\n",
      " |      \n",
      " |       2. Derive the DMatrix from H2OFrame:\n",
      " |       \n",
      " |        - nativeDMatrix = trainFile.convert_H2OFrame_2_DMatrix(myX, y, h2oModelD)\n",
      " |      \n",
      " |       3. Derive the parameters for native XGBoost:\n",
      " |       \n",
      " |        - nativeParams = h2oModelD.convert_H2OXGBoostParams_2_XGBoostParams()\n",
      " |      \n",
      " |       4. Train your native XGBoost model and generate a prediction:\n",
      " |       \n",
      " |        - nativeModel = xgb.train(params=nativeParams[0], dtrain=nativeDMatrix, num_boost_round=nativeParams[1])\n",
      " |        - nativePredict = nativeModel.predict(data=nativeDMatrix, ntree_limit=nativeParams[1]\n",
      " |      \n",
      " |       5. Compare the predictions h2oPredict from H2OXGBoost, nativePredict from native XGBoost.\n",
      " |      \n",
      " |      :return: nativeParams, num_boost_round\n",
      " |  \n",
      " |  fit(self, X, y=None, **params)\n",
      " |      Fit an H2O model as part of a scikit-learn pipeline or grid search.\n",
      " |      \n",
      " |      A warning will be issued if a caller other than sklearn attempts to use this method.\n",
      " |      \n",
      " |      :param H2OFrame X: An H2OFrame consisting of the predictor variables.\n",
      " |      :param H2OFrame y: An H2OFrame consisting of the response variable.\n",
      " |      :param params: Extra arguments.\n",
      " |      :returns: The current instance of H2OEstimator for method chaining.\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Obtain parameters for this estimator.\n",
      " |      \n",
      " |      Used primarily for sklearn Pipelines and sklearn grid search.\n",
      " |      \n",
      " |      :param deep: If True, return parameters of all sub-objects that are estimators.\n",
      " |      \n",
      " |      :returns: A dict of parameters\n",
      " |  \n",
      " |  join(self)\n",
      " |      Wait until job's completion.\n",
      " |  \n",
      " |  set_params(self, **parms)\n",
      " |      Used by sklearn for updating parameters during grid search.\n",
      " |      \n",
      " |      :param parms: A dictionary of parameters that will be set on this model.\n",
      " |      :returns: self, the current estimator object with the parameters all set as desired.\n",
      " |  \n",
      " |  start(self, x, y=None, training_frame=None, offset_column=None, fold_column=None, weights_column=None, validation_frame=None, **params)\n",
      " |      Train the model asynchronously (to block for results call :meth:`join`).\n",
      " |      \n",
      " |      :param x: A list of column names or indices indicating the predictor columns.\n",
      " |      :param y: An index or a column name indicating the response column.\n",
      " |      :param H2OFrame training_frame: The H2OFrame having the columns indicated by x and y (as well as any\n",
      " |          additional columns specified by fold, offset, and weights).\n",
      " |      :param offset_column: The name or index of the column in training_frame that holds the offsets.\n",
      " |      :param fold_column: The name or index of the column in training_frame that holds the per-row fold\n",
      " |          assignments.\n",
      " |      :param weights_column: The name or index of the column in training_frame that holds the per-row weights.\n",
      " |      :param validation_frame: H2OFrame with validation data to be scored on while training.\n",
      " |  \n",
      " |  train(self, x=None, y=None, training_frame=None, offset_column=None, fold_column=None, weights_column=None, validation_frame=None, max_runtime_secs=None, ignored_columns=None, model_id=None, verbose=False)\n",
      " |      Train the H2O model.\n",
      " |      \n",
      " |      :param x: A list of column names or indices indicating the predictor columns.\n",
      " |      :param y: An index or a column name indicating the response column.\n",
      " |      :param H2OFrame training_frame: The H2OFrame having the columns indicated by x and y (as well as any\n",
      " |          additional columns specified by fold, offset, and weights).\n",
      " |      :param offset_column: The name or index of the column in training_frame that holds the offsets.\n",
      " |      :param fold_column: The name or index of the column in training_frame that holds the per-row fold\n",
      " |          assignments.\n",
      " |      :param weights_column: The name or index of the column in training_frame that holds the per-row weights.\n",
      " |      :param validation_frame: H2OFrame with validation data to be scored on while training.\n",
      " |      :param float max_runtime_secs: Maximum allowed runtime in seconds for model training. Use 0 to disable.\n",
      " |      :param bool verbose: Print scoring history to stdout. Defaults to False.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from h2o.estimators.estimator_base.H2OEstimator:\n",
      " |  \n",
      " |  mixin(obj, cls)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from h2o.model.model_base.ModelBase:\n",
      " |  \n",
      " |  __getattr__(self, name)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  aic(self, train=False, valid=False, xval=False)\n",
      " |      Get the AIC (Akaike Information Criterium).\n",
      " |      \n",
      " |      If all are False (default), then return the training metric value.\n",
      " |      If more than one options is set to True, then return a dictionary of metrics where the keys are \"train\",\n",
      " |      \"valid\", and \"xval\".\n",
      " |      \n",
      " |      :param bool train: If train is True, then return the AIC value for the training data.\n",
      " |      :param bool valid: If valid is True, then return the AIC value for the validation data.\n",
      " |      :param bool xval:  If xval is True, then return the AIC value for the validation data.\n",
      " |      \n",
      " |      :returns: The AIC.\n",
      " |  \n",
      " |  auc(self, train=False, valid=False, xval=False)\n",
      " |      Get the AUC (Area Under Curve).\n",
      " |      \n",
      " |      If all are False (default), then return the training metric value.\n",
      " |      If more than one options is set to True, then return a dictionary of metrics where the keys are \"train\",\n",
      " |      \"valid\", and \"xval\".\n",
      " |      \n",
      " |      :param bool train: If train is True, then return the AUC value for the training data.\n",
      " |      :param bool valid: If valid is True, then return the AUC value for the validation data.\n",
      " |      :param bool xval:  If xval is True, then return the AUC value for the validation data.\n",
      " |      \n",
      " |      :returns: The AUC.\n",
      " |  \n",
      " |  aucpr(self, train=False, valid=False, xval=False)\n",
      " |      Get the aucPR (Area Under PRECISION RECALL Curve).\n",
      " |      \n",
      " |      If all are False (default), then return the training metric value.\n",
      " |      If more than one options is set to True, then return a dictionary of metrics where the keys are \"train\",\n",
      " |      \"valid\", and \"xval\".\n",
      " |      \n",
      " |      :param bool train: If train is True, then return the aucpr value for the training data.\n",
      " |      :param bool valid: If valid is True, then return the aucpr value for the validation data.\n",
      " |      :param bool xval:  If xval is True, then return the aucpr value for the validation data.\n",
      " |      \n",
      " |      :returns: The aucpr.\n",
      " |  \n",
      " |  biases(self, vector_id=0)\n",
      " |      Return the frame for the respective bias vector.\n",
      " |      \n",
      " |      :param: vector_id: an integer, ranging from 0 to number of layers, that specifies the bias vector to return.\n",
      " |      \n",
      " |      :returns: an H2OFrame which represents the bias vector identified by vector_id\n",
      " |  \n",
      " |  catoffsets(self)\n",
      " |      Categorical offsets for one-hot encoding.\n",
      " |  \n",
      " |  coef(self)\n",
      " |      Return the coefficients which can be applied to the non-standardized data.\n",
      " |      \n",
      " |      Note: standardize = True by default, if set to False then coef() return the coefficients which are fit directly.\n",
      " |  \n",
      " |  coef_norm(self)\n",
      " |      Return coefficients fitted on the standardized data (requires standardize = True, which is on by default).\n",
      " |      \n",
      " |      These coefficients can be used to evaluate variable importance.\n",
      " |  \n",
      " |  cross_validation_fold_assignment(self)\n",
      " |      Obtain the cross-validation fold assignment for all rows in the training data.\n",
      " |      \n",
      " |      :returns: H2OFrame\n",
      " |  \n",
      " |  cross_validation_holdout_predictions(self)\n",
      " |      Obtain the (out-of-sample) holdout predictions of all cross-validation models on the training data.\n",
      " |      \n",
      " |      This is equivalent to summing up all H2OFrames returned by cross_validation_predictions.\n",
      " |      \n",
      " |      :returns: H2OFrame\n",
      " |  \n",
      " |  cross_validation_metrics_summary(self)\n",
      " |      Retrieve Cross-Validation Metrics Summary.\n",
      " |      \n",
      " |      :returns: The cross-validation metrics summary as an H2OTwoDimTable\n",
      " |  \n",
      " |  cross_validation_models(self)\n",
      " |      Obtain a list of cross-validation models.\n",
      " |      \n",
      " |      :returns: list of H2OModel objects.\n",
      " |  \n",
      " |  cross_validation_predictions(self)\n",
      " |      Obtain the (out-of-sample) holdout predictions of all cross-validation models on their holdout data.\n",
      " |      \n",
      " |      Note that the predictions are expanded to the full number of rows of the training data, with 0 fill-in.\n",
      " |      \n",
      " |      :returns: list of H2OFrame objects.\n",
      " |  \n",
      " |  deepfeatures(self, test_data, layer)\n",
      " |      Return hidden layer details.\n",
      " |      \n",
      " |      :param test_data: Data to create a feature space on\n",
      " |      :param layer: 0 index hidden layer\n",
      " |  \n",
      " |  detach(self)\n",
      " |      Detach the Python object from the backend, usually by clearing its key\n",
      " |  \n",
      " |  download_model(self, path='')\n",
      " |      Download an H2O Model object to disk.\n",
      " |      \n",
      " |      :param model: The model object to download.\n",
      " |      :param path: a path to the directory where the model should be saved.\n",
      " |      \n",
      " |      :returns: the path of the downloaded model\n",
      " |  \n",
      " |  download_mojo(self, path='.', get_genmodel_jar=False, genmodel_name='')\n",
      " |      Download the model in MOJO format.\n",
      " |      \n",
      " |      :param path: the path where MOJO file should be saved.\n",
      " |      :param get_genmodel_jar: if True, then also download h2o-genmodel.jar and store it in folder ``path``.\n",
      " |      :param genmodel_name: Custom name of genmodel jar\n",
      " |      :returns: name of the MOJO file written.\n",
      " |  \n",
      " |  download_pojo(self, path='', get_genmodel_jar=False, genmodel_name='')\n",
      " |      Download the POJO for this model to the directory specified by path.\n",
      " |      \n",
      " |      If path is an empty string, then dump the output to screen.\n",
      " |      \n",
      " |      :param path:  An absolute path to the directory where POJO should be saved.\n",
      " |      :param get_genmodel_jar: if True, then also download h2o-genmodel.jar and store it in folder ``path``.\n",
      " |      :param genmodel_name: Custom name of genmodel jar\n",
      " |      :returns: name of the POJO file written.\n",
      " |  \n",
      " |  feature_frequencies(self, test_data)\n",
      " |      Retrieve the number of occurrences of each feature for given observations \n",
      " |      on their respective paths in a tree ensemble model.\n",
      " |      Available for GBM, Random Forest and Isolation Forest models.\n",
      " |      \n",
      " |      :param H2OFrame test_data: Data on which to calculate feature frequencies.\n",
      " |      \n",
      " |      :returns: A new H2OFrame made of feature contributions.\n",
      " |  \n",
      " |  get_xval_models(self, key=None)\n",
      " |      Return a Model object.\n",
      " |      \n",
      " |      :param key: If None, return all cross-validated models; otherwise return the model that key points to.\n",
      " |      \n",
      " |      :returns: A model or list of models.\n",
      " |  \n",
      " |  gini(self, train=False, valid=False, xval=False)\n",
      " |      Get the Gini coefficient.\n",
      " |      \n",
      " |      If all are False (default), then return the training metric value.\n",
      " |      If more than one options is set to True, then return a dictionary of metrics where the keys are \"train\",\n",
      " |      \"valid\", and \"xval\"\n",
      " |      \n",
      " |      :param bool train: If train is True, then return the Gini Coefficient value for the training data.\n",
      " |      :param bool valid: If valid is True, then return the Gini Coefficient value for the validation data.\n",
      " |      :param bool xval:  If xval is True, then return the Gini Coefficient value for the cross validation data.\n",
      " |      \n",
      " |      :returns: The Gini Coefficient for this binomial model.\n",
      " |  \n",
      " |  is_cross_validated(self)\n",
      " |      Return True if the model was cross-validated.\n",
      " |  \n",
      " |  logloss(self, train=False, valid=False, xval=False)\n",
      " |      Get the Log Loss.\n",
      " |      \n",
      " |      If all are False (default), then return the training metric value.\n",
      " |      If more than one options is set to True, then return a dictionary of metrics where the keys are \"train\",\n",
      " |      \"valid\", and \"xval\".\n",
      " |      \n",
      " |      :param bool train: If train is True, then return the log loss value for the training data.\n",
      " |      :param bool valid: If valid is True, then return the log loss value for the validation data.\n",
      " |      :param bool xval:  If xval is True, then return the log loss value for the cross validation data.\n",
      " |      \n",
      " |      :returns: The log loss for this regression model.\n",
      " |  \n",
      " |  mae(self, train=False, valid=False, xval=False)\n",
      " |      Get the Mean Absolute Error.\n",
      " |      \n",
      " |      If all are False (default), then return the training metric value.\n",
      " |      If more than one options is set to True, then return a dictionary of metrics where the keys are \"train\",\n",
      " |      \"valid\", and \"xval\".\n",
      " |      \n",
      " |      :param bool train: If train is True, then return the MAE value for the training data.\n",
      " |      :param bool valid: If valid is True, then return the MAE value for the validation data.\n",
      " |      :param bool xval:  If xval is True, then return the MAE value for the cross validation data.\n",
      " |      \n",
      " |      :returns: The MAE for this regression model.\n",
      " |  \n",
      " |  mean_residual_deviance(self, train=False, valid=False, xval=False)\n",
      " |      Get the Mean Residual Deviances.\n",
      " |      \n",
      " |      If all are False (default), then return the training metric value.\n",
      " |      If more than one options is set to True, then return a dictionary of metrics where the keys are \"train\",\n",
      " |      \"valid\", and \"xval\".\n",
      " |      \n",
      " |      :param bool train: If train is True, then return the Mean Residual Deviance value for the training data.\n",
      " |      :param bool valid: If valid is True, then return the Mean Residual Deviance value for the validation data.\n",
      " |      :param bool xval:  If xval is True, then return the Mean Residual Deviance value for the cross validation data.\n",
      " |      \n",
      " |      :returns: The Mean Residual Deviance for this regression model.\n",
      " |  \n",
      " |  model_performance(self, test_data=None, train=False, valid=False, xval=False)\n",
      " |      Generate model metrics for this model on test_data.\n",
      " |      \n",
      " |      :param H2OFrame test_data: Data set for which model metrics shall be computed against. All three of train,\n",
      " |          valid and xval arguments are ignored if test_data is not None.\n",
      " |      :param bool train: Report the training metrics for the model.\n",
      " |      :param bool valid: Report the validation metrics for the model.\n",
      " |      :param bool xval: Report the cross-validation metrics for the model. If train and valid are True, then it\n",
      " |          defaults to True.\n",
      " |      \n",
      " |      :returns: An object of class H2OModelMetrics.\n",
      " |  \n",
      " |  mse(self, train=False, valid=False, xval=False)\n",
      " |      Get the Mean Square Error.\n",
      " |      \n",
      " |      If all are False (default), then return the training metric value.\n",
      " |      If more than one options is set to True, then return a dictionary of metrics where the keys are \"train\",\n",
      " |      \"valid\", and \"xval\".\n",
      " |      \n",
      " |      :param bool train: If train is True, then return the MSE value for the training data.\n",
      " |      :param bool valid: If valid is True, then return the MSE value for the validation data.\n",
      " |      :param bool xval:  If xval is True, then return the MSE value for the cross validation data.\n",
      " |      \n",
      " |      :returns: The MSE for this regression model.\n",
      " |  \n",
      " |  normmul(self)\n",
      " |      Normalization/Standardization multipliers for numeric predictors.\n",
      " |  \n",
      " |  normsub(self)\n",
      " |      Normalization/Standardization offsets for numeric predictors.\n",
      " |  \n",
      " |  ntrees_actual(self)\n",
      " |      Returns actual number of trees in a tree model. If early stopping enabled, GBM can reset the ntrees value.\n",
      " |      In this case, the actual ntrees value is less than the original ntrees value a user set before\n",
      " |      building the model.\n",
      " |      \n",
      " |      Type: ``float``\n",
      " |  \n",
      " |  null_degrees_of_freedom(self, train=False, valid=False, xval=False)\n",
      " |      Retreive the null degress of freedom if this model has the attribute, or None otherwise.\n",
      " |      \n",
      " |      :param bool train: Get the null dof for the training set. If both train and valid are False, then train is\n",
      " |          selected by default.\n",
      " |      :param bool valid: Get the null dof for the validation set. If both train and valid are True, then train is\n",
      " |          selected by default.\n",
      " |      \n",
      " |      :returns: Return the null dof, or None if it is not present.\n",
      " |  \n",
      " |  null_deviance(self, train=False, valid=False, xval=False)\n",
      " |      Retreive the null deviance if this model has the attribute, or None otherwise.\n",
      " |      \n",
      " |      :param bool train: Get the null deviance for the training set. If both train and valid are False, then train\n",
      " |          is selected by default.\n",
      " |      :param bool valid: Get the null deviance for the validation set. If both train and valid are True, then train\n",
      " |          is selected by default.\n",
      " |      \n",
      " |      :returns: Return the null deviance, or None if it is not present.\n",
      " |  \n",
      " |  partial_plot(self, data, cols=None, destination_key=None, nbins=20, weight_column=None, plot=True, plot_stddev=True, figsize=(7, 10), server=False, include_na=False, user_splits=None, col_pairs_2dpdp=None, save_to_file=None, row_index=None)\n",
      " |      Create partial dependence plot which gives a graphical depiction of the marginal effect of a variable on the\n",
      " |      response. The effect of a variable is measured in change in the mean response.\n",
      " |      \n",
      " |      :param H2OFrame data: An H2OFrame object used for scoring and constructing the plot.\n",
      " |      :param cols: Feature(s) for which partial dependence will be calculated.\n",
      " |      :param destination_key: An key reference to the created partial dependence tables in H2O.\n",
      " |      :param nbins: Number of bins used. For categorical columns make sure the number of bins exceed the level count. If you enable add_missing_NA, the returned length will be nbin+1.\n",
      " |      :param weight_column: A string denoting which column of data should be used as the weight column.\n",
      " |      :param plot: A boolean specifying whether to plot partial dependence table.\n",
      " |      :param plot_stddev: A boolean specifying whether to add std err to partial dependence plot.\n",
      " |      :param figsize: Dimension/size of the returning plots, adjust to fit your output cells.\n",
      " |      :param server: Specify whether to activate matplotlib \"server\" mode. In this case, the plots are saved to a file instead of being rendered.\n",
      " |      :param include_na: A boolean specifying whether missing value should be included in the Feature values.\n",
      " |      :param user_splits: a dictionary containing column names as key and user defined split values as value in a list.\n",
      " |      :param col_pairs_2dpdp: list containing pairs of column names for 2D pdp\n",
      " |      :param save_to_file: Fully qualified name to an image file the resulting plot should be saved to, e.g. '/home/user/pdpplot.png'. The 'png' postfix might be omitted. If the file already exists, it will be overridden. Plot is only saved if plot = True.\n",
      " |      :param row_index: Row for which partial dependence will be calculated instead of the whole input frame.\n",
      " |      :returns: Plot and list of calculated mean response tables for each feature requested.\n",
      " |  \n",
      " |  pprint_coef(self)\n",
      " |      Pretty print the coefficents table (includes normalized coefficients).\n",
      " |  \n",
      " |  pr_auc(self, train=False, valid=False, xval=False)\n",
      " |      ModelBase.pr_auc is deprecated, please use ``ModelBase.aucpr`` instead.\n",
      " |  \n",
      " |  predict(self, test_data, custom_metric=None, custom_metric_func=None)\n",
      " |      Predict on a dataset.\n",
      " |      \n",
      " |      :param H2OFrame test_data: Data on which to make predictions.\n",
      " |      :param custom_metric:  custom evaluation function defined as class reference, the class get uploaded\n",
      " |          into the cluster\n",
      " |      :param custom_metric_func: custom evaluation function reference, e.g, result of upload_custom_metric\n",
      " |      \n",
      " |      :returns: A new H2OFrame of predictions.\n",
      " |  \n",
      " |  predict_contributions(self, test_data)\n",
      " |      Predict feature contributions - SHAP values on an H2O Model (only GBM and XGBoost models).\n",
      " |      \n",
      " |      Returned H2OFrame has shape (#rows, #features + 1) - there is a feature contribution column for each input\n",
      " |      feature, the last column is the model bias (same value for each row). The sum of the feature contributions\n",
      " |      and the bias term is equal to the raw prediction of the model. Raw prediction of tree-based model is the sum \n",
      " |      of the predictions of the individual trees before before the inverse link function is applied to get the actual\n",
      " |      prediction. For Gaussian distribution the sum of the contributions is equal to the model prediction. \n",
      " |      \n",
      " |      Note: Multinomial classification models are currently not supported.\n",
      " |      \n",
      " |      :param H2OFrame test_data: Data on which to calculate contributions.\n",
      " |      \n",
      " |      :returns: A new H2OFrame made of feature contributions.\n",
      " |  \n",
      " |  predict_leaf_node_assignment(self, test_data, type='Path')\n",
      " |      Predict on a dataset and return the leaf node assignment (only for tree-based models).\n",
      " |      \n",
      " |      :param H2OFrame test_data: Data on which to make predictions.\n",
      " |      :param Enum type: How to identify the leaf node. Nodes can be either identified by a path from to the root node\n",
      " |          of the tree to the node or by H2O's internal node id. One of: ``\"Path\"``, ``\"Node_ID\"`` (default: ``\"Path\"``).\n",
      " |      \n",
      " |      :returns: A new H2OFrame of predictions.\n",
      " |  \n",
      " |  r2(self, train=False, valid=False, xval=False)\n",
      " |      Return the R squared for this regression model.\n",
      " |      \n",
      " |      Will return R^2 for GLM Models and will return NaN otherwise.\n",
      " |      \n",
      " |      The R^2 value is defined to be 1 - MSE/var, where var is computed as sigma*sigma.\n",
      " |      \n",
      " |      If all are False (default), then return the training metric value.\n",
      " |      If more than one options is set to True, then return a dictionary of metrics where the keys are \"train\",\n",
      " |      \"valid\", and \"xval\".\n",
      " |      \n",
      " |      :param bool train: If train is True, then return the R^2 value for the training data.\n",
      " |      :param bool valid: If valid is True, then return the R^2 value for the validation data.\n",
      " |      :param bool xval:  If xval is True, then return the R^2 value for the cross validation data.\n",
      " |      \n",
      " |      :returns: The R squared for this regression model.\n",
      " |  \n",
      " |  residual_degrees_of_freedom(self, train=False, valid=False, xval=False)\n",
      " |      Retreive the residual degress of freedom if this model has the attribute, or None otherwise.\n",
      " |      \n",
      " |      :param bool train: Get the residual dof for the training set. If both train and valid are False, then train\n",
      " |          is selected by default.\n",
      " |      :param bool valid: Get the residual dof for the validation set. If both train and valid are True, then train\n",
      " |          is selected by default.\n",
      " |      \n",
      " |      :returns: Return the residual dof, or None if it is not present.\n",
      " |  \n",
      " |  residual_deviance(self, train=False, valid=False, xval=None)\n",
      " |      Retreive the residual deviance if this model has the attribute, or None otherwise.\n",
      " |      \n",
      " |      :param bool train: Get the residual deviance for the training set. If both train and valid are False, then\n",
      " |          train is selected by default.\n",
      " |      :param bool valid: Get the residual deviance for the validation set. If both train and valid are True, then\n",
      " |          train is selected by default.\n",
      " |      \n",
      " |      :returns: Return the residual deviance, or None if it is not present.\n",
      " |  \n",
      " |  respmul(self)\n",
      " |      Normalization/Standardization multipliers for numeric response.\n",
      " |  \n",
      " |  respsub(self)\n",
      " |      Normalization/Standardization offsets for numeric response.\n",
      " |  \n",
      " |  rmse(self, train=False, valid=False, xval=False)\n",
      " |      Get the Root Mean Square Error.\n",
      " |      \n",
      " |      If all are False (default), then return the training metric value.\n",
      " |      If more than one options is set to True, then return a dictionary of metrics where the keys are \"train\",\n",
      " |      \"valid\", and \"xval\".\n",
      " |      \n",
      " |      :param bool train: If train is True, then return the RMSE value for the training data.\n",
      " |      :param bool valid: If valid is True, then return the RMSE value for the validation data.\n",
      " |      :param bool xval:  If xval is True, then return the RMSE value for the cross validation data.\n",
      " |      \n",
      " |      :returns: The RMSE for this regression model.\n",
      " |  \n",
      " |  rmsle(self, train=False, valid=False, xval=False)\n",
      " |      Get the Root Mean Squared Logarithmic Error.\n",
      " |      \n",
      " |      If all are False (default), then return the training metric value.\n",
      " |      If more than one options is set to True, then return a dictionary of metrics where the keys are \"train\",\n",
      " |      \"valid\", and \"xval\".\n",
      " |      \n",
      " |      :param bool train: If train is True, then return the RMSLE value for the training data.\n",
      " |      :param bool valid: If valid is True, then return the RMSLE value for the validation data.\n",
      " |      :param bool xval:  If xval is True, then return the RMSLE value for the cross validation data.\n",
      " |      \n",
      " |      :returns: The RMSLE for this regression model.\n",
      " |  \n",
      " |  rotation(self)\n",
      " |      Obtain the rotations (eigenvectors) for a PCA model\n",
      " |      \n",
      " |      :return: H2OFrame\n",
      " |  \n",
      " |  save_model_details(self, path='', force=False)\n",
      " |      Save Model Details of an H2O Model in JSON Format to disk.\n",
      " |      \n",
      " |      :param model: The model object to save.\n",
      " |      :param path: a path to save the model details at (hdfs, s3, local)\n",
      " |      :param force: if True overwrite destination directory in case it exists, or throw exception if set to False.\n",
      " |      \n",
      " |      :returns str: the path of the saved model details\n",
      " |  \n",
      " |  save_mojo(self, path='', force=False)\n",
      " |      Save an H2O Model as MOJO (Model Object, Optimized) to disk.\n",
      " |      \n",
      " |      :param model: The model object to save.\n",
      " |      :param path: a path to save the model at (hdfs, s3, local)\n",
      " |      :param force: if True overwrite destination directory in case it exists, or throw exception if set to False.\n",
      " |      \n",
      " |      :returns str: the path of the saved model\n",
      " |  \n",
      " |  score_history(self)\n",
      " |      DEPRECATED. Use :meth:`scoring_history` instead.\n",
      " |  \n",
      " |  scoring_history(self)\n",
      " |      Retrieve Model Score History.\n",
      " |      \n",
      " |      :returns: The score history as an H2OTwoDimTable or a Pandas DataFrame.\n",
      " |  \n",
      " |  show(self)\n",
      " |      Print innards of model, without regards to type.\n",
      " |  \n",
      " |  staged_predict_proba(self, test_data)\n",
      " |      Predict class probabilities at each stage of an H2O Model (only GBM models).\n",
      " |      \n",
      " |      The output structure is analogous to the output of function predict_leaf_node_assignment. For each tree t and\n",
      " |      class c there will be a column Tt.Cc (eg. T3.C1 for tree 3 and class 1). The value will be the corresponding\n",
      " |      predicted probability of this class by combining the raw contributions of trees T1.Cc,..,TtCc. Binomial models\n",
      " |      build the trees just for the first class and values in columns Tx.C1 thus correspond to the the probability p0.\n",
      " |      \n",
      " |      :param H2OFrame test_data: Data on which to make predictions.\n",
      " |      \n",
      " |      :returns: A new H2OFrame of staged predictions.\n",
      " |  \n",
      " |  std_coef_plot(self, num_of_features=None, server=False)\n",
      " |      Plot a GLM model\"s standardized coefficient magnitudes.\n",
      " |      \n",
      " |      :param num_of_features: the number of features shown in the plot.\n",
      " |      :param server: ?\n",
      " |      \n",
      " |      :returns: None.\n",
      " |  \n",
      " |  summary(self)\n",
      " |      Print a detailed summary of the model.\n",
      " |  \n",
      " |  training_model_metrics(self)\n",
      " |      Return training model metrics for any model.\n",
      " |  \n",
      " |  varimp(self, use_pandas=False)\n",
      " |      Pretty print the variable importances, or return them in a list.\n",
      " |      \n",
      " |      :param bool use_pandas: If True, then the variable importances will be returned as a pandas data frame.\n",
      " |      \n",
      " |      :returns: A list or Pandas DataFrame.\n",
      " |  \n",
      " |  varimp_plot(self, num_of_features=None, server=False)\n",
      " |      Plot the variable importance for a trained model.\n",
      " |      \n",
      " |      :param num_of_features: the number of features shown in the plot (default is 10 or all if less than 10).\n",
      " |      :param server: ?\n",
      " |      \n",
      " |      :returns: None.\n",
      " |  \n",
      " |  weights(self, matrix_id=0)\n",
      " |      Return the frame for the respective weight matrix.\n",
      " |      \n",
      " |      :param matrix_id: an integer, ranging from 0 to number of layers, that specifies the weight matrix to return.\n",
      " |      \n",
      " |      :returns: an H2OFrame which represents the weight matrix identified by matrix_id\n",
      " |  \n",
      " |  xval_keys(self)\n",
      " |      Return model keys for the cross-validated model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from h2o.model.model_base.ModelBase:\n",
      " |  \n",
      " |  actual_params\n",
      " |      Dictionary of actual parameters of the model.\n",
      " |  \n",
      " |  default_params\n",
      " |      Dictionary of the default parameters of the model.\n",
      " |  \n",
      " |  end_time\n",
      " |      Timestamp (milliseconds since 1970) when the model training was ended.\n",
      " |  \n",
      " |  full_parameters\n",
      " |      Dictionary of the full specification of all parameters.\n",
      " |  \n",
      " |  have_mojo\n",
      " |      True, if export to MOJO is possible\n",
      " |  \n",
      " |  have_pojo\n",
      " |      True, if export to POJO is possible\n",
      " |  \n",
      " |  key\n",
      " |      :return: the unique key representing the object on the backend\n",
      " |  \n",
      " |  model_id\n",
      " |      Model identifier.\n",
      " |  \n",
      " |  params\n",
      " |      Get the parameters and the actual/default values only.\n",
      " |      \n",
      " |      :returns: A dictionary of parameters used to build this model.\n",
      " |  \n",
      " |  run_time\n",
      " |      Model training time in milliseconds\n",
      " |  \n",
      " |  start_time\n",
      " |      Timestamp (milliseconds since 1970) when the model training was started.\n",
      " |  \n",
      " |  type\n",
      " |      The type of model built: ``\"classifier\"`` or ``\"regressor\"`` or ``\"unsupervised\"``\n",
      " |  \n",
      " |  xvals\n",
      " |      Return a list of the cross-validated models.\n",
      " |      \n",
      " |      :returns: A list of models.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from h2o.base.Keyed:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from h2o.estimators.word2vec import H2OWord2vecEstimator\n",
    "help(H2OWord2vecEstimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_reviews = review_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pdf_reviews' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-0abfe5d900ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcount_vect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfit_count_vect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_vect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_reviews\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_count_vect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_reviews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pdf_reviews' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "fit_count_vect = count_vect.fit(pdf_reviews)\n",
    "\n",
    "x = fit_count_vect.transform(pdf_reviews.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The usage of `cmp` is deprecated and will be removed on or after 2021-06-01.  Please use `eq` and `order` instead.\n",
      "The default validate=True will be replaced by validate=False in 0.22.\n",
      "Beginning in version 0.22, arrays of bytes/strings will be converted to decimal numbers if dtype='numeric'. It is recommended that you convert the array to a float dtype before using it in scikit-learn, for example by using your_array = your_array.astype(np.float64).\n",
      "The default validate=True will be replaced by validate=False in 0.22.\n",
      "Beginning in version 0.22, arrays of bytes/strings will be converted to decimal numbers if dtype='numeric'. It is recommended that you convert the array to a float dtype before using it in scikit-learn, for example by using your_array = your_array.astype(np.float64).\n",
      "The default validate=True will be replaced by validate=False in 0.22.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving artifact of size: 2.221 KB to Splice Machine DB\n",
      "Saving model of size: 2.221 KB to Splice Machine DB\n",
      "Prediction labels found. Using ['C0', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7'] as labels for predictions [0, 1, 2, 3, 4, 5, 6, 7] respectively\n",
      "Deploying model 893ed233c789 to table ben.sk_vec_pipe\n",
      "Creating data table ... \n",
      " CREATE TABLE ben.sk_vec_pipe (\n",
      "\tsentence VARCHAR(5000),\tMOMENT_KEY INT,\n",
      "\tPRIMARY KEY(MOMENT_KEY)\n",
      ")\n",
      "\n",
      "Done.\n",
      "Creating prediction table ... \n",
      "CREATE TABLE ben.sk_vec_pipe_PREDS (\n",
      "        \tCUR_USER VARCHAR(50) DEFAULT CURRENT_USER,\n",
      "        \tEVAL_TIME TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
      "        \tRUN_ID VARCHAR(50) DEFAULT '893ed233c789',\n",
      "        \tMOMENT_KEY INT,\n",
      "\t\"C0\" DOUBLE,\n",
      "\t\"C1\" DOUBLE,\n",
      "\t\"C2\" DOUBLE,\n",
      "\t\"C3\" DOUBLE,\n",
      "\t\"C4\" DOUBLE,\n",
      "\t\"C5\" DOUBLE,\n",
      "\t\"C6\" DOUBLE,\n",
      "\t\"C7\" DOUBLE,\n",
      "\tPRIMARY KEY(MOMENT_KEY)\n",
      ")\n",
      "\n",
      "Done.\n",
      "Creating model prediction trigger ... \n",
      "CREATE TRIGGER runModel_ben_sk_vec_pipe_893ed233c789\n",
      " \tAFTER INSERT\n",
      " \tON ben.sk_vec_pipe\n",
      " \tREFERENCING NEW AS NEWROW\n",
      " \tFOR EACH ROW\n",
      " \t\tINSERT INTO ben.sk_vec_pipe_PREDS(MOMENT_KEY,\"C0\",\"C1\",\"C2\",\"C3\",\"C4\",\"C5\",\"C6\",\"C7\") SELECT \tNEWROW.MOMENT_KEY, b.\"C0\",b.\"C1\",b.\"C2\",b.\"C3\",b.\"C4\",b.\"C5\",b.\"C6\",b.\"C7\" FROM new com.splicemachine.mlrunner.MLRunner('key_value', '893ed233c789', TRIM(CAST(NEWROW.sentence as CHAR(41))), 'sentence VARCHAR(5000)', 'transform', 'None') as b (\"C0\" DOUBLE,\"C1\" DOUBLE,\"C2\" DOUBLE,\"C3\" DOUBLE,\"C4\" DOUBLE,\"C5\" DOUBLE,\"C6\" DOUBLE,\"C7\" DOUBLE)\n",
      "\n",
      "Done.\n",
      "Model Deployed.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline as skPipe\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import pandas as pd\n",
    "# #Custom Transformer that extracts columns passed as argument to its constructor \n",
    "# class PreProcessor( BaseEstimator, TransformerMixin ):\n",
    "#     #Class Constructor \n",
    "#     def __init__( self ):\n",
    "#         pass\n",
    "    \n",
    "#     #Return self nothing else to do here    \n",
    "#     def fit( self, X, y = None):\n",
    "#         return self\n",
    "    \n",
    "#     #Method that describes what we need this transformer to do\n",
    "#     def transform( self, X, y = None ):\n",
    "#         return X[0]\n",
    "\n",
    "# #Custom Transformer that extracts columns passed as argument to its constructor \n",
    "# class VectorSelector( BaseEstimator, TransformerMixin ):\n",
    "#     #Class Constructor \n",
    "#     def __init__( self ):\n",
    "#         pass\n",
    "    \n",
    "#     #Return self nothing else to do here    \n",
    "#     def fit( self, X, y = None):\n",
    "#         return self \n",
    "    \n",
    "#     #Method that describes what we need this transformer to do\n",
    "#     def transform( self, X, y = None ):\n",
    "#         return X.toarray()\n",
    "\n",
    "\n",
    "# def pre_process(X):\n",
    "#     return X[0]\n",
    "# def get_vector(X):\n",
    "#     return X.toarray()\n",
    "# p = skPipe(steps = [\n",
    "#     ('preprocessor', FunctionTransformer(pre_process)),\n",
    "#     ('vectorizer', TfidfVectorizer()),\n",
    "#     ('vector_returner', FunctionTransformer(get_vector, accept_sparse=True))\n",
    "# ])\n",
    "\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\",\n",
    "        \"The dog.\",\n",
    "        \"The fox\"]\n",
    "\n",
    "p = skPipe(steps = [\n",
    "    ('preprocessor', FunctionTransformer(lambda x: x[0])),\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('vector_returner', FunctionTransformer(lambda x: x.toarray(), accept_sparse=True))\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# tokenize and build vocab\n",
    "p.fit([text])\n",
    "\n",
    "# vector = p.transform([[text[0]]])\n",
    "\n",
    "\n",
    "splice._dropTableIfExists('ben.sk_vec_pipe')\n",
    "splice._dropTableIfExists('ben.sk_vec_pipe_preds')\n",
    "with mlflow.start_run(run_name = 'sk_tokenizer2'):\n",
    "    mlflow.log_model(p, 'tokenizer_model2')\n",
    "    mlflow.deploy_db(p, pd.DataFrame(text,columns=['sentence']), 'ben', 'sk_vec_pipe', [('MOMENT_KEY', 'INT')], run_id=mlflow.current_run_id(),\n",
    "                     classes = [f'C{i}' for i in range(8)], verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f5e1e44-b1b8-48b9-82c7-5e8ea45eb874",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0417e1d-397c-4d83-b7c8-7e8b9e979fcc",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%sql\n",
    "\n",
    "CREATE TABLE ben.sk_vec_pipe_2 (\n",
    "\tsentence VARCHAR(5000),\tMOMENT_KEY INT,\n",
    "    CUR_USER VARCHAR(50) DEFAULT CURRENT_USER,\n",
    "    EVAL_TIME TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "    RUN_ID VARCHAR(50) DEFAULT '893ed233c789',\n",
    "    \"C0\" DOUBLE,\n",
    "\t\"C1\" DOUBLE,\n",
    "\t\"C2\" DOUBLE,\n",
    "\t\"C3\" DOUBLE,\n",
    "\t\"C4\" DOUBLE,\n",
    "\t\"C5\" DOUBLE,\n",
    "\t\"C6\" DOUBLE,\n",
    "\t\"C7\" DOUBLE,\n",
    "\tPRIMARY KEY(MOMENT_KEY)\n",
    ");\n",
    "CREATE TRIGGER runModel_ben_sk_vec_pipe_2_893ed233c789\n",
    " \tAFTER INSERT\n",
    " \tON ben.sk_vec_pipe_2\n",
    " \tREFERENCING NEW AS NEWROW\n",
    " \tFOR EACH ROW\n",
    " \t\tINSERT INTO ben.sk_vec_pipe_2(MOMENT_KEY,\"C0\",\"C1\",\"C2\",\"C3\",\"C4\",\"C5\",\"C6\",\"C7\") --splice-properties insertMode=UPSERT\n",
    "        SELECT \tNEWROW.MOMENT_KEY, b.\"C0\",b.\"C1\",b.\"C2\",b.\"C3\",b.\"C4\",b.\"C5\",b.\"C6\",b.\"C7\" FROM new com.splicemachine.mlrunner.MLRunner('key_value', '893ed233c789', TRIM(CAST(NEWROW.sentence as CHAR(41))), 'sentence VARCHAR(5000)', 'transform', 'None') as b (\"C0\" DOUBLE,\"C1\" DOUBLE,\"C2\" DOUBLE,\"C3\" DOUBLE,\"C4\" DOUBLE,\"C5\" DOUBLE,\"C6\" DOUBLE,\"C7\" DOUBLE) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudpickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = 'The quick brown fox jumped over the lazy dog.'\n",
    "X = [fast_float(x) for x in X.split(',')]\n",
    "preds = p.transform([X])\n",
    "len(preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudpickle\n",
    "cloudpickle.dump(p,open('model.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7609dad-358c-487c-bd73-b81c23dc8cfd",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e22415a-ecf3-42c2-b84c-f9e5f0d6c319",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.56 ms, sys: 4.02 ms, total: 8.58 ms\n",
      "Wall time: 1.06 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%sql\n",
    "insert into ben.sk_vec_pipe_2 (SENTENCE, MOMENT_KEY) values('The dog.', 4);\n",
    "\n",
    "select * from ben.sk_vec_pipe_2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "895eba38-f9b9-4745-a69d-f0c7a27b5852",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfea2c14-1444-48e0-bee6-57d5620b9165",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "%%sql\n",
    "insert into ben.sk_vec_pipe values('The dog.', 1230);\n",
    "\n",
    "select * from ben.sk_vec_pipe_PREDS;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Query executed successfully. Affected rows : 0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "select * from sk_vec_pipe_preds;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The quick brown fox jumped over the lazy dog.'"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[0][:46]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.36388646, 0.27674503, 0.27674503, 0.36388646, 0.36388646,\n",
       "        0.36388646, 0.36388646, 0.42983441]])"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = 'The quick brown fox jumped over the lazy dog.'\n",
    "X = [x for x in X.split(',')]\n",
    "p.transform([X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "212px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
