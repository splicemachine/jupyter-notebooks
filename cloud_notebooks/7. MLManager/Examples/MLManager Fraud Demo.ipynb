{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "#s {\n",
    "}\n",
    "h1, h2, h3, h4, h5, h6, table, button, a, p, blockquote {\n",
    "font-family:Geneva;\n",
    "}\n",
    "\n",
    ".log {\n",
    "transition: all .2s ease-in-out;\n",
    "}\n",
    "\n",
    ".log:hover {a\n",
    "transform: scale(1.05);\n",
    "}\n",
    "</style>\n",
    "<div id='s' style='width:100%'>\n",
    "<center><img class='log' src='https://splicemachine.com/wp-content/uploads/splice-logo-1.png' width='20%' style='z-index:5'></center>\n",
    "<center><h1 class='log' style='font-size:50px; color:black;'>Welcome to Splice Machine MLManager</h1></center>\n",
    "<center><h2 class = 'log' style='font-size:25px; color:grey;'>The data platform for intelligent applications</center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "blockquote{\n",
    "  font-size: 15px;\n",
    "  background: #f9f9f9;\n",
    "  border-left: 10px solid #ccc;\n",
    "  margin: .5em 10px;\n",
    "  padding: 30em, 10px;\n",
    "  quotes: \"\\201C\"\"\\201D\"\"\\2018\"\"\\2019\";\n",
    "  padding: 10px 20px;\n",
    "  line-height: 1.4;\n",
    "}\n",
    "\n",
    "blockquote:before {\n",
    "  content: open-quote;\n",
    "  display: inline;\n",
    "  height: 0;\n",
    "  line-height: 0;\n",
    "  left: -10px;\n",
    "  position: relative;\n",
    "  top: 30px;\n",
    "  bottom:30px;\n",
    "  color: #ccc;\n",
    "  font-size: 3em;\n",
    "    display:none;\n",
    "\n",
    "}\n",
    "\n",
    "p{\n",
    "  margin: 0;\n",
    "}\n",
    "\n",
    "footer{\n",
    "  margin:0;\n",
    "  text-align: right;\n",
    "  font-size: 1em;\n",
    "  font-style: italic;\n",
    "}\n",
    "</style>\n",
    "<blockquote><p class='quotation'><b><br><span style='font-size:25px'>Why use Splice Machine ML</span></b><br><br>Splice Machine ML isn't just a machine learning platform, it is a complete machine learning lifecycle management solution, giving you total control of your models, from retrieving data to scalable deployment.  <br><br>\n",
    "    <center><img class='log' src='https://s3.amazonaws.com/splice-demo/splice-machine-data-science-process.png' width='30%' style='z-index:5'></center>\n",
    "    <br><ul><li>Our platform runs directly on Apache Spark, allowing you to complete massive jobs in parallel</li><li>Our native <code>PySpliceContext</code> lets you directly access the data in your database and convert as a Spark DataFrame, no ETL.</li><li><code>MLFlow</code> is integrated directly into all Splice Machine clusters, allowing you to keep track of your entire Data Science workflow</li><li>After you have found the best model for your task, you can easily deploy it live to AWS SageMaker or AzureML to make predictions in real time.</li><li>MLFlow does not force a standard workflow, instead it allows teams to develop their own methodology easily that fits their teams and problems</li></ul><br>In this demo we will guide you through the entire MLManager life cycle.<br></p><footer>Your friends at Splice Machine</footer></blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style='font-size:25px;  font-weight:bold'>How does this work?</h2>\n",
    "<style>\n",
    "blockquote{\n",
    "  font-size: 15px;\n",
    "  background: #f9f9f9;\n",
    "  border-left: 10px solid #ccc;\n",
    "  margin: .5em 10px;\n",
    "  padding: 30em, 10px;\n",
    "  quotes: \"\\201C\"\"\\201D\"\"\\2018\"\"\\2019\";\n",
    "  padding: 10px 20px;\n",
    "  line-height: 1.4;\n",
    "}\n",
    "\n",
    "blockquote:before {\n",
    "  content: open-quote;\n",
    "  display: inline;\n",
    "  height: 0;\n",
    "  line-height: 0;\n",
    "  left: -10px;\n",
    "  position: relative;\n",
    "  top: 30px;\n",
    "  bottom:30px;\n",
    "  color: #ccc;\n",
    "  font-size: 3em;\n",
    "    display:none;\n",
    "\n",
    "}\n",
    "\n",
    "p{\n",
    "  margin: 0;\n",
    "}\n",
    "\n",
    "footer{\n",
    "  margin:0;\n",
    "  text-align: right;\n",
    "  font-size: 1em;\n",
    "  font-style: italic;\n",
    "}\n",
    "</style>\n",
    "<blockquote><p class='quotation'><b><br><span style='font-size:25px'>Jupyter</span></b> <br><br>Jupyter notebooks are a simple, easy and intuitive way to do data science, directly in your browser. Any Spark computations you run inside of the notebook are executed right on your cluster's Spark executors.<br><br>Jupyter notebooks also make machine learning easier. By using Jupyter <i>magics</i>, you can run different languages inside the same notebook. The language you want to run is signified by a %% sign followed by a magic at the top of a cell. For example, one of the interpreters you will become very familiar with while using our platform is <code>%%sql</code> magic. In the <code>%%sql</code> magic you can run standard SQL queries and visualize the results in Jupyter's built in visualization tools.<br> <br><i>This entire demo was written inside a Jupyter notebook</i></br><footer>Splice Machine</footer></blockquote><br>\n",
    "<blockquote><p class='quotation'><b><br><span style='font-size:25px'>MLFlow</span></b><br><br>As a data scientist constantly creating new models and testing new features, it is necessary to effectively track and manage those different ML runs. MLFlow allows you to track entire <code>experiments</code> and individual <code>run</code> parameters and metrics. The way you organize your flow is unique to you, and the intuitive Python API allows you to organize your development process and run with it.<br>\n",
    "<center><img class='log' src='https://splice-demo.s3.amazonaws.com/New+MLFlow+UI.png' width='30%' style='z-index:5'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<center><h1 style='font-size:35px;'>Ready? Let's get started<b style='font-size:35px'>.</b></h1></center>\n",
    "\n",
    "## Problem statement:\n",
    "### Can we predict the likelihood of fraudulent transactions after training on historical actuals? \n",
    "#### We're going to find out using Splice Machine's <code>MLManager</code>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "drop table if exists cc_fraud_data; \n",
    "create table cc_fraud_data (\n",
    "    time_offset integer,\n",
    "    expected_weekly_trans_cnt double,\n",
    "    expected_weekly_trans_amnt double,\n",
    "    expected_daily_trans_cnt double,\n",
    "    expected_daily_trans_amnt double,\n",
    "    weekly_trans_cnt double,\n",
    "    weekly_trans_amnt double,\n",
    "    daily_trans_cnt double,\n",
    "    daily_trans_amnt double,\n",
    "    rolling_avg_weekly_trans_cnt double,\n",
    "    rolling_avg_weekly_trans_amnt double,\n",
    "    rolling_avg_daily_trans_cnt double,\n",
    "    rolling_avg_daily_trans_amnt double,\n",
    "    MACD_trans_amnt double,\n",
    "    MACD_trans_cnt double,\n",
    "    RSI_trans_amnt double,\n",
    "    RSI_trans_cnt double,\n",
    "    Aroon_trans_amnt double,\n",
    "    Aroon_trans_cnt double,\n",
    "    ADX_trans_amnt double,\n",
    "    ADX_trans_cnt double,\n",
    "    current_balance double,\n",
    "    rolling_avg_balance double,\n",
    "    MACD_balance double,\n",
    "    Aroon_balance double,\n",
    "    RSI_balance double,\n",
    "    ADX_balance double,\n",
    "    credit_score double,\n",
    "    credit_limit double,\n",
    "    amount decimal(10,2),\n",
    "    class_result int,\n",
    "    primary key(time_offset)\n",
    ");\n",
    "\n",
    "call SYSCS_UTIL.IMPORT_DATA (\n",
    "     null,\n",
    "     'cc_fraud_data',\n",
    "     null,\n",
    "     's3a://splice-demo/kaggle-fraud-data/creditcard.csv',\n",
    "     ',',\n",
    "     null,\n",
    "     null,\n",
    "     null,\n",
    "     null,\n",
    "     -1,\n",
    "     's3a://splice-demo/kaggle-fraud-data/bad',\n",
    "     null, \n",
    "     null);\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%sql\n",
    "select top 10 * from cc_fraud_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%sql\n",
    "select class_result, count(*) from cc_fraud_data group by 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%sql \n",
    "explain select class_result, count(*) from cc_fraud_data group by 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style='font-size:30px;font-weight:bold'>Connecting to your database</h1><br>\n",
    "<blockquote><p class='quotation'><b><br><span style='font-size:15px'>Now, let's establish a connection to your database using Python via our <a href=\"https://www.splicemachine.com/the-splice-machine-native-spark-datasource/\">Native Spark Datasource</a>. We will use the <code>PySpliceContext</code> to establish our direct connection-- it allows us to do inserts, selects, upserts, updates and many more functions without serialization<footer>Splice Machine</footer></blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from splicemachine.spark import PySpliceContext\n",
    "# Create our Spark Session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "# Create our Native Database Connection\n",
    "splice = PySpliceContext(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Our Spark UI\n",
    "<blockquote>Now that we've created a SparkSession dedicated to this notebook, we can monitor the active jobs in the Spark UI. You can navigate to <code>/sparkmonitor/PORT</code> in the URL, replacing the port with the port of your active Spark Session. You can also view the Spark Session right here in the notebook using our <code>get_spark_ui</code> function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splicemachine.notebook import get_spark_ui\n",
    "help(get_spark_ui)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_spark_ui()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's import our <code>Splice MLFlow Support</code>\n",
    "<blockquote><p class='quotation'><b><br><span style='font-size:20px'> Our Splice MLFlow Support class has a built-in <code>mlflow</code> object, so no need to import anything else. Our mlflow support allows you to access all standard mlflow functionality, along with custom built Splice functions (like database deployment). You can treat the mlflow object just like you would with standard <a href=https://www.mlflow.org/docs/1.6.0/tracking.html>mlflow</a>. Whenever custom Splice Machine functions are used, we will explain beforehand. <br><br>\n",
    "After importing our mlflow support, we will run <code>mlflow.register_splice_context</code>. This allows us to use all of the Splice Machine capabilities like storing artifacts and deploying models to the database.\n",
    "<footer>Splice Machine</footer></blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splicemachine.mlflow_support import *\n",
    "mlflow.register_splice_context(splice) # allow mlflow to use our database connection for artifact storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style='font-size:30px;font-weight:bold'>Loading The Data</h1><br>\n",
    "<blockquote><p class='quotation'><b><br><span style='font-size:25px'>Data Loading</span></b><br><br>Loading data into Splice Machine couldn't be easier, no matter the source. Because we connect directly to our database source, there is no ETL necessary.<footer>Splice Machine</footer></blockquote>\n",
    "\n",
    "\n",
    "### Let's import our data into a Spark DataFrame using our <code>PySpliceContext</code>\n",
    "<blockquote>We can also use beakerx here and set <code>pandas_display_table()</code>to give us interactive tables for pandas dataframes. You can sort, filter, search, and even apply heatmaps and databars to columns in the table. Give it a try by clicking the ellipsis at the top of each column.</blockquote>\n",
    " \n",
    "#### Now is also a good time to create our MLFlow <code>Experiment</code> which we will call fraud_demo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from beakerx.object import beakerx\n",
    "from splicemachine.mlflow_support.utilities import get_user\n",
    "# BeakerX interactive pandas tables\n",
    "beakerx.pandas_display_table()\n",
    "#create our MLFlow experiment\n",
    "mlflow.set_experiment('fraud_demo')\n",
    "schema = get_user()\n",
    "df = splice.df(f\"SELECT * FROM {schema}.cc_fraud_data\")\n",
    "df = df.withColumnRenamed('CLASS_RESULT', 'label')\n",
    "display(df.limit(10).toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can now see our experiment in the MLFlow UI \n",
    "\n",
    "<blockquote>You can always navigate to <a href=/mlflow>/mlflow</a> to view the MLFlow UI, or you can view it right here in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splicemachine.notebook import get_mlflow_ui\n",
    "get_mlflow_ui()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data investigation\n",
    "### Before going further, it's important to look at the correlations between all of your features and each other as well as the label\n",
    "#### We can easily create a heatmap to compare all features against each other and the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from pyspark.sql.types import FloatType, IntegerType\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pdf = df.where(df['label']==1).toPandas().append(df.limit(1000).toPandas())\n",
    "corr = pdf.corr()\n",
    "\n",
    "ticks = [i for i in range(len(corr.columns))]\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=np.bool))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Color Scheme\n",
    "cmap = \"coolwarm\"\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "\n",
    "plt.xticks(ticks, corr.columns)\n",
    "plt.yticks(ticks, corr.columns)\n",
    "plt.title('Fraud Data correlation heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from beakerx.object import beakerx\n",
    "beakerx.pandas_display_table()\n",
    "most_correlated = corr.abs()['label'].sort_values(ascending=False).reset_index()\n",
    "most_correlated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your turn!\n",
    "## You will create a <code>run</code>  keep track of your model's metrics, parameters and other associated data. Get started using <code>mlflow.start_run()</code>. You can set <code>tags</code> to your run such as <code>team</code>, <code>purpose</code>, or anything you'd like to track your runs. You can also set a <code>run_name</code> as a parameter. \n",
    "###### The user_id will automatically be added as the user that is signed into this notebook (currently that's you)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you navigate to the <a href='/mlflow'>MLFlow UI</a> you will now see the fraud-demo experiment, but there is nothing in that experiment yet. Let's start our first <code>run</code> and track our progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start our first MLFlow run\n",
    "from datetime import datetime\n",
    "\n",
    "tags = {\n",
    "        'team': 'Splice Machine',\n",
    "        'purpose': 'fraud r&d'\n",
    "}\n",
    "\n",
    "mlflow.start_run(tags=tags, run_name='RF_run')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's take a look at the MLFlow UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splicemachine.notebook import get_mlflow_ui\n",
    "get_mlflow_ui(experiment_id=mlflow.current_exp_id(), run_id=mlflow.current_run_id())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Details of this dataset:\n",
    "* Because we have so few fraud examples, we need to oversample our fraudulent transactions and undersample the non-fraud transactions\n",
    "* We need to make sure the model isn't overfit and doesn't always predict non-fraud (due to the lack of fraud data) so we can't only rely on [accuracy](https://en.wikipedia.org/wiki/Accuracy_and_precision)\n",
    "* We want to pick a model that doesn't have a high overfitting rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we will define a Machine Learning Pipeline\n",
    "\n",
    "<blockquote>You can use Spark's <code>Pipeline</code> class to define a set of <code>Transformers</code> that get your dataset ready for modeling<br>\n",
    "We'll then use <code>mlflow</code> to <code>log</code> our Pipeline stages. Both <code>log_pipeline_stages</code> and <code>log_feature_transformations</code> are custom Splice Machine functions for tracking Spark Pipelines.</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml import Pipeline,PipelineModel\n",
    "from pyspark.ml.classification import RandomForestClassifier, MultilayerPerceptronClassifier\n",
    "\n",
    "feature_cols = df.columns[:-1]\n",
    "\"\"\"\n",
    "The preprocessing stages for this example are: \n",
    "1) Vector assembling the feature columns \n",
    "2) Standardizing our feature columns\n",
    "\"\"\"\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol='scaledFeatures')\n",
    "rf = RandomForestClassifier(featuresCol = 'scaledFeatures', labelCol = 'label')\n",
    "\n",
    "# Pipeline to preprocess and model our data\n",
    "mlpipe = Pipeline(stages=[assembler,scaler, rf])\n",
    "# Custom Splice functions to add granularity and governance to your Spark Pipeline Models\n",
    "mlflow.log_pipeline_stages(mlpipe)\n",
    "mlflow.log_feature_transformations(mlpipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model setup\n",
    "\n",
    "<blockquote><ul>\n",
    "    <li>Now we can start our machine learning process. We will use Splice's custom <code>OverSampleCrossValidator</code> to properly tune the hyperparameters for our machine learning model while oversampling the minority class during training. We avoid overfitting by not oversampling the validation set.</li>\n",
    "    <li><a href=https://en.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation>Cross Validation</a> is a popular and important Machine Learning practice that helps ensure your model is not overfit to your training data.</li>\n",
    "    <li>While we do that, we'll add just a few lines of code to track all of our moves in MLFlow. We can use Splcie's custom <code>with mlflow.timer('timer_name')</code> block function to track the time it takes to complete a block. Everything in the block will be timed, and then logged to mlflow under the timer name provided to the function. This can we used to time a CrossValidator, a Model, or anything you'd like.</li>\n",
    "    <li>Due to the time it takes to run a Cross Validation, we will only use a few different parameters here, but you can try adding more values to the params to try to improve performance (but don't forget to end this run and start a new one)! This can take anywhere from 1-3 minutes to train, your model is learning a lot.</li>\n",
    "    <li>Make sure you have started a run before using the timer (or any mlflow tracking functions).</li></ul></blockquote>\n",
    "    \n",
    "#### Note: This will create a lot of spark jobs! This may slow down the UI. You can minimize the monitor below by clicking the little arrow on the top left of the orange Spark Jobs bar below the cell.\n",
    "<img src=https://splice-demo.s3.amazonaws.com/Spark+Monitor+Minimize.png></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, CrossValidatorModel\n",
    "from splicemachine.stats import OverSampleCrossValidator\n",
    "\n",
    "# Evaluators to assess model performance\n",
    "PRevaluator = BinaryClassificationEvaluator(metricName = 'areaUnderPR')\n",
    "AUCevaluator = BinaryClassificationEvaluator(metricName = 'areaUnderROC')\n",
    "ACCevaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "F1evaluator = MulticlassClassificationEvaluator(metricName=\"f1\")\n",
    "TPRevaluator = MulticlassClassificationEvaluator(metricName=\"weightedRecall\")\n",
    "\n",
    "params = {rf.maxDepth: [5], \\\n",
    "          rf.numTrees: [10,30], \\\n",
    "          rf.maxBins: [32]}\n",
    "\n",
    "paramGrid_stages = ParamGridBuilder()\n",
    "for param in params:\n",
    "    paramGrid_stages.addGrid(param,params[param])    \n",
    "paramGrid = paramGrid_stages.build()\n",
    "\n",
    "# Custom Splice function to time codeblocks\n",
    "with mlflow.timer('cross_validate'):\n",
    "    # Running the Cross Validation Routine to Determine the best hyperparameter combinations\n",
    "    rf_CV = OverSampleCrossValidator(estimator=mlpipe, estimatorParamMaps=paramGrid, evaluator=AUCevaluator, numFolds=3, altEvaluators = [PRevaluator, ACCevaluator, F1evaluator, TPRevaluator],parallelism=3,seed = 1234)\n",
    "    rf_CVModel = rf_CV.fit(df)\n",
    "# Log the parameters of the best model\n",
    "mlflow.log_model_params(rf_CVModel.bestModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing Metrics\n",
    "<blockquote>Here we can view the different parameter combinations tried and the metrics of those combinations. We'll print all of them out, then grab the metrics from the best performing model to log to mlflow</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "beakerx.pandas_display_table()\n",
    "# Showing the params for the best performing model\n",
    "simpleParamGrid = []\n",
    "for i in range(len(paramGrid)):\n",
    "    simpleParamGrid.append([str(param.name) + ' = '+ str(paramGrid[i][param]) for param in paramGrid[i]])\n",
    "\n",
    "hyperparam_combination_performance = pd.DataFrame(zip(simpleParamGrid,np.array(rf_CVModel.avgMetrics)[:,0],\n",
    "                                                     [i[0] for i in np.array(rf_CVModel.avgMetrics)[:,1]],\n",
    "                                                     [i[1] for i in np.array(rf_CVModel.avgMetrics)[:,1]],\n",
    "                                                     [i[2] for i in np.array(rf_CVModel.avgMetrics)[:,1]],\n",
    "                                                     [i[3] for i in np.array(rf_CVModel.avgMetrics)[:,1]]), columns = ['ParamGrid', 'ROC_AUC','PR_AUC','Accuracy', 'F1', 'TPR'])\n",
    "hyperparam_combination_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab best model's metrics and log\n",
    "best_model_index = np.array(rf_CVModel.avgMetrics)[:,0].argmax()\n",
    "best_model_metrics = hyperparam_combination_performance.iloc[best_model_index].drop('ParamGrid')\n",
    "mlflow.log_metrics(best_model_metrics.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log the less performant models for deeper comparison and governance\n",
    "<blockquote>Even though other parameter combinations didn't perform as well, we can log those to mlflow as <code>nested</code> runs. Nested runs will appear as dropdowns to the original runs, and will have a reference to their parent run</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log all of the hyperparam combinations as nested runs for deeper comparison\n",
    "for i in range(len(rf_CVModel.avgMetrics)):\n",
    "    mlflow.start_run(nested=True)\n",
    "    cv_params = hyperparam_combination_performance.iloc[i]['ParamGrid']\n",
    "    cv_params = dict( (p.split('=')[0],int(p.split(\"=\")[1]) ) for p in cv_params )\n",
    "    mlflow.log_params(cv_params)\n",
    "    metrics = hyperparam_combination_performance.iloc[i].drop('ParamGrid').to_dict()\n",
    "    mlflow.log_metrics(metrics)\n",
    "    mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artifact logging\n",
    "<blockquote>After our Cross Validation completes and we've logged our metrics, we can log some artifacts to mlflow. We will the model itself and the notebook we used (this one!) to build the model.</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the notebook for easy retrieval\n",
    "mlflow.log_artifact('MLManager Fraud Demo.ipynb', 'training_notebook')\n",
    "#Log the best model\n",
    "mlflow.log_model(rf_CVModel.bestModel, 'rf_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artifact loading\n",
    "<blockquote>We can load models back from mlflow by running <code>load_model</code> and we can download any artifacts that we've stored by running <code>download_artifact</code>\n",
    "<br>\n",
    "By default, these functions will retrieve the model/artifact associated with the currently active run. If you pass in a different run_id using the <code>run_id</code> parameter, that will be used</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_pipe = mlflow.load_model(name='rf_model')\n",
    "print(loaded_pipe)\n",
    "# Pass in the name you gave the artifact in the log_artifact function and the download path\n",
    "# You will see the notebook back on the Jupyter home page in the same directory as this notebook\n",
    "mlflow.download_artifact('training_notebook', './downloaded_notebook.ipynb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect our RandomForest\n",
    "<blockquote>Before we finish our run, we can take a look at our Random Forest to see some of the splits it's making. Random Forests are made up of Decision Trees, so we can look at one of the trees with Splice's <code>DecisionTreeVisualizer</code> and get a better understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from splicemachine.stats import DecisionTreeVisualizer as DTV\n",
    "best_rf_model = rf_CVModel.bestModel.stages[-1]\n",
    "DTV.visualize(best_rf_model.trees[0], df.drop('label').columns, ['No_Fraud','Fraud'], visual=True, horizontal=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finish our run\n",
    "<blockquote>Now we'll end our run, and view the results in the <a href=\"/mlflow\">MLFlow UI</a>. We can look at our different runs, the parameters, metrics, tags and artifacts logged, and download our notebook directly. You'll know the run is complete fom the small green check mark on the leftmost side of the run</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executing another run with a different algorithm: a Logistic Regression\n",
    "<blockquote> We are going to now start a new run and try a different model. After running the cells, we will be able to view the results in the UI and compare with our first run</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = {\n",
    "        'team': 'Splice Machine',\n",
    "        'purpose': 'test r&d',\n",
    "       }\n",
    "mlflow.start_run(tags=tags, run_name='LR_run')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Defining a Machine Learning Pipeline \n",
    "Now using a `LogisticRegression` instead of a `RandomForest`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml import Pipeline,PipelineModel\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression\n",
    "\n",
    "feature_cols = df.columns[:-1]\n",
    "\"\"\"\n",
    "The preprocessing stages for this example are: \n",
    "1) Vector assembling the feature columns \n",
    "2) Standardizing our feature columns\n",
    "\"\"\"\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol='scaledFeatures')\n",
    "lr = LogisticRegression(featuresCol = 'scaledFeatures', labelCol = 'label')\n",
    "\n",
    "# Pipeline to preprocess and model our data\n",
    "mlpipe = Pipeline(stages=[assembler,scaler, lr])\n",
    "mlflow.log_pipeline_stages(mlpipe)\n",
    "mlflow.log_feature_transformations(mlpipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the same `OverSamplerCrossValidator` routine to find the best hyperparameters\n",
    "<blockquote>These hyperparameters are different as they are specific to our algorithmic choice in this run.<br>Just like before, this will take a few minutes to run, just think of all the learning!<br>We're trying more hyperparameter combinations in this run, so many more iterations will occur.</blockquote>\n",
    "\n",
    "### More Hyperparameters = Longer Run Time\n",
    "<blockquote>In the last run, we had 1*2*1 = 2 total run combinations. In this run, we have 2*2*1 = 4 total combinations, so you should expect the CrossValidator to take longer.</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluators to assess model performance\n",
    "PRevaluator = BinaryClassificationEvaluator(metricName = 'areaUnderPR')\n",
    "AUCevaluator = BinaryClassificationEvaluator(metricName = 'areaUnderROC')\n",
    "ACCevaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "F1evaluator = MulticlassClassificationEvaluator(metricName=\"f1\")\n",
    "TPRevaluator = MulticlassClassificationEvaluator(metricName=\"weightedRecall\")\n",
    "\n",
    "params = {lr.maxIter: [10, 100], \\\n",
    "          lr.regParam: [0.0,0.3], \\\n",
    "          lr.elasticNetParam: [0.0]}\n",
    "\n",
    "paramGrid_stages = ParamGridBuilder()\n",
    "for param in params:\n",
    "    paramGrid_stages.addGrid(param,params[param])    \n",
    "paramGrid = paramGrid_stages.build()\n",
    "\n",
    "# Custom Splice function to time codeblocks\n",
    "with mlflow.timer('cross_validate'):\n",
    "    # Running the Cross Validation Routine to Determine the best hyperparameter combinations\n",
    "    lr_CV = OverSampleCrossValidator(estimator=mlpipe, estimatorParamMaps=paramGrid, evaluator=AUCevaluator, numFolds=3, altEvaluators = [PRevaluator, ACCevaluator, F1evaluator, TPRevaluator],parallelism=3,seed = 1234)\n",
    "    lr_CVModel = lr_CV.fit(df)\n",
    "    \n",
    "# Log the best parameters \n",
    "mlflow.log_model_params(lr_CVModel.bestModel)\n",
    "mlflow.log_model(lr_CVModel.bestModel, 'LogRegModel')\n",
    "# Store the notebook for easy retrieval\n",
    "mlflow.log_artifact('MLManager Fraud Demo.ipynb', 'training_notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "beakerx.pandas_display_table()\n",
    "# Showing the params for the best performing model\n",
    "simpleParamGrid = []\n",
    "for i in range(len(paramGrid)):\n",
    "    simpleParamGrid.append([str(param.name) + ' = '+ str(paramGrid[i][param]) for param in paramGrid[i]])\n",
    "\n",
    "hyperparam_combination_performance = pd.DataFrame(zip(simpleParamGrid,np.array(lr_CVModel.avgMetrics)[:,0],\n",
    "                                                     [i[0] for i in np.array(lr_CVModel.avgMetrics)[:,1]],\n",
    "                                                     [i[1] for i in np.array(lr_CVModel.avgMetrics)[:,1]],\n",
    "                                                     [i[2] for i in np.array(lr_CVModel.avgMetrics)[:,1]],\n",
    "                                                     [i[3] for i in np.array(lr_CVModel.avgMetrics)[:,1]]), columns = ['ParamGrid', 'ROC_AUC','PR_AUC','Accuracy', 'F1', 'TPR'])\n",
    "hyperparam_combination_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Grab best model's metrics\n",
    "best_model_index = np.array(lr_CVModel.avgMetrics)[:,0].argmax()\n",
    "best_model_metrics = hyperparam_combination_performance.iloc[best_model_index].drop('ParamGrid')\n",
    "mlflow.log_metrics(best_model_metrics.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log all of the hyperparam combinations as nested runs for deeper comparison\n",
    "for i in range(len(lr_CVModel.avgMetrics)):\n",
    "    mlflow.start_run(nested=True)\n",
    "    cv_params = hyperparam_combination_performance.iloc[i]['ParamGrid']\n",
    "    cv_params = dict( (p.split('=')[0],p.split(\"=\")[1] ) for p in cv_params )\n",
    "    mlflow.log_params(cv_params)\n",
    "    metrics = hyperparam_combination_performance.iloc[i].drop('ParamGrid').to_dict()\n",
    "    mlflow.log_metrics(metrics)\n",
    "    mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Logistic Regression\n",
    "<blockquote>Before ending our run, we can look at the ROC Curve of this model. The <a href=https://en.wikipedia.org/wiki/Receiver_operating_characteristic>Receiver Operating Characteristic Curve</a> is created by plotting your True Positive Rate against your False Positive Rate. <br>Ideally you want your graph to be as high as possible on the Y axis before increasing on the X axis (lot's of True Positives without many False Positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splicemachine.stats import SpliceBinaryClassificationEvaluator\n",
    "from matplotlib import pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "ev = SpliceBinaryClassificationEvaluator(spark)\n",
    "ev.plotROC(lr_CVModel.bestModel.stages[-1], ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's make this more modular\n",
    "<blockquote>As you may have noticed, the code for these 2 runs was pretty similar. Let's make this code a bit more modular and create some functions for training and logging of CV models so we can test some new runs</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainCV(model_name, model, model_params):\n",
    "    tags = {'team': 'Splice Machine','purpose': 'fraud r&d'}\n",
    "    mlflow.start_run(run_name = f'{model_name}_run', tags=tags)\n",
    "    \n",
    "    \n",
    "    # Pipeline to preprocess and model our data\n",
    "    mlpipe = Pipeline(stages=[assembler,scaler, model])\n",
    "    mlflow.log_feature_transformations(mlpipe)\n",
    "    mlflow.log_pipeline_stages(mlpipe)\n",
    "    \n",
    "    # Set our param grid for the cross validation\n",
    "    paramGrid_stages = ParamGridBuilder()\n",
    "    for param in model_params:\n",
    "        paramGrid_stages.addGrid(param,model_params[param])    \n",
    "    paramGrid = paramGrid_stages.build()\n",
    "    \n",
    "    # Custom Splice function to time codeblocks\n",
    "    with mlflow.timer('cross_validate'):\n",
    "        # Running the Cross Validation Routine to Determine the best hyperparameter combinations\n",
    "        cv_model = OverSampleCrossValidator(estimator=mlpipe, estimatorParamMaps=paramGrid, evaluator=AUCevaluator, numFolds=3, altEvaluators = [PRevaluator, ACCevaluator, F1evaluator, TPRevaluator],parallelism=3,seed = 1234)\n",
    "        print('Training Model')\n",
    "        trained_CVModel = cv_model.fit(df)\n",
    "    # Log params model params notebook of best model\n",
    "    mlflow.log_model_params(trained_CVModel.bestModel)\n",
    "    mlflow.log_model(trained_CVModel.bestModel, f'{model_name}_Model')\n",
    "    mlflow.log_artifact('MLManager Fraud Demo.ipynb', 'training_notebook')\n",
    "    return trained_CVModel, paramGrid\n",
    "\n",
    "def log_cv_results(trained_CVModel, param_grid):\n",
    "    # Construct our dataframe of params\n",
    "    metrics = [[i[0],*i[1]] for i in trained_cv.avgMetrics]\n",
    "    param_df = pd.DataFrame(zip(simpleParamGrid,metrics))\n",
    "    hyperparam_combination_performance = pd.DataFrame(metrics, columns = ['ROC_AUC','PR_AUC','Accuracy', 'F1', 'TPR'])\n",
    "    hyperparam_combination_performance['ParamGrid'] = param_df[0]\n",
    "    \n",
    "    # Grab best model's metrics\n",
    "    best_model_index = np.array(trained_CVModel.avgMetrics)[:,0].argmax()\n",
    "    best_model_metrics = hyperparam_combination_performance.iloc[best_model_index].drop('ParamGrid')\n",
    "    mlflow.log_metrics(best_model_metrics.to_dict())\n",
    "    \n",
    "    # Log all of the hyperparam combinations as nested runs for deeper comparison\n",
    "    for i in range(len(trained_CVModel.avgMetrics)):\n",
    "        mlflow.start_run(nested=True)\n",
    "        cv_params = hyperparam_combination_performance.iloc[i]['ParamGrid']\n",
    "        cv_params = dict( (p.split('=')[0],p.split(\"=\")[1] ) for p in cv_params )\n",
    "        mlflow.log_params(cv_params)\n",
    "        metrics = hyperparam_combination_performance.iloc[i].drop('ParamGrid').to_dict()\n",
    "        mlflow.log_metrics(metrics)\n",
    "        mlflow.end_run()\n",
    "    mlflow.end_run() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LinearSVC, GBTClassifier\n",
    "\n",
    "gbt = GBTClassifier(featuresCol = 'scaledFeatures', labelCol = 'label')\n",
    "gbt_params = {gbt.maxDepth: [6,12], gbt.maxIter: [5,10], gbt.maxBins: [40]}\n",
    "svc = LinearSVC(featuresCol = 'scaledFeatures', labelCol = 'label')\n",
    "svc_params = {svc.maxIter: [25,50], svc.regParam: [0.0,0.3], svc.threshold: [10]}\n",
    "\n",
    "models = {\"GBT\": [gbt,gbt_params],\"SVC\": [svc,svc_params]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the CV for each model\n",
    "<blockquote>This cell is going to take some time, after all you're doing real machine learning! Remember, the top left arrow of the Spark Jobs monitor will minimize the window, which you may want to use as a large number of jobs are going to be displayed.</blockquote>\n",
    "\n",
    "### Sit back, grab some coffee, and see your algorithms learn!\n",
    "<blockquote>You can always check out the <a href=\"/mlflow\">MLFlow UI</a> and refresh to see the new runs trickle in, or watch the Spark UI to see how your ML models are being built under the hood.</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_spark_ui()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from tqdm.notebook import tqdm\n",
    "for name in tqdm(models):\n",
    "    print(f\"Starting: {name}_run ...\")\n",
    "    trained_cv, paramGrid = trainCV(name,models[name][0],models[name][1])\n",
    "    log_cv_results(trained_cv, paramGrid)\n",
    "    print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's deploy our LogisticRegression\n",
    "<blockquote>\n",
    "    Now we are going to deploy our model to the database! This is custom Splice Machine functionality that allows you to take your model and apply it to a table, so as data streams into that table, your model will be automatically run on the new rows of data, and predictions will be stored. No needing to worry about external endpoints or APIs, simply treat the model as a table, inserting rows for new predictions, and selecting rows to see the results. No app development or IT needed! It takes just a few seconds as you'll see below.\n",
    "</blockquote>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy our model to the Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "drop table if exists fraudmodel;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Get run ID from mlflow\n",
    "run_id = mlflow.get_run_ids_by_name('LR_run')[0]\n",
    "# Enter the schema name of the model to deploy to. You can use your username that you logged in with as the schema.\n",
    "jid = mlflow.deploy_db(schema, 'fraudmodel', run_id, primary_key={'MOMENT_KEY':'INTEGER'}, df=df.drop('label'), create_model_table=True, classes=['no_fraud','fraud'])\n",
    "mlflow.watch_job(jid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sure the table is empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "select count(*) from fraudmodel;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert data into our deployed model table and immediately view results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%sql\n",
    "\n",
    "insert into fraudmodel (TIME_OFFSET,EXPECTED_WEEKLY_TRANS_CNT,EXPECTED_WEEKLY_TRANS_AMNT,EXPECTED_DAILY_TRANS_CNT,EXPECTED_DAILY_TRANS_AMNT,WEEKLY_TRANS_CNT,WEEKLY_TRANS_AMNT,DAILY_TRANS_CNT,DAILY_TRANS_AMNT,ROLLING_AVG_WEEKLY_TRANS_CNT,ROLLING_AVG_WEEKLY_TRANS_AMNT,ROLLING_AVG_DAILY_TRANS_CNT,ROLLING_AVG_DAILY_TRANS_AMNT,MACD_TRANS_AMNT,MACD_TRANS_CNT,RSI_TRANS_AMNT,RSI_TRANS_CNT,AROON_TRANS_AMNT,AROON_TRANS_CNT,ADX_TRANS_AMNT,ADX_TRANS_CNT,CURRENT_BALANCE,ROLLING_AVG_BALANCE,MACD_BALANCE,AROON_BALANCE,RSI_BALANCE,ADX_BALANCE,CREDIT_SCORE,CREDIT_LIMIT,AMOUNT,MOMENT_KEY)\n",
    "    values(0, 7.768,2.566,2.496,7.43,2.488,8.762,9.601,5.847,4.957,7.979,4.957,2.281,9.715,4.086,7.222,5.017,3.844,9.683,6.43,1.439,5.908,3.563,1.116,7.097,8.717,0.06,2.273,4.846, 10000.22, 5);\n",
    "insert into fraudmodel (TIME_OFFSET,EXPECTED_WEEKLY_TRANS_CNT,EXPECTED_WEEKLY_TRANS_AMNT,EXPECTED_DAILY_TRANS_CNT,EXPECTED_DAILY_TRANS_AMNT,WEEKLY_TRANS_CNT,WEEKLY_TRANS_AMNT,DAILY_TRANS_CNT,DAILY_TRANS_AMNT,ROLLING_AVG_WEEKLY_TRANS_CNT,ROLLING_AVG_WEEKLY_TRANS_AMNT,ROLLING_AVG_DAILY_TRANS_CNT,ROLLING_AVG_DAILY_TRANS_AMNT,MACD_TRANS_AMNT,MACD_TRANS_CNT,RSI_TRANS_AMNT,RSI_TRANS_CNT,AROON_TRANS_AMNT,AROON_TRANS_CNT,ADX_TRANS_AMNT,ADX_TRANS_CNT,CURRENT_BALANCE,ROLLING_AVG_BALANCE,MACD_BALANCE,AROON_BALANCE,RSI_BALANCE,ADX_BALANCE,CREDIT_SCORE,CREDIT_LIMIT,AMOUNT,MOMENT_KEY)\n",
    "    values(1, -5.577,-9.337,-7.048,-0.761,-2.862,-3.333,-7.063,-5.609,-2.905,-8.311,-3.427,-7.309,-0.231,-3.197,-0.681,-6.836,-3.565,-4.843,-5.158,-0.268,-5.082,-1.777,-1.195,-3.64,-2.74,-5.08,-2.38,-8.941,9284.22, 6);\n",
    "\n",
    "select * from fraudmodel;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "select * from fraudmodel;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's deploy to an existing table\n",
    "<blockquote>\n",
    "    We just deployed a model as a table in the database. That's great! But sometimes, you want to transform an existing table into an intelligent one. You can do that to using the same function. We are going to deploy this model to the table that we ingested our data from. The function will alter that original table, adding columns and triggers as necessary. <br>\n",
    "    What's also important is that you can specify which columns from the table are going to be used in the model. Some tables have more columns than what the model needs, and using the <code>model_cols</code> parameter you can specify which go into the model. <br><b>Let's try it out</b>\n",
    "\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Feature columns from the table\n",
    "feature_vector = ['TIME_OFFSET','EXPECTED_WEEKLY_TRANS_CNT','EXPECTED_WEEKLY_TRANS_AMNT','EXPECTED_DAILY_TRANS_CNT','EXPECTED_DAILY_TRANS_AMNT','WEEKLY_TRANS_CNT','WEEKLY_TRANS_AMNT','DAILY_TRANS_CNT','DAILY_TRANS_AMNT','ROLLING_AVG_WEEKLY_TRANS_CNT','ROLLING_AVG_WEEKLY_TRANS_AMNT','ROLLING_AVG_DAILY_TRANS_CNT','ROLLING_AVG_DAILY_TRANS_AMNT','MACD_TRANS_AMNT','MACD_TRANS_CNT','RSI_TRANS_AMNT','RSI_TRANS_CNT','AROON_TRANS_AMNT','AROON_TRANS_CNT','ADX_TRANS_AMNT','ADX_TRANS_CNT','CURRENT_BALANCE','ROLLING_AVG_BALANCE','MACD_BALANCE','AROON_BALANCE','RSI_BALANCE','ADX_BALANCE','CREDIT_SCORE','CREDIT_LIMIT','AMOUNT']\n",
    "# Get run ID from mlflow\n",
    "run_id = mlflow.get_run_ids_by_name('LR_run')[0]\n",
    "jid = mlflow.deploy_db(schema,'cc_fraud_data', run_id, classes=['not_fraud','fraud'], model_cols=feature_vector)\n",
    "mlflow.watch_job(jid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "insert into cc_fraud_data (TIME_OFFSET,EXPECTED_WEEKLY_TRANS_CNT,EXPECTED_WEEKLY_TRANS_AMNT,EXPECTED_DAILY_TRANS_CNT,EXPECTED_DAILY_TRANS_AMNT,WEEKLY_TRANS_CNT,WEEKLY_TRANS_AMNT,DAILY_TRANS_CNT,DAILY_TRANS_AMNT,ROLLING_AVG_WEEKLY_TRANS_CNT,ROLLING_AVG_WEEKLY_TRANS_AMNT,ROLLING_AVG_DAILY_TRANS_CNT,ROLLING_AVG_DAILY_TRANS_AMNT,MACD_TRANS_AMNT,MACD_TRANS_CNT,RSI_TRANS_AMNT,RSI_TRANS_CNT,AROON_TRANS_AMNT,AROON_TRANS_CNT,ADX_TRANS_AMNT,ADX_TRANS_CNT,CURRENT_BALANCE,ROLLING_AVG_BALANCE,MACD_BALANCE,AROON_BALANCE,RSI_BALANCE,ADX_BALANCE,CREDIT_SCORE,CREDIT_LIMIT,AMOUNT)\n",
    "    values(990773, 7.768,2.566,2.496,7.43,2.488,8.762,9.601,5.847,4.957,7.979,4.957,2.281,9.715,4.086,7.222,5.017,3.844,9.683,6.43,1.439,5.908,3.563,1.116,7.097,8.717,0.06,2.273,4.846, 10000.22);\n",
    "insert into cc_fraud_data (TIME_OFFSET,EXPECTED_WEEKLY_TRANS_CNT,EXPECTED_WEEKLY_TRANS_AMNT,EXPECTED_DAILY_TRANS_CNT,EXPECTED_DAILY_TRANS_AMNT,WEEKLY_TRANS_CNT,WEEKLY_TRANS_AMNT,DAILY_TRANS_CNT,DAILY_TRANS_AMNT,ROLLING_AVG_WEEKLY_TRANS_CNT,ROLLING_AVG_WEEKLY_TRANS_AMNT,ROLLING_AVG_DAILY_TRANS_CNT,ROLLING_AVG_DAILY_TRANS_AMNT,MACD_TRANS_AMNT,MACD_TRANS_CNT,RSI_TRANS_AMNT,RSI_TRANS_CNT,AROON_TRANS_AMNT,AROON_TRANS_CNT,ADX_TRANS_AMNT,ADX_TRANS_CNT,CURRENT_BALANCE,ROLLING_AVG_BALANCE,MACD_BALANCE,AROON_BALANCE,RSI_BALANCE,ADX_BALANCE,CREDIT_SCORE,CREDIT_LIMIT,AMOUNT)\n",
    "    values(990774, -5.577,-9.337,-7.048,-9.761,-2.862,-3.333,-7.063,-5.609,-2.905,-8.311,-3.427,-7.309,-0.231,-3.197,-100.681,-6.836,-3.565,-4.843,-5.158,-0.268,-5.082,-1.777,-1.195,-3.64,-2.74,-5.08,-2.38,-8.941,9284.22);\n",
    "\n",
    "select * from cc_fraud_data where time_offset = 990773 or time_offset = 990774;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see which models are currently deployed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.get_deployed_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazing!\n",
    "### Let's recap what we just accomplished. We:\n",
    "<blockquote>\n",
    "    <ul>\n",
    "        <li>Imported a large dataset from S3 using SQL and Splice Machine stored procedures</li>\n",
    "        <li>Inspected that data both in sql and python (with pandas and spark) in the same notebook</li>\n",
    "        <li>Created a machine learning preprocessing pipeline to create feature vectors</li>\n",
    "        <li>Built a Cross Validator model (that Over Samples your training dataset) to compare different hyperparameters for a Random Forest and Logistic Regression</li>\n",
    "        <li>Saved the parameters, metrics, models, and training notebooks associated with each run to MLFlow and compared metrics in the UI</li>\n",
    "        <li>Modularized our code into functions that we when called to train a Gradient Boosted Tree and a Support Vector Machine</li>\n",
    "        <li>Inspected all of our runs in the MLFlow UI and downloaded notebooks that we wanted to look at further</li>\n",
    "        <li>Deployed our Logistic Regression model directly to a table in the database</li>\n",
    "        <li>Inserted rows of data using raw SQL and saw immediate predictions on those rows without the need for endpoints, REST APIs, or messy configuration</li>\n",
    "        <li>Deployed the same model to an <b>existing</b> table and watched it transform into an <b>intelligent</b> table</li>\n",
    "        <li> Tracked which models were in production </li>\n",
    "        <li>Never left the notebook for the entirety of our Data Science process</li>\n",
    "     </ul>\n",
    "</blockquote>\n",
    "\n",
    "### That's quite the job well done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
