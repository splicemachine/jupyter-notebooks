{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "#s {\n",
    "}\n",
    "h1, h2, h3, h4, h5, h6, table, button, a, p, blockquote {\n",
    "font-family:Geneva;\n",
    "}\n",
    "\n",
    ".log {\n",
    "transition: all .2s ease-in-out;\n",
    "}\n",
    "\n",
    ".log:hover {a\n",
    "transform: scale(1.05);\n",
    "}\n",
    "</style>\n",
    "<div id='s' style='width:100%'>\n",
    "<center><img class='log' src='https://splicemachine.com/wp-content/uploads/splice-logo-1.png' width='20%' style='z-index:5'></center>\n",
    "<center><h1 class='log' style='font-size:50px; color:black;'>Welcome to Splice Machine MLManager</h1></center>\n",
    "<center><h2 class = 'log' style='font-size:25px; color:grey;'>The data platform for intelligent applications</center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "blockquote{\n",
    "  font-size: 15px;\n",
    "  background: #f9f9f9;\n",
    "  border-left: 10px solid #ccc;\n",
    "  margin: .5em 10px;\n",
    "  padding: 30em, 10px;\n",
    "  quotes: \"\\201C\"\"\\201D\"\"\\2018\"\"\\2019\";\n",
    "  padding: 10px 20px;\n",
    "  line-height: 1.4;\n",
    "}\n",
    "\n",
    "blockquote:before {\n",
    "  content: open-quote;\n",
    "  display: inline;\n",
    "  height: 0;\n",
    "  line-height: 0;\n",
    "  left: -10px;\n",
    "  position: relative;\n",
    "  top: 30px;\n",
    "  bottom:30px;\n",
    "  color: #ccc;\n",
    "  font-size: 3em;\n",
    "    display:none;\n",
    "\n",
    "}\n",
    "\n",
    "p{\n",
    "  margin: 0;\n",
    "}\n",
    "\n",
    "footer{\n",
    "  margin:0;\n",
    "  text-align: right;\n",
    "  font-size: 1em;\n",
    "  font-style: italic;\n",
    "}\n",
    "</style>\n",
    "<blockquote><p class='quotation'><b><br><span style='font-size:25px'>Why use Splice Machine ML</span></b><br><br>Splice Machine ML isn't just a machine learning platform, it is a complete machine learning lifecycle management solution, giving you total control of your models, from retrieving data to scalable deployment.  <br><br>\n",
    "    <center><img class='log' src='https://s3.amazonaws.com/splice-demo/splice-machine-data-science-process.png' width='40%' style='z-index:5'></center>\n",
    "    <br><ul><li>Our platform runs directly on Apache Spark, allowing you to complete massive jobs in parallel</li><li>Our native <code>PySpliceContext</code> lets you directly access the data in your database and convert as a Spark DataFrame, no ETL.</li><li><code>MLFlow</code> is integrated directly into all Splice Machine clusters, allowing you to keep track of your entire Data Science workflow</li><li>After you have found the best model for your task, you can easily deploy it live to AWS SageMaker or AzureML to make predictions in real time.</li><li>MLFlow does not force a standard workflow, instead it allows teams to develop their own methodology easily that fits their teams and problems</li></ul><br>In this demo we will guide you through the entire MLManager life cycle.<br></p><footer>Your friends at Splice Machine</footer></blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style='font-size:25px;  font-weight:bold'>How does this work?</h2>\n",
    "<style>\n",
    "blockquote{\n",
    "  font-size: 15px;\n",
    "  background: #f9f9f9;\n",
    "  border-left: 10px solid #ccc;\n",
    "  margin: .5em 10px;\n",
    "  padding: 30em, 10px;\n",
    "  quotes: \"\\201C\"\"\\201D\"\"\\2018\"\"\\2019\";\n",
    "  padding: 10px 20px;\n",
    "  line-height: 1.4;\n",
    "}\n",
    "\n",
    "blockquote:before {\n",
    "  content: open-quote;\n",
    "  display: inline;\n",
    "  height: 0;\n",
    "  line-height: 0;\n",
    "  left: -10px;\n",
    "  position: relative;\n",
    "  top: 30px;\n",
    "  bottom:30px;\n",
    "  color: #ccc;\n",
    "  font-size: 3em;\n",
    "    display:none;\n",
    "\n",
    "}\n",
    "\n",
    "p{\n",
    "  margin: 0;\n",
    "}\n",
    "\n",
    "footer{\n",
    "  margin:0;\n",
    "  text-align: right;\n",
    "  font-size: 1em;\n",
    "  font-style: italic;\n",
    "}\n",
    "</style>\n",
    "<blockquote><p class='quotation'><b><br><span style='font-size:25px'>Jupyter</span></b> <br><br>Jupyter notebooks are a simple, easy and intuitive way to do data science, directly in your browser. Any Spark computations you run inside of the notebook are executed right on your cluster's Spark executors.<br><br>Jupyter notebooks also make machine learning easier. By using Jupyter <i>magics</i>, you can run different languages inside the same notebook. The language you want to run is signified by a %% sign followed by a magic at the top of a cell. For example, one of the interpreters you will become very familiar with while using our platform is <code>%%sql</code> magic. In the <code>%%sql</code> magic you can run standard SQL queries and visualize the results in Jupyter's built in visualization tools.<br> <br><i>This entire demo was written inside a Jupyter notebook</i></br><footer>Splice Machine</footer></blockquote><br>\n",
    "<blockquote><p class='quotation'><b><br><span style='font-size:25px'>MLFlow</span></b><br><br>As a data scientist constantly creating new models and testing new features, it is necessary to effectively track and manage those different ML runs. MLFlow allows you to track entire <code>experiments</code> and individual <code>run</code> parameters and metrics. The way you organize your flow is unique to you, and the intuitive Python API allows you to organize your development process and run with it.<br>\n",
    "     <center><img class='log' src='https://s3.amazonaws.com/splice-demo/mlflow+ui.png' width='40%' style='z-index:5'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<center><h1 style='font-size:35px;'>Ready? Let's get started<b style='font-size:35px'>.</b></h1></center>\n",
    "\n",
    "## Problem statement:\n",
    "### Can we predict the likelihood of fraudulent transactions after training on historical actuals? \n",
    "#### We're going to find out using Splice Machine's <code>MLManager</code>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "drop table if exists cc_fraud_data; \n",
    "create table cc_fraud_data (\n",
    "    time_offset integer,\n",
    "    v1 double,\n",
    "    v2 double,\n",
    "    v3 double,\n",
    "    v4 double,\n",
    "    v5 double,\n",
    "    v6 double,\n",
    "    v7 double,\n",
    "    v8 double,\n",
    "    v9 double,\n",
    "    v10 double,\n",
    "    v11 double,\n",
    "    v12 double,\n",
    "    v13 double,\n",
    "    v14 double,\n",
    "    v15 double,\n",
    "    v16 double,\n",
    "    v17 double,\n",
    "    v18 double,\n",
    "    v19 double,\n",
    "    v20 double,\n",
    "    v21 double,\n",
    "    v22 double,\n",
    "    v23 double,\n",
    "    v24 double,\n",
    "    v25 double,\n",
    "    v26 double,\n",
    "    v27 double,\n",
    "    v28 double,\n",
    "    amount decimal(10,2),\n",
    "    class_result int\n",
    ");\n",
    "\n",
    "call SYSCS_UTIL.IMPORT_DATA (\n",
    "     null,\n",
    "     'cc_fraud_data',\n",
    "     null,\n",
    "     's3a://splice-demo/kaggle-fraud-data/creditcard.csv',\n",
    "     ',',\n",
    "     null,\n",
    "     null,\n",
    "     null,\n",
    "     null,\n",
    "     -1,\n",
    "     's3a://splice-demo/kaggle-fraud-data/bad',\n",
    "     null, \n",
    "     null);\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "select top 10 * from cc_fraud_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "%%time\n",
    "select class_result, count(*) from cc_fraud_data group by 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "explain select class_result, count(*) from cc_fraud_data group by 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style='font-size:30px;font-weight:bold'>Connecting to your database</h1><br>\n",
    "<blockquote><p class='quotation'><b><br><span style='font-size:15px'>Now, let's establish a connection to your database using Python via our <a href=\"https://www.splicemachine.com/the-splice-machine-native-spark-datasource/\">Native Spark Datasource</a>. We will use the <code>PySpliceContext</code> to establish our direct connection-- it allows us to do inserts, selects, upserts, updates and many more functions without serialization<footer>Splice Machine</footer></blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from splicemachine.spark.context import PySpliceContext\n",
    "# Create our Spark Session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "# Create out Native Database Connection\n",
    "splice = PySpliceContext(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create our <code>MLManager</code>\n",
    "<blockquote><p class='quotation'><b><br><span style='font-size:20px'> When you create an MLManager object, a tracking URL is returned to you. There is one tracking URL _per cluster_ so if you create another one in a new notebook, it will return the same tracking URL. This is useful because you can create multiple different experiments across all notebooks, and all will be tracked in the MLFlow UI.\n",
    "\n",
    "<footer>Splice Machine</footer></blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splicemachine.ml.management import MLManager\n",
    "manager = MLManager(splice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style='font-size:30px;font-weight:bold'>Loading The Data</h1><br>\n",
    "<blockquote><p class='quotation'><b><br><span style='font-size:25px'>Data Loading</span></b><br><br>Loading data into Splice Machine couldn't be easier, no matter the source. Because we connect directly to our database source, there is no ETL necessary.<footer>Splice Machine</footer></blockquote>\n",
    "\n",
    "\n",
    "### Let's import our data into a Spark DataFrame using our <code>PySpliceContext</code>\n",
    "#### Now is also a good time to create our MLFlow <code>Experiment</code> which we will call fraud_demo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create our MLFlow experiment\n",
    "manager.create_experiment('fraud_demo')\n",
    "df = splice.df(\"SELECT * FROM REPLACE_ME_DBSCHEMA.cc_fraud_data\")\n",
    "df = df.withColumnRenamed('CLASS_RESULT', 'label')\n",
    "display(df.limit(10).toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can now see our experiment in the MLFlow UI at port <code>5001</code>\n",
    "<center><img class='log' src='https://s3.amazonaws.com/splice-demo/mlflow_UI_fraud.png' width='60%' style='z-index:5'></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data investigation\n",
    "### Before going further, it's important to look at the correlations between all of your features and each other as well as the label\n",
    "#### We can easily create a heatmap to compare all features against each other and the label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "for i in df.columns:\n",
    "    df = df.withColumn(i,df[i].cast(FloatType()))\n",
    "\n",
    "pdf = df.limit(5000).toPandas()\n",
    "correlations = pdf.corr()\n",
    "correlations.style.set_precision(2)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (8,12)\n",
    "plt.matshow(correlations, cmap='coolwarm')\n",
    "\n",
    "ticks = [i for i in range(len(correlations.columns))]\n",
    "plt.xticks(ticks, correlations.columns)\n",
    "plt.yticks(ticks, correlations.columns)\n",
    "\n",
    "\n",
    "plt.title('Fraud Data correlation heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ben's run\n",
    "### Ben, our first Data Scientist, has an idea for the steps to build this model. He will create a <code>run</code> and log his name as to keep track of what he did\n",
    "#### <code>manager.start_run()</code>\n",
    "##### You can set <code>tags</code> to your run such as <code>team</code>, <code>purpose</code>, or anything you'd like to track your runs. You can also set a <code>run_name</code> as a parameter. \n",
    "###### The user_id will automatically be added as the user that is signed into this notebook (currently that's me, Ben)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you navigate to the mlflow port you will now see the fraud-demo experiment, but there is nothing in that experiment yet. Let's start our first <code>run</code> and track our progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start our first MLFlow run\n",
    "tags = {\n",
    "        'team': 'Splice Machine',\n",
    "        'purpose': 'fraud r&d',\n",
    "        'attempt-date': '11/07/2019',\n",
    "        'attempt-number': '1'\n",
    "       }\n",
    "manager.start_run(tags=tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's look at some of the attributes of this dataset:\n",
    "* Because we have so few fraud examples, we need to oversample our fraudulent transactions and undersample the non-fraud transactions\n",
    "* We need to make sure the model isn't overfit and doesn't always predict non-fraud (due to the lack of fraud data) so we can't only rely on accuracy\n",
    "* We want to pick a model that doesn't have a high overfitting rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's define our Pipeline\n",
    "\n",
    "You can use Spark's <code>Pipeline</code> class to define a set of <code>Transformers</code> that set up your dataset for modeling<br>\n",
    "We'll then use <code>MLManager</code> to <code>log</code> our Pipeline stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml import Pipeline,PipelineModel\n",
    "from pyspark.ml.classification import RandomForestClassifier, MultilayerPerceptronClassifier\n",
    "\n",
    "feature_cols = df.columns[:-1]\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol='scaledFeatures')\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "stages = [assembler,scaler,rf]\n",
    "mlpipe = Pipeline(stages=stages)\n",
    "manager.log_pipeline_stages(mlpipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model setup\n",
    "\n",
    "Now we can set up our modeling process. We will use our <code>OverSampleCrossValidator</code> to properly oversample our dataset for model building.<br>\n",
    "While we do that, we'll add just a few lines of code to track all of our moves in MLFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the CV\n",
    "\n",
    "Now we can run the CrossValidator and log the results to MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from splicemachine.ml.utilities import SpliceBinaryClassificationEvaluator\n",
    "rf_depth = [5,8,10,12]\n",
    "rf_trees = [5,7,9,11]\n",
    "rf_subsampling_rate = [1.0,0.9,0.8]\n",
    "oversample_rate = [0.4,0.7,1.0]\n",
    "\n",
    "for i in range(1,5):\n",
    "    tags = {\n",
    "        'team': 'Splice Machine',\n",
    "        'purpose': 'fraud r&d',\n",
    "        'attempt-date': '11/07/2019',\n",
    "        'attempt-number': f'{i}'\n",
    "       }\n",
    "    manager.start_run(tags=tags)\n",
    "    \n",
    "    #random variable choice\n",
    "    depth = random.choice(rf_depth)\n",
    "    trees = random.choice(rf_trees)\n",
    "    subsamp_rate = random.choice(rf_subsampling_rate)\n",
    "    ovrsmpl_rate = random.choice(oversample_rate)\n",
    "    \n",
    "    #transformers\n",
    "    feature_cols = df.columns[:-1]\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
    "    scaler = StandardScaler(inputCol=\"features\", outputCol='scaledFeatures')\n",
    "    rf = RandomForestClassifier(maxDepth=depth, numTrees=trees, subsamplingRate=subsamp_rate)\n",
    "    \n",
    "    #pipeline\n",
    "    stages = [assembler,scaler,rf]\n",
    "    mlpipe = Pipeline(stages=stages)\n",
    "    #log the stages of the pipeline\n",
    "    manager.log_pipeline_stages(mlpipe)\n",
    "    #log what happens to each feature\n",
    "    manager.log_feature_transformations(mlpipe)\n",
    "    \n",
    "    #run on the data\n",
    "    train, test = df.randomSplit([0.8,0.2])\n",
    "    manager.start_timer(f'CV iteration {i}')\n",
    "    trainedModel = mlpipe.fit(train)\n",
    "    execution_time = manager.log_and_stop_timer()\n",
    "    print(f\"--- {execution_time} ms == {execution_time/1000} seconds == {execution_time/1000/60} minutes\")\n",
    "\n",
    "    #log model parameters\n",
    "    manager.log_model_params(trainedModel)\n",
    "    preds = trainedModel.transform(test)\n",
    "    #evaluate\n",
    "    evaluator = SpliceBinaryClassificationEvaluator(spark)\n",
    "    evaluator.input(preds)\n",
    "    metrics = evaluator.get_results(dict=True)\n",
    "    #log model performance\n",
    "    manager.log_metrics(list(metrics.items()))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can login to the manager to deploy spark models from code\n",
    "#Fill in below with your DB login (same as Jupyter)\n",
    "USER = ''\n",
    "PWD = ''\n",
    "manager.login_director(USER,PWD)\n",
    "#This serializes the model into the database\n",
    "manager.log_spark_model(trainedModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "select NAME,RUN_UUID from ARTIFACTS;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If we want to deploy, we can run\n",
    "#manager.deploy_aws('my_fraud_model')\n",
    "#or\n",
    "#manager.deploy_azure(<unique_endpoint_name>, 'my_resource_group', 'my_workspace')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
