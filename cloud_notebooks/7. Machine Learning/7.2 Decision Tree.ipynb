{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup-- \n",
    "import os\n",
    "import pyspark\n",
    "from splicemachine.spark.context import PySpliceContext\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# make sure pyspark tells workers to use python3 not 2 if both are installed\n",
    "jdbc_host = os.environ['JDBC_HOST']\n",
    "\n",
    "conf = pyspark.SparkConf()\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
    "\n",
    "splicejdbc=f\"jdbc:splice://{jdbc_host}:1527/splicedb;user=splice;password=admin\"\n",
    "\n",
    "splice = PySpliceContext(spark, splicejdbc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%scala\n",
    "%%spark <> --noUI\n",
    "import java.net.InetAddress\n",
    "val driver_host = InetAddress.getLocalHost.getHostAddress\n",
    "SparkSession.builder()\n",
    "\t.appName(\"jt1test2\")\n",
    "\t.master(\"k8s://https://kubernetes.default.svc.cluster.local:443\")\n",
    "\t.config(\"spark.kubernetes.container.image\", \"splicemachine/sm_k8_spark:0.0.4\")\n",
    "\t.config(\"spark.executor.instances\", \"2\")\n",
    "\t.config(\"spark.submit.deployMode\", \"cluster\")\n",
    "\t.config(\"spark.submit.deployMode\", \"cluster\")\n",
    "\t.config(\"spark.driver.extraClassPath\", \"/opt/spark/conf:/opt/spark/jars/*\")\n",
    "\t.config(\"spark.executor.extraClassPath\", \"./:/opt/hbase/conf:/opt/splicemachine/lib/*:/opt/spark/jars/*:/opt/hbase/lib/*\")\n",
    "\t.config(\"splice.spark.executor.extraLibraryPath\", \"/opt/native\")\n",
    "\t.config(\"spark.files\", \"/opt/spark/conf/hbase-site.xml,/opt/spark/conf/core-site.xml,/opt/spark/conf/hdfs-site.xml,/opt/spark/jars/hbase_sql-2.8.0.1926-cdh5.14.0.jar\")\n",
    "\t.config(\"spark.kubernetes.authenticate.caCertFile\", \"/var/run/secrets/kubernetes.io/serviceaccount/ca.crt\")\n",
    "\t.config(\"spark.kubernetes.authenticate.oauthTokenFile\", \"/var/run/secrets/kubernetes.io/serviceaccount/token\")\n",
    "\t.config(\"spark.driver.host\", driver_host)\n",
    "\t.config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"spark\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "The [Decision Tree](https://spark.apache.org/docs/latest/mllib-decision-tree.html) is a greedy algorithm that performs a recursive binary partitioning of the feature space for predictive modeling; locally optimal decisions are made at each node in hopes of a globally optimal decision.\n",
    "Because of it's greedy nature, it cannot guarantee the globally optimal tree.\n",
    "\n",
    "At its most simplified core, decision trees are simply a system of if-else statements, always taking the most optimal answer, resulting in what is hopefully the most optimal decision, although optimality is not guaranteed. Here's an illustration:\n",
    "\n",
    "<img class=\"fitwidth\" src=\"http://mines.humanoriented.com/classes/2010/fall/csci568/portfolio_exports/lguo/image/decisionTree/classification.jpg\">\n",
    "\n",
    "\n",
    "## A Scala Example\n",
    "\n",
    "The example in this notebook demonstrates how to:\n",
    "\n",
    "* Load a CSV dataset from a public S3 bucket.\n",
    "* Parse that data as an RDD.\n",
    "* Perform classification using a decision tree.\n",
    "\n",
    "Our decision tree uses Gini impurity as an impurity measure, and a maximum tree depth of 5. The test error is calculated to measure the algorithm accuracy. \n",
    "\n",
    "For more information about Decision Trees, see this [Apache Spark page](https://spark.apache.org/docs/latest/ml-classification-regression.html#decision-tree-classifier).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%scala \n",
    "import org.apache.spark.ml.clustering.KMeans\n",
    "import org.apache.spark.ml.evaluation.ClusteringEvaluator\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.ml.feature.{PCA, StandardScaler,StringIndexer,VectorAssembler,IndexToString}\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "\n",
    "//Create the data schema\n",
    "import org.apache.spark.sql.types.StructType\n",
    "val schema = new StructType()\n",
    "    .add(\"sepal_length\", \"float\")\n",
    "    .add(\"sepal_width\", \"float\")\n",
    "    .add(\"petal_length\", \"float\")\n",
    "    .add(\"petal_width\", \"float\")\n",
    "    .add(\"species\", \"string\")\n",
    "\n",
    "\n",
    "//Grabbing data\n",
    "val rdd = sc.textFile(\"s3a://splice-demo/iris.csv\")\n",
    "val header = rdd.first()\n",
    "val rdd2 = rdd.filter(x => x != header).map(_.split(\",\"))\n",
    "    .map(p => Row(p(0).toFloat,p(1).toFloat,p(2).toFloat,p(3).toFloat,p(4)))\n",
    "\n",
    "val df = spark.createDataFrame(rdd2, schema)\n",
    "\n",
    "\n",
    "val features = header.split(\",\").filter(_ != \"species\")\n",
    "\n",
    "// StringIndex our label (species) because all values (columns) must be numerical\n",
    "val indexer = new StringIndexer()\n",
    "   .setInputCol(\"species\")\n",
    "   .setOutputCol(\"label\")\n",
    "\n",
    "// Assemble our feature vector\n",
    "val assembler = new VectorAssembler()\n",
    "  .setInputCols(features)\n",
    "  .setOutputCol(\"features\")\n",
    "\n",
    "\n",
    "// Assemble our Pipeline for proper parralelism\n",
    "val pipeline = new Pipeline()\n",
    "  .setStages(Array(indexer, assembler))\n",
    "\n",
    "val df2 = pipeline.fit(df).transform(df)\n",
    "\n",
    "\n",
    "// Display feature columns\n",
    "df2.select(features.head, features:_*).display(10)\n",
    "\n",
    "// Inspect the schema\n",
    "println(\"Schema:\")\n",
    "df2.schema.foreach(i => println(i))\n",
    "println()\n",
    "// Inspect the features vector from the features (VectorAssembler)\n",
    "println(\"Features:\")\n",
    "df2.select(\"features\", features:_*).show(10, truncate=false)\n",
    "// Look at species and label coversion (StringIndexer)\n",
    "print(\"Label from Species:\")\n",
    "df2.select(\"species\", \"label\").distinct().display(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%scala \n",
    "import org.apache.spark.ml.classification.{DecisionTreeClassifier,DecisionTreeClassificationModel}\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "\n",
    "// Split the data into training and test sets (30% held out for testing)\n",
    "val splits = df2.randomSplit(Array(0.7, 0.3))\n",
    "val (trainingData, testData) = (splits(0), splits(1))\n",
    "\n",
    "// Train a DecisionTree model.\n",
    "//  Empty categoricalFeaturesInfo indicates all features are continuous.\n",
    "val numClasses = 3\n",
    "val impurity = \"gini\"\n",
    "val maxDepth = 5\n",
    "val maxBins = 32\n",
    "\n",
    "val model = new DecisionTreeClassifier()\n",
    "            .setFeaturesCol(\"features\")\n",
    "            .setLabelCol(\"label\")\n",
    "            .setMaxDepth(5)\n",
    "            .setImpurity(impurity)\n",
    "            .setMaxBins(maxBins)\n",
    "\n",
    "// Convert indexed labels back to original labels (species values).\n",
    "val labelConverter = new IndexToString()\n",
    "  .setInputCol(\"prediction\")\n",
    "  .setOutputCol(\"predictedLabel\")\n",
    "  .setLabels(Array(\"Iris-versicolor\",\"Iris-virginica\",\"Iris-setosa\"))\n",
    "\n",
    "val pipeline2 = new Pipeline()\n",
    "            .setStages(Array(model, labelConverter))\n",
    "\n",
    "val trainedModel = pipeline2.fit(trainingData)\n",
    "val predictions = trainedModel.transform(testData)\n",
    "\n",
    "// Select example rows to display.\n",
    "predictions.select(\"predictedLabel\", \"species\", \"features\").show(5)\n",
    "\n",
    "// Select (prediction, true label) and compute test error.\n",
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"accuracy\")\n",
    "val accuracy = evaluator.evaluate(predictions)\n",
    "println(s\"Accuracy = ${(accuracy)}\")\n",
    "\n",
    "val treeModel = trainedModel.stages(0).asInstanceOf[DecisionTreeClassificationModel]\n",
    "println(s\"Learned classification tree model:\\n ${treeModel.toDebugString}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark Example\n",
    "\n",
    "This section presents a PySpark Decision Tree example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "import plotly.express as px\n",
    "from beakerx.object import beakerx\n",
    "beakerx.pandas_display_table()\n",
    "\n",
    "data = spark.createDataFrame(px.data.iris()).drop('species_id')\n",
    "\n",
    "# Convert species column into int type\n",
    "si = StringIndexer(inputCol='species', outputCol='species_vec')\n",
    "\n",
    "# Create a vector of features\n",
    "cols = [c for c in data.columns if c != 'species']\n",
    "va = VectorAssembler(inputCols=cols, outputCol='features')\n",
    "\n",
    "# Define stages of a Pipeline for Spark\n",
    "pipeline = Pipeline(stages = [si, va])\n",
    "\n",
    "data = pipeline.fit(data).transform(data)\n",
    "\n",
    "# Show the final dataset\n",
    "display(data.orderBy('sepal_width').toPandas()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize our Data \n",
    "\n",
    "Let's visualize our data with [Plotly](https://plot.ly/python/)\n",
    "\n",
    "* The X axis is the `petal_length`.\n",
    "* The Y axis is `sepal_width`.\n",
    "* The Z axis is the `sepal_length`.\n",
    "* The Datapoint size is the `petal_width`.\n",
    "* The color is the species type: `versicolor, virginica, setosa`.\n",
    "\n",
    "In the next cell, you change any of the variables in the plot function to see a new chart layout; trying different combinations can give you new insight into the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hover over any datapoint to get it's exact dimensions\n",
    "px.scatter_3d(data.toPandas(), x='petal_length', y='sepal_width', z='sepal_length', size='petal_width', color='species')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Our Decision Tree\n",
    "\n",
    "Now we can create our Decision Tree to predict the species, based on its `sepal_length, sepal_width, petal_length,` and `petal_width`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from splicemachine.ml.utilities import SpliceMultiClassificationEvaluator\n",
    "\n",
    "# The data has already been preprocessed above into a feature vector called \"features\"\n",
    "# Create the decision tree\n",
    "dt = DecisionTreeClassifier(labelCol='species_vec', featuresCol='features', maxDepth=20)\n",
    "\n",
    "# Split our dataset into training and testing\n",
    "train, test = data.randomSplit([0.8,0.2])\n",
    "\n",
    "# Train on our training data\n",
    "model = dt.fit(train)\n",
    "# Make predictions\n",
    "predictions = model.transform(test)\n",
    "\n",
    "predictions.select(['features','species','species_vec','prediction']).show()\n",
    "\n",
    "# Evaluate results\n",
    "e = SpliceMultiClassificationEvaluator(spark, label_column='species_vec')\n",
    "e.input(predictions)\n",
    "results = e.get_results(dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's turns out to be quite a good Decision Tree. Run the next cell to visualize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splicemachine.ml.utilities import DecisionTreeVisualizer as dtv\n",
    "import pprint\n",
    "\n",
    "print(dtv.visualize(model, ['sepal_length', 'sepal_width', 'petal_length', 'petal_width'], ['versicolor', 'virginica' ,'setosa'],'First_Decision_Tree', visual=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}