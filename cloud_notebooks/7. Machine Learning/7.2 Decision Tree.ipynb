{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "os.environ['JDBC_HOST'] = 'jrtest01-splice-hregion'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# setup-- \n",
                "import os\n",
                "import pyspark\n",
                "from splicemachine.spark.context import PySpliceContext\n",
                "from pyspark.conf import SparkConf\n",
                "from pyspark.sql import SparkSession\n",
                "!pip install plotly\n",
                "\n",
                "# make sure pyspark tells workers to use python3 not 2 if both are installed\n",
                "os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3'\n",
                "jdbc_host = os.environ['JDBC_HOST']\n",
                "\n",
                "conf = pyspark.SparkConf()\n",
                "sc = pyspark.SparkContext(conf=conf)\n",
                "\n",
                "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
                "\n",
                "splicejdbc=f\"jdbc:splice://{jdbc_host}:1527/splicedb;user=splice;password=admin\"\n",
                "\n",
                "splice = PySpliceContext(spark, splicejdbc)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Decision Trees\n",
                "The [Decision Tree](https://spark.apache.org/docs/latest/mllib-decision-tree.html) is a greedy algorithm that performs a recursive binary partitioning of the feature space for predictive modeling. \n",
                "* Locally optimal decisions are made at each node in hopes of a globally optimal decision\n",
                "* Because of it's greedy nature, it cannot guarantee the globally optimal tree\n",
                "\n",
                "At its core (and most simplified), decision trees are simply a system of if-else statements, always taking the most optimal answer, resulting in (hopefully) the most optimal decision. See [here](http://mines.humanoriented.com/classes/2010/fall/csci568/portfolio_exports/lguo/image/decisionTree/classification.jpg)\n",
                "\n",
                "The example below demonstrates how to load a [LIBSVM](https://github.com/apache/spark/blob/master/data/mllib/sample_libsvm_data.txt) data file, parse it as an RDD of LabeledPoint and then perform classification using a decision tree with Gini impurity as an impurity measure and a maximum tree depth of 5. The test error is calculated to measure the algorithm accuracy. For more information, check out Spark's Decision Tree [page](https://spark.apache.org/docs/latest/mllib-decision-tree.html)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%scala \n",
                "import org.apache.spark.mllib.tree.DecisionTree\n",
                "import org.apache.spark.mllib.tree.model.DecisionTreeModel\n",
                "import org.apache.spark.mllib.util.MLUtils\n",
                "\n",
                "// Load and parse the data file.\n",
                "val data = MLUtils.loadLibSVMFile(sc, \"/Users/benepstein/Desktop/sample_libsvm_data.txt\")\n",
                "// Split the data into training and test sets (30% held out for testing)\n",
                "val splits = data.randomSplit(Array(0.7, 0.3))\n",
                "val (trainingData, testData) = (splits(0), splits(1))\n",
                "\n",
                "// Train a DecisionTree model.\n",
                "//  Empty categoricalFeaturesInfo indicates all features are continuous.\n",
                "val numClasses = 2\n",
                "val categoricalFeaturesInfo = Map[Int, Int]()\n",
                "val impurity = \"gini\"\n",
                "val maxDepth = 5\n",
                "val maxBins = 32\n",
                "\n",
                "val model = DecisionTree.trainClassifier(trainingData, numClasses, categoricalFeaturesInfo,\n",
                "  impurity, maxDepth, maxBins)\n",
                "\n",
                "// Evaluate model on test instances and compute test error\n",
                "val labelAndPreds = testData.map { point =>\n",
                "  val prediction = model.predict(point.features)\n",
                "  (point.label, prediction)\n",
                "}\n",
                "val testErr = labelAndPreds.filter(r => r._1 != r._2).count().toDouble / testData.count()\n",
                "println(\"Test Error = \" + testErr)\n",
                "println(\"Learned classification tree model:\\n\" + model.toDebugString)\n",
                "\n",
                "// Save and load model\n",
                "// model.save(sc, \"target/tmp/myDecisionTreeClassificationModel\")\n",
                "// val sameModel = DecisionTreeModel.load(sc, \"target/tmp/myDecisionTreeClassificationModel\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## And in PySpark"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
                "from pyspark.ml import Pipeline\n",
                "import plotly.express as px\n",
                "\n",
                "data = spark.createDataFrame(px.data.iris()).drop('species_id')\n",
                "\n",
                "# Convert species column into int type\n",
                "si = StringIndexer(inputCol='species', outputCol='species_vec')\n",
                "\n",
                "# Create a vector of features\n",
                "cols = [c for c in data.columns if c != 'species']\n",
                "va = VectorAssembler(inputCols=cols, outputCol='features')\n",
                "\n",
                "# Define stages of a Pipeline for Spark\n",
                "pipeline = Pipeline(stages = [si, va])\n",
                "\n",
                "data = pipeline.fit(data).transform(data)\n",
                "\n",
                "# Show the final dataset\n",
                "data.orderBy('sepal_width').show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Let's visualize our data with [Plotly](https://plot.ly/python/):\n",
                "* X axis will be petal_length\n",
                "* Y axis will be sepal_width\n",
                "* Z axis will sepal_length\n",
                "* Datapoint size will be petal_width\n",
                "* Color will be species type (versicolor, virginica, setosa)\n",
                "\n",
                "### In the next cell, change any of the variables in the plot function to see a new chart layout. Trying different combinations can give you new insight into the data!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Hover over any datapoint to get it's exact dimensions\n",
                "px.scatter_3d(data.toPandas(), x='petal_length', y='sepal_width', z='sepal_length', size='petal_width', color='species')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Now we can create our Decision Tree to predict which species it is based on its sepal_length, sepal_width, petal_length, and petal_width"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyspark.ml.classification import DecisionTreeClassifier\n",
                "from splicemachine.ml.utilities import SpliceMultiClassificationEvaluator\n",
                "\n",
                "# The data has already been preprocessed above into a feature vector called \"features\"\n",
                "# Create the decision tree\n",
                "dt = DecisionTreeClassifier(labelCol='species_vec', featuresCol='features', maxDepth=20)\n",
                "\n",
                "# Split our dataset into training and testing\n",
                "train, test = data.randomSplit([0.8,0.2])\n",
                "\n",
                "# Train on our training data\n",
                "model = dt.fit(train)\n",
                "# Make predictions\n",
                "predictions = model.transform(test)\n",
                "\n",
                "predictions.select(['features','species','species_vec','prediction']).show()\n",
                "\n",
                "# Evaluate results\n",
                "e = SpliceMultiClassificationEvaluator(spark, label_column='species_vec')\n",
                "e.input(predictions)\n",
                "results = e.get_results(dict=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# That's quite a good Decision Tree! \n",
                "### Let's see if we can get a better understanding for it"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from splicemachine.ml.utilities import DecisionTreeVisualizer as dtv\n",
                "import pprint\n",
                "\n",
                "print(dtv.visualize(model, ['sepal_length', 'sepal_width', 'petal_length', 'petal_width'], ['versicolor', 'virginica' ,'setosa'],'First_Decision_Tree', visual=False))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.3"
        },
        "toc": {
            "base_numbering": 1,
            "nav_menu": {},
            "number_sections": false,
            "sideBar": false,
            "skip_h1_title": false,
            "title_cell": "Table of Contents",
            "title_sidebar": "Contents",
            "toc_cell": false,
            "toc_position": {},
            "toc_section_display": false,
            "toc_window_display": false
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}