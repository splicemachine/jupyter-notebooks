{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup-- \n",
    "import os\n",
    "import pyspark\n",
    "from splicemachine.spark.context import PySpliceContext\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "splice = PySpliceContext(spark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%scala\n",
    "%%spark --start\n",
    "SparkSession.builder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMeans\n",
    "KMeans is an unsupervised-learning, clustering algorithm used to determine similarities and trends within a given dataset\n",
    "KMeans is an iterative process, where K clusters are created by the user and continualy computed on a given dataset until the data converges and the algorithm ends.\n",
    "\n",
    "## Setting up a KMeans\n",
    "To implement KMeans, you will need two things:\n",
    "\n",
    "* A dataset of structured data. Learn about structured data [here](https://www.quora.com/What-are-Structured-semi-structured-and-unstructured-data-in-Big-Data/answer/Manoj-R-Patil?srid=33JGI).\n",
    "\n",
    "* A value for K. K can be computed a number of ways, none of which are necessarily incorrect. It is dependent on the specific dataset you are working with. It is suggested to first plot your data and do trials with multiple values of K. Learn more about choosing a good K [here](https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set).\n",
    "\n",
    "\n",
    "## Computing KMeans\n",
    "A KMeans algorithm is computed in three main steps:\n",
    "\n",
    "1. K clusters are created and assigned locations, either randomly generated or randomly taken from K datapoints.\n",
    "\n",
    "2. For each datapoint in your dataset, the square Euclidian Distance is computed against all clusters until a minimum is found. That datapoint is assigned to the cluster of minimum distance.\n",
    "\n",
    "3. After all datapoints are assigned, clusters are recomputed and reassigned locations using the mean distance of its assigned datapoints.\n",
    "\n",
    "4. Repeat steps 2 and 3 until one of the following occurs:\n",
    "\n",
    "   a. A set number of iterations completes.\n",
    "   \n",
    "   b. No datapoints are reassigned to new clusters.\n",
    "   \n",
    "   c. Minimum distance changes occur within new clusters.\n",
    "\n",
    "You can learn more about the KMeans algorithm here:\n",
    "\n",
    "* [KMeans Algorithm](https://www.datascience.com/blog/introduction-to-k-means-clustering-algorithm-learn-data-science-tutorials)\n",
    "\n",
    "* [KMeans Clustering](http://scikit-learn.org/stable/modules/clustering.html#k-means)\n",
    "\n",
    "## Create a Simple KMeans Example\n",
    "\n",
    "We use a US weather dataset, engineering the features and plotting our results.\n",
    "\n",
    "* To follow along with this dataset, [download this data](https://raw.githubusercontent.com/fivethirtyeight/data/master/us-weather-history/KMDW.csv).\n",
    "\n",
    "* For more examples of KMeans and Scala examples, [visit this site](https://github.com/apache/spark/tree/master/examples/src/main/scala/org/apache/spark/examples/mllib).\n",
    "\n",
    "* To learn more about the Sum of Squares for Errors, which is used later in this notebook, [visit this site](http://www.wikihow.com/Calculate-the-Sum-of-Squares-for-Error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%scala \n",
    "import org.apache.spark.ml.clustering.KMeans\n",
    "import org.apache.spark.ml.evaluation.ClusteringEvaluator\n",
    "import org.apache.spark.sql.Row\n",
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.ml.feature.{PCA, StandardScaler}\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "\n",
    "//Create the data schema\n",
    "import org.apache.spark.sql.types.StructType\n",
    "val schema = new StructType()\n",
    "    .add(\"date\", \"string\")\n",
    "    .add(\"actual_mean_temp\", \"float\")\n",
    "    .add(\"actual_min_temp\", \"float\")\n",
    "    .add(\"actual_max_temp\", \"float\")\n",
    "    .add(\"average_min_temp\", \"float\")\n",
    "    .add(\"average_max_temp\", \"float\")\n",
    "    .add(\"record_min_temp\", \"float\")\n",
    "    .add(\"record_max_temp\", \"float\")\n",
    "    .add(\"record_min_temp_year\", \"float\")\n",
    "    .add(\"record_max_temp_year\", \"float\")\n",
    "    .add(\"actual_precipitation\", \"float\")\n",
    "    .add(\"average_precipitation\", \"float\")\n",
    "    .add(\"record_precipitation\", \"float\")\n",
    "\n",
    "\n",
    "//Grabbing data\n",
    "val datas = sc.textFile(\"s3a://splice-demo/weather_data.csv\")\n",
    "val rdd = sc.parallelize(datas.take(165))\n",
    "val header = rdd.first()\n",
    "val rdd2 = rdd.filter(x => x != header).map(_.split(\",\")).filter(x => x!= \"\")\n",
    "    .map(p => Row(p(0),p(1).toFloat,p(2).toFloat,p(3).toFloat,p(4).toFloat,p(5).toFloat,p(6).toFloat,p(7).toFloat,p(8).toFloat,p(9).toFloat,p(10).toFloat,p(11).toFloat,p(12).toFloat))\n",
    "    .filter(row => row.size == 13)\n",
    "\n",
    "\n",
    "val df = spark.createDataFrame(rdd2, schema)\n",
    "\n",
    "val features = header.split(\",\").filter(_ != \"date\")\n",
    "\n",
    "// Assemble our feature vector\n",
    "val assembler = new VectorAssembler()\n",
    "  .setInputCols(features)\n",
    "  .setOutputCol(\"features\")\n",
    "\n",
    "//Normalize all features to the same scale\n",
    "val scaler = new StandardScaler()\n",
    "  .setInputCol(\"features\")\n",
    "  .setOutputCol(\"scaledFeatures\")\n",
    "\n",
    "// PCA to reduce dimensionality\n",
    "val pca = new PCA()\n",
    "  .setInputCol(\"scaledFeatures\")\n",
    "  .setOutputCol(\"pcaFeatures\")\n",
    "  .setK(2)\n",
    "\n",
    "// Assemble our Pipeline for proper parralelism\n",
    "val pipeline = new Pipeline()\n",
    "  .setStages(Array(assembler, scaler, pca))\n",
    "\n",
    "val df2 = pipeline.fit(df).transform(df)\n",
    "\n",
    "\n",
    "// Display feature columns\n",
    "df2.select(features.head, features:_*).display(10)\n",
    "\n",
    "// Inspect the schema\n",
    "println(\"Schema:\")\n",
    "df2.schema.foreach(i => println(i))\n",
    "println()\n",
    "// Inspect the features column\n",
    "println(\"Features:\")\n",
    "df2.select(\"features\", \"pcaFeatures\").show(10, truncate=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Out PCA Features\n",
    "\n",
    "Run the following cell to plot out PCA features. The generated plot is interactive; try zooming in and out to inspect your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%scala\n",
    "import org.apache.spark.ml.linalg.DenseVector\n",
    "\n",
    "val x = df2.select(col(\"pcaFeatures\")).collect()\n",
    "\n",
    "// Get x and y values for plotting\n",
    "val xVals = x.map(i => i(0).asInstanceOf[DenseVector].values(0: Int))\n",
    "val yVals = x.map(i => i(0).asInstanceOf[DenseVector].values(1: Int))\n",
    "\n",
    "// Generate our plot\n",
    "val p = new Plot {\n",
    "    title = \"PCA Features\"\n",
    "    labelStyle = \"font-size:32px; font-weight: bold; font-family: courier; fill: green;\"\n",
    "    gridLineStyle = \"stroke: purple; stroke-width: 3;\"\n",
    "    titleStyle = \"color: green;\"\n",
    "}\n",
    "\n",
    "p.add(new Points {\n",
    "    x = xVals\n",
    "    y = yVals\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Our Clusters\n",
    "\n",
    "Run the following cell to:\n",
    "\n",
    "* cluster the data into groups\n",
    "* train a k-means model\n",
    "* make predictions\n",
    "* evaluate clustering\n",
    "* display the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%scala \n",
    "\n",
    "//Clustering data into 3 groups\n",
    "val K = 4\n",
    "val maxIterations = 5000\n",
    "\n",
    "// Trains a k-means model.\n",
    "val kmeans = new KMeans().setK(K).setSeed(1L)\n",
    "val model = kmeans.setFeaturesCol(\"pcaFeatures\").fit(df2)\n",
    "\n",
    "// Make predictions\n",
    "val predictions = model.transform(df2)\n",
    "\n",
    "// Evaluate clustering by computing Silhouette score\n",
    "val evaluator = new ClusteringEvaluator()\n",
    "\n",
    "val silhouette = evaluator.evaluate(predictions)\n",
    "println(s\"Silhouette with squared euclidean distance = $silhouette\")\n",
    "\n",
    "// Shows the result.\n",
    "println(\"Cluster Centers: \")\n",
    "model.clusterCenters.foreach(println)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Clustering Algorithm \n",
    "\n",
    "We plot our datapoints with our cluster centers to evaluate how well the algorithm worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%scala\n",
    "\n",
    "val clusterX = model.clusterCenters.map(i => i(0))\n",
    "val clusterY = model.clusterCenters.map(i => i(1))\n",
    "\n",
    "// Generate our plot with cluster centers\n",
    "val p = new Plot {\n",
    "    title = \"PCA Features with Clusters\"\n",
    "    labelStyle = \"font-size:32px; font-weight: bold; font-family: courier; fill: green;\"\n",
    "    gridLineStyle = \"stroke: purple; stroke-width: 3;\"\n",
    "    titleStyle = \"color: green;\"\n",
    "}\n",
    "\n",
    "p.add(new Points {\n",
    "    x = clusterX\n",
    "    y = clusterY\n",
    "    color = Color.red\n",
    "    size = 10\n",
    "})\n",
    "p.add(new Points {\n",
    "    x = xVals\n",
    "    y = yVals\n",
    "    color = Color.blue\n",
    "    size = 5\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our algorithm didn't do too badly: the fourth cluster handled the outliers.  Now try going back and editing the number of cluster (in the `K` variable), then re-plot and re-evaluate the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark Example\n",
    "\n",
    "For our PySpark example, we use the well-known Iris dataset to show off some of the great plotting features build into [BeakerX](http://beakerx.com) and created by [Plotly](https://plot.ly/python/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "import plotly.express as px\n",
    "\n",
    "data = spark.createDataFrame(px.data.iris()).drop('species_id')\n",
    "\n",
    "# Convert species column into int type\n",
    "si = StringIndexer(inputCol='species', outputCol='species_vec')\n",
    "\n",
    "# Create a vector of features\n",
    "cols = [c for c in data.columns if c != 'species' and c != 'petal_length']\n",
    "va = VectorAssembler(inputCols=cols, outputCol='features')\n",
    "\n",
    "# Define stages of a Pipeline for Spark\n",
    "pipeline = Pipeline(stages = [si, va])\n",
    "\n",
    "data = pipeline.fit(data).transform(data)\n",
    "\n",
    "# Show the final dataset\n",
    "data.orderBy('sepal_width').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Kmeans Algorithm and Display Results\n",
    "\n",
    "Run the following cell to run the algorithm and display results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains a k-means model.\n",
    "kmeans = KMeans().setK(3).setSeed(1)\n",
    "model = kmeans.fit(data)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(data)\n",
    "\n",
    "# Evaluate clustering by computing Silhouette score\n",
    "evaluator = ClusteringEvaluator()\n",
    "\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n",
    "\n",
    "# Shows the result.\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine the Cluster Data for Plotting\n",
    "\n",
    "Now we combine the cluster data to the dataset for plotting: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "centers = []\n",
    "for center in model.clusterCenters():\n",
    "    centers.append(center)\n",
    "# Match the schema of the centers dataframe to the predictions dataframe\n",
    "cents = pd.DataFrame(centers,columns=['sepal_length','sepal_width','petal_width'])\n",
    "# The 3 labels we are trying to cluster on\n",
    "cents['species'] = ['setosa','virginica','versicolor']\n",
    "# Add a column with the value 10 to all cluster center datapoints. This is for plotting purposes\n",
    "cents.insert(0,'center',[10]*len(cents))\n",
    "cents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the center datapoints to the predictions dataframe\n",
    "preds = predictions.toPandas()\n",
    "# Add a column with the value of 2 to all non-cluster-center datapoints. Again, for plotting purposes\n",
    "preds.insert(0, \"center\", [2]*len(preds), True)\n",
    "# Insert the cluster center data\n",
    "preds = preds.append(cents, sort=False)\n",
    "preds[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Data Points\n",
    "\n",
    "Now we can plot the datapoints, colored by cluster, with the cluster centers as the large, diamond datapoints in the *center*.\n",
    "\n",
    "You'll notice that the cluster center datapoints are larger, due to the data formatting we performed in the cells above.\n",
    "\n",
    "Again, this plot is interactive: try moving the plot around to understand the datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use plotly to visualize our data\n",
    "px.scatter_3d(preds, x='sepal_length', y='sepal_width', z='petal_width',\n",
    "              color='species', symbol='center', size='center')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
