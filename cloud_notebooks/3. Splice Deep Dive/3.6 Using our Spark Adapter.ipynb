{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "os.environ['JDBC_HOST'] = 'jrtest01-splice-hregion'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# setup-- \n",
                "import os\n",
                "import pyspark\n",
                "from splicemachine.spark.context import PySpliceContext\n",
                "from pyspark.conf import SparkConf\n",
                "from pyspark.sql import SparkSession\n",
                "\n",
                "# make sure pyspark tells workers to use python3 not 2 if both are installed\n",
                "os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3'\n",
                "jdbc_host = os.environ['JDBC_HOST']\n",
                "\n",
                "conf = pyspark.SparkConf()\n",
                "sc = pyspark.SparkContext(conf=conf)\n",
                "\n",
                "spark = SparkSession.builder.config(conf=conf).getOrCreate()\n",
                "\n",
                "splicejdbc=f\"jdbc:splice://{jdbc_host}:1527/splicedb;user=splice;password=admin\"\n",
                "\n",
                "splice = PySpliceContext(spark, splicejdbc)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "\n",
                "# Using our Spark Adapter\n",
                "\n",
                "Data Scientists have adopted Spark as the de facto data science platform, and Splice Machine provides an industry leading in-process integration to a Spark cluster. This means data scientists and data engineers can adopt the full power of Spark and manipulate dataframes but also get the power of full ANSI, ACID-compliant SQL.\n",
                "<sub>Learn more about ANSI [here](https://share.ansi.org/Shared%20Documents/News%20and%20Publications/Brochures/WhatIsANSI_brochure.pdf) and ACID compliance [here](https://www.clustrix.com/bettersql/acid-compliance-means-care/)\n",
                "\n",
                "The Splice Machine Spark adapter provides:\n",
                "\n",
                "* A durable, ACID compliant persistence model for Spark Dataframes.\n",
                "* Lazy result sets returned as Spark Dataframes.\n",
                "* Access to Spark libraries such as MLLib and GraphX.\n",
                "* Avoidance of expensive ETL of data from OLTP to OLAP.\n",
                "\n",
                "This notebook will be our first look at writing code with <b>Python</b> using a Spark library called <b>PySpark.</b> \n",
                "Learn more about <b>Python</b> [here](https://docs.python.org/3/tutorial/index.html)\n",
                "Learn more about <b>Spark</b> and <b>PySpark</b> [here](https://spark.apache.org/docs/latest/api/python/index.html)\n",
                "\n",
                "This notebook demonstrates using the Spark Adapter with Python, in these steps:\n",
                "\n",
                "1. Import the `PySpliceContextClass` to interface with the Python API.\n",
                "2. Create a simple table in Splice Machine.\n",
                "3. Create a Spark dataframe and insert that into Splice Machine\n",
                "4. Run a simple Splice Machine transaction using the Spark context.\n",
                "5. View results in the table\n",
                "\n",
                "<br />\n",
                "\n",
                "## 1. Import the PySpliceContext Class\n",
                "\n",
                "Our first step is to import the `PySpliceContext` class:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from splicemachine.spark.context import PySpliceContext"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Create a Spark context\n",
                "\n",
                "Next, we use the `PySpliceContext` to create a connection to Splice Machine:\n",
                "<sub>We can also use `inspect` to see more about what makes the PySpliceContext class"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import inspect\n",
                "jdbcURL = input('Enter your jdbcURL here, which you can find on the bottom of the cloud UI for your cluster')\n",
                "splice = PySpliceContext(spark, jdbcURL)\n",
                "print(inspect.getsource(PySpliceContext))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Create a Simple Table\n",
                "\n",
                "Now we create a simple table in Splice Machine that we'll subsequently populate:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%sql \n",
                "drop table if exists foo;\n",
                "create table foo (I int, F float, V varchar(100), primary key (I));\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Create a Spark Dataframe and Insert into Splice Machine\n",
                "\n",
                "Then we use `pyspark` to create a Spark dataframe from some sample data, and insert that into our Splice Machine table.\n",
                "\n",
                "After inserting the data, we do a `select *` to display the contents of the Splice Machine table. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pyspark.sql import Row\n",
                "l = [(0,3.14,'Turing'), (1,4.14,'Newell'), (2,5.14,'Simon'), (3,6.14,'Minsky')]\n",
                "rdd = sc.parallelize(l)\n",
                "rows = rdd.map(lambda x: Row(I=x[0], F=float(x[1]), V=str(x[2])))\n",
                "schemaRows = spark.createDataFrame(rows)\n",
                "splice.insert(schemaRows,'foo')\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%sql \n",
                "select * from foo;"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Run a Simple Splice Machine Transaction\n",
                "\n",
                "Now we'll add more data to that table in a transactional context: "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "conn = splice.getConnection()\n",
                "conn.setAutoCommit(False)\n",
                "l = [(4,3.14,'Turing'), (5,4.14,'Newell'), (6,5.14,'Simon'), (7,6.14,'Minsky')]\n",
                "rdd = sc.parallelize(l)\n",
                "rows = rdd.map(lambda x: Row(I=x[0], F=float(x[1]), V=str(x[2])))\n",
                "schemaRows = spark.createDataFrame(rows)\n",
                "splice.insert(schemaRows,'foo')\n",
                "df = splice.df(\"select * from foo\")\n",
                "df.collect\n",
                "df.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Rollback the transaction\n",
                "\n",
                "Finally, we'll rollback the transaction we just ran:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "conn.rollback()\n",
                "df = splice.df(\"select * from foo\")\n",
                "df.collect\n",
                "df.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Where to Go Next\n",
                "\n",
                "The next notebook in this presentation shows an example of <a href=\"./3.7%20Python%20MLlib%20example.ipynb\">Using the Spark Machine Learning Library (MLlib) with Splice Machine.</a>\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.7.3"
        },
        "toc": {
            "base_numbering": 1,
            "nav_menu": {},
            "number_sections": false,
            "sideBar": false,
            "skip_h1_title": false,
            "title_cell": "Table of Contents",
            "title_sidebar": "Contents",
            "toc_cell": false,
            "toc_position": {},
            "toc_section_display": false,
            "toc_window_display": false
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
