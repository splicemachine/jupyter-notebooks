{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "#s {\n",
    "}\n",
    "h1, h2, h3, h4, h5, h6, table, button, a, p, blockquote {\n",
    "font-family:Geneva;\n",
    "}\n",
    "\n",
    ".log {\n",
    "transition: all .2s ease-in-out;\n",
    "}\n",
    "\n",
    ".log:hover {a\n",
    "transform: scale(1.05);\n",
    "}\n",
    "</style>\n",
    "<div id='s' style='width:100%'>\n",
    "<center><img class='log' src='https://splicemachine.com/wp-content/uploads/splice-logo-1.png' width='20%' style='z-index:5'></center>\n",
    "<center><h1 class='log' style='font-size:50px; color:black;'>Getting Started with Splice Machine</h1></center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<style>\n",
    "blockquote{\n",
    "  font-size: 15px;\n",
    "  background: #f9f9f9;\n",
    "  border-left: 10px solid #ccc;\n",
    "  margin: .5em 10px;\n",
    "  padding: 30em, 10px;\n",
    "  quotes: \"\\201C\"\"\\201D\"\"\\2018\"\"\\2019\";\n",
    "  padding: 10px 20px;\n",
    "  line-height: 1.4;\n",
    "}\n",
    "\n",
    "blockquote:before {\n",
    "  content: open-quote;\n",
    "  display: inline;\n",
    "  height: 0;\n",
    "  line-height: 0;\n",
    "  left: -10px;\n",
    "  position: relative;\n",
    "  top: 30px;\n",
    "  bottom:30px;\n",
    "  color: #ccc;\n",
    "  font-size: 3em;\n",
    "    display:none;\n",
    "\n",
    "}\n",
    "\n",
    "p{\n",
    "  margin: 0;\n",
    "}\n",
    "\n",
    "footer{\n",
    "  margin:0;\n",
    "  text-align: right;\n",
    "  font-size: 1em;\n",
    "  font-style: italic;\n",
    "}\n",
    "</style>\n",
    "<blockquote><p class='quotation'><b><br><span style='font-size:25px'>What is Splice Machine?</span></b><br><br>Splice Machine ML isn't just a machine learning platform, it is a complete machine learning lifecycle management solution, giving you total control of your models, from retrieving data to scalable deployment.  <br><br>\n",
    "    <center><img class='log' src='https://s3.amazonaws.com/splice-demo/splice-machine-data-science-process.png' width='30%' style='z-index:5'></center>\n",
    "    <br><ul><li>Our platform runs directly on Apache Spark, allowing you to complete massive jobs in parallel</li><li>Our native <code>PySpliceContext</code> lets you directly access the data in your database and convert as a Spark DataFrame, no ETL</li><li><code>MLFlow</code> is integrated directly into all Splice Machine clusters, allowing you to keep track of your entire Data Science workflow</li><li>After you have found the best model for your task, you can easily deploy it as a table inside of our database to make predictions in real time</li><li>MLFlow does not force a standard workflow, instead it allows teams to develop their own methodology easily that fits their teams and problems</li></ul><br>In this demo we will guide you through the entire MLManager life cycle<br></p><footer>Your friends at Splice Machine</footer></blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analytics and Machine Learning powered by Spark\n",
    "## Splice Machine's platform is built around Spark. Begin by initializing a Spark Session, as well as our custom PySplice context.\n",
    "The pysplice context allows you to create a Spark Dataframe using our Native Spark DataSource. The Native Spark DataSource allows you to create a Spark dataframe directly and instantly. No need to stream data using a database driver. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Begin spark session \n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "#create pysplice context. Allows you to create a Spark dataframe using our Native Spark DataSource \n",
    "from splicemachine.spark import PySpliceContext\n",
    "splice = PySpliceContext(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start by adding data to the Splice Machine database\n",
    "## Data can also be added to the database using the 'Data Import' tab on the landing page, through a Pandas/Spark dataframe, or through SQL, as shown below.\n",
    "Documentation about importing data into the database can be found [here](https://doc.splicemachine.com/bestpractices_ingest_import.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "--This %% notation is a Jupyter magic command. It specifies that this cell should execute SQL, rather than the default Python.\n",
    "--With Splice Machine, you can also specify a cell to use R, Scala, Java, Javascript and more, all in the same notebook!\n",
    "\n",
    "-- Drop the sql table if it exists, and create a new one. \n",
    "drop table if exists cc_fraud_data; \n",
    "create table cc_fraud_data (\n",
    "    time_offset integer,\n",
    "    expected_weekly_trans_cnt double,\n",
    "    expected_weekly_trans_amnt double,\n",
    "    expected_daily_trans_cnt double,\n",
    "    expected_daily_trans_amnt double,\n",
    "    weekly_trans_cnt double,\n",
    "    weekly_trans_amnt double,\n",
    "    daily_trans_cnt double,\n",
    "    daily_trans_amnt double,\n",
    "    rolling_avg_weekly_trans_cnt double,\n",
    "    rolling_avg_weekly_trans_amnt double,\n",
    "    rolling_avg_daily_trans_cnt double,\n",
    "    rolling_avg_daily_trans_amnt double,\n",
    "    MACD_trans_amnt double,\n",
    "    MACD_trans_cnt double,\n",
    "    RSI_trans_amnt double,\n",
    "    RSI_trans_cnt double,\n",
    "    Aroon_trans_amnt double,\n",
    "    Aroon_trans_cnt double,\n",
    "    ADX_trans_amnt double,\n",
    "    ADX_trans_cnt double,\n",
    "    current_balance double,\n",
    "    rolling_avg_balance double,\n",
    "    MACD_balance double,\n",
    "    Aroon_balance double,\n",
    "    RSI_balance double,\n",
    "    ADX_balance double,\n",
    "    credit_score double,\n",
    "    credit_limit double,\n",
    "    amount decimal(10,2),\n",
    "    class_result int\n",
    ");\n",
    "\n",
    "-- insert data directly into this table from s3\n",
    "call SYSCS_UTIL.IMPORT_DATA (\n",
    "     null,\n",
    "     'cc_fraud_data',\n",
    "     null,\n",
    "     's3a://splice-demo/kaggle-fraud-data/creditcard.csv',\n",
    "     ',',\n",
    "     null,\n",
    "     null,\n",
    "     null,\n",
    "     null,\n",
    "     -1,\n",
    "     's3a://splice-demo/kaggle-fraud-data/bad',\n",
    "     null, \n",
    "     null);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use ANSI SQL to query the table we just created\n",
    "You can find documenation about our database [here](https://doc.splicemachine.com/sqlref_intro.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "SELECT \n",
    "    class_result, \n",
    "    AVG(expected_weekly_trans_cnt) as avg_expected_weekly_trans_cnt, \n",
    "    AVG(MACD_trans_amnt) as avg_MACD_trans_amnt, \n",
    "    AVG(RSI_trans_amnt) as avg_RSI_trans_amnt\n",
    "from cc_fraud_data\n",
    "group by class_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Spark dataframe using the Native Spark DataSource\n",
    "## The Native Spark DataSource creates a Spark dataframe directly from the database. There is no need to stream the data from the database to the Jupyter notebook, and back again. This is lightning fast because there is no serialization or deserialization, data is accessed directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = f\"SELECT * FROM cc_fraud_data\"\n",
    "df = splice.df(sql_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take a look at your dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Splice Machine has embedded the Spark UI directly into our notebooks, allowing you to easily track your job status."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyize your dataframe using Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "\n",
    "df.groupBy(\"CLASS_RESULT\").mean(\"CREDIT_SCORE\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with integrated, enhanced, MLflow\n",
    "## What is MLflow?\n",
    "MLflow is an open source platform to manage the ML lifecycle, including experimentation, reproducibility, deployment, and a central model registry. Splice Machine has embedded MLflow directly into our platform, and made several unique enhancements, further simplifing the ML lifecycle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take a look at the MLflow UI\n",
    "If you haven't yet run any models, your UI will be mosly empty. Once models are run, you will see the results logged in this UI. \n",
    "\n",
    "In addition to being embedded in the notebook, the MLflow UI can be viewed in a seperate tab.\n",
    "\n",
    "You can view our MLflow documentation [here](https://doc.splicemachine.com/mlmanager_api.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splicemachine.mlflow_support import *\n",
    "mlflow.register_splice_context(splice)\n",
    "\n",
    "from splicemachine.notebook import get_mlflow_ui\n",
    "get_mlflow_ui()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a new MLflow experiment\n",
    "An experiment is a object that allows you to store multiple runs, or iterations, of your model training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splicemachine.mlflow_support import *\n",
    "mlflow.register_splice_context(splice)\n",
    "\n",
    "mlflow.set_experiment('Splice Machine Demo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a model, tracking your steps using MLflow\n",
    "Any type of model can be tracked using MLflow. In this example, we are creating a simple SparkML Random Forrest Classifer, trained on a subset of the data we loaded above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from splicemachine.stats import SpliceMultiClassificationEvaluator\n",
    "from datetime import datetime\n",
    "\n",
    "# Create our dataset\n",
    "df_subset = df.select('WEEKLY_TRANS_CNT','CURRENT_BALANCE','CREDIT_SCORE','AMOUNT','CLASS_RESULT' )\n",
    "train, test = df_subset.randomSplit([0.8,0.2])\n",
    "\n",
    "#log all aspects of the model training process as a single model run\n",
    "with mlflow.start_run(run_name='sparkml run 1'):\n",
    "        \n",
    "    #Create MLflow tags. (any information about the modeling process we would like stored)\n",
    "    mlflow.set_tag('teammates', 'carol, daniel')\n",
    "    mlflow.lp('spark executors', '10')\n",
    "    \n",
    "    # Set our feature vector to be all column except the label\n",
    "    va = VectorAssembler(inputCols = train.columns[:-1], outputCol='features')\n",
    "    rf = RandomForestClassifier(labelCol='CLASS_RESULT', featuresCol='features')\n",
    "    pipeline = Pipeline(stages=[va,rf])\n",
    "    mlflow.log_feature_transformations(pipeline) #log what columns are used in the model \n",
    "    \n",
    "    #train the model\n",
    "    trainedModel = pipeline.fit(train)\n",
    "    mlflow.log_model(trainedModel, 'spark_model')# Log our model for deployment or future use\n",
    "    \n",
    "    #Calculate predictions on test data\n",
    "    predictions = trainedModel.transform(test)\n",
    "    \n",
    "    #Evaluate model performance metrics\n",
    "    ev = SpliceMultiClassificationEvaluator(spark,labelCol='CLASS_RESULT')\n",
    "    ev.input(predictions)\n",
    "    mlflow.log_metrics(ev.get_results(as_dict = True)) #log the metrics you calculate \n",
    "    \n",
    "    #log the Jupyter notebook itself to MLflow\n",
    "    mlflow.log_artifact(\"Getting Started with Splice Machine.ipynb\") \n",
    "    \n",
    "    #end the Mlflow run\n",
    "    run_id = mlflow.current_run_id()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Return to the MLflow UI, and see the model we have just trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_mlflow_ui()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database Model Deployment\n",
    "## Deploy the Machine Learning model you have created as a table in the database. When new data is inserted into this table, a prediction is automatically generated and stored in that same database table. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://splice-demo.s3.amazonaws.com/Database+Deployment.png\" width=\"700\" align=\"left\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the example below, we are deploying our model as a table called ```{schema}.spark_model```. The model itself is specified by run_id. The run_id was created by ```mlflow.log_model(trainedModel, 'spark_model') ``` in the code above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use your username as your database schema\n",
    "from splicemachine.mlflow_support.utilities import get_user\n",
    "schema = get_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the database table if it already exists \n",
    "splice._dropTableIfExists(f'{schema}.spark_model')\n",
    "\n",
    "#Create a new table called {schema}.spark_model. Deploy the mode specified by run_id into that table. \n",
    "jid = mlflow.deploy_db(f'{schema}', 'spark_model', run_id, primary_key={'MOMENT_KEY': 'INT'}, df=df_subset, model_cols=df_subset.columns[:-1], create_model_table=True, classes= ['No Fraud', 'Fraud'])\n",
    "\n",
    "#View logs of deployment process\n",
    "mlflow.watch_job(jid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert two rows of data into the table we created above. Select from that table to see the output.\n",
    "Only the features and a primary key are inserted into the table. Based on the features, the table automatically generates, and stores, the result of the Machine Learning Model. Additionally, metadata, such as who ran the model and when, are stored in the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "insert into spark_model (WEEKLY_TRANS_CNT,CURRENT_BALANCE,CREDIT_SCORE,AMOUNT, moment_key) values (-5.1, 3.5, 1.4, 0.2, 1);\n",
    "insert into spark_model (WEEKLY_TRANS_CNT,CURRENT_BALANCE,CREDIT_SCORE,AMOUNT, moment_key) values (-5.1, 3.8, 100, 300, 2);\n",
    "select * from spark_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert two additional rows of data into the table.\n",
    "Each new row much have a unique primary key, in this case, ```moment_key```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "insert into spark_model (WEEKLY_TRANS_CNT,CURRENT_BALANCE,CREDIT_SCORE,AMOUNT, moment_key) values (-4.1, 3.9, 1.4, -3, 3);\n",
    "insert into spark_model (WEEKLY_TRANS_CNT,CURRENT_BALANCE,CREDIT_SCORE,AMOUNT, moment_key) values (8.5, 114, 1120, 300, 4);\n",
    "select * from spark_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End your Spark Session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For more in-depth demos on how to use Splice Machine, including about our Feature Store, automatic model re-training, and Beakerx integraration, explore our our notebooks. Jupyter Notebooks. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "643px",
    "left": "10px",
    "top": "150px",
    "width": "1729.34px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
