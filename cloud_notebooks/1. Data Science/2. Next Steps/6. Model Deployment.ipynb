{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Talk About Model Deployment\n",
    "<blockquote>\n",
    "    Model Deployment is extrememly important and overly complex. We think that it should be as easy and straightforward as possible for Data Scientists to hand off their work to the Application Developers. We see that existing in the database itself. Straightforward APIs for the Data Scientists, and simple SQL statements for the Application Developers. When models are deployed to the database, a table is created and as soon as data enters that table, predictions are made immediately.<footer>Splice Machine</footer>\n",
    "</blockquote><br><br>\n",
    "<center><img class='log' src='https://splice-demo.s3.amazonaws.com/Database+Deployment.png' width='30%' style='z-index:5'></center><br><br>\n",
    "\n",
    "\n",
    "#### Let's take a look at deploying some simple models to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "from pyspark.sql import SparkSession\n",
    "from splicemachine.spark import PySpliceContext\n",
    "from splicemachine.mlflow_support import *\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .config('spark.dynamicAllocation.enabled','false')\\\n",
    "        .config('spark.executor.instances',2)\\\n",
    "        .getOrCreate()\n",
    "splice = PySpliceContext(spark)\n",
    "mlflow.register_splice_context(splice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(mlflow.deploy_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Choice\n",
    "<blockquote>\n",
    "    With our MLManager API, we've abstracted away the model itself, and made our functions model agnostic. Functions like <code>log_model</code> and <code>load_model</code> take any supported model type and handle the rest under the hood<footer>Splice Machine</footer>\n",
    "</blockquote>\n",
    "\n",
    "## We'll try it out with SKLearn, Spark and H2O\n",
    "<blockquote>\n",
    "   Because we're focusing on model deployment specifically, we will skip the logging of parameters and metrics etc. For more information on that, see some of our other <a href='./Examples'>MLManager tutorials</a><footer>Splice Machine</footer>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "-- Ingest Data\n",
    "drop table if exists cc_fraud_data; \n",
    "create table cc_fraud_data (\n",
    "    time_offset integer,\n",
    "    expected_weekly_trans_cnt double,\n",
    "    expected_weekly_trans_amnt double,\n",
    "    expected_daily_trans_cnt double,\n",
    "    expected_daily_trans_amnt double,\n",
    "    weekly_trans_cnt double,\n",
    "    weekly_trans_amnt double,\n",
    "    daily_trans_cnt double,\n",
    "    daily_trans_amnt double,\n",
    "    rolling_avg_weekly_trans_cnt double,\n",
    "    rolling_avg_weekly_trans_amnt double,\n",
    "    rolling_avg_daily_trans_cnt double,\n",
    "    rolling_avg_daily_trans_amnt double,\n",
    "    MACD_trans_amnt double,\n",
    "    MACD_trans_cnt double,\n",
    "    RSI_trans_amnt double,\n",
    "    RSI_trans_cnt double,\n",
    "    Aroon_trans_amnt double,\n",
    "    Aroon_trans_cnt double,\n",
    "    ADX_trans_amnt double,\n",
    "    ADX_trans_cnt double,\n",
    "    current_balance double,\n",
    "    rolling_avg_balance double,\n",
    "    MACD_balance double,\n",
    "    Aroon_balance double,\n",
    "    RSI_balance double,\n",
    "    ADX_balance double,\n",
    "    credit_score double,\n",
    "    credit_limit double,\n",
    "    amount decimal(10,2),\n",
    "    class_result int\n",
    ");\n",
    "\n",
    "call SYSCS_UTIL.IMPORT_DATA (\n",
    "     null,\n",
    "     'cc_fraud_data',\n",
    "     null,\n",
    "     's3a://splice-demo/kaggle-fraud-data/creditcard.csv',\n",
    "     ',',\n",
    "     null,\n",
    "     null,\n",
    "     null,\n",
    "     null,\n",
    "     -1,\n",
    "     's3a://splice-demo/kaggle-fraud-data/bad',\n",
    "     null, \n",
    "     null);\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set our Experiment\n",
    "mlflow.set_experiment('simple model deployment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKLearn\n",
    "#### Build our SKLearn Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from splicemachine.mlflow_support.utilities import get_user\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = splice.df('select EXPECTED_WEEKLY_TRANS_CNT,WEEKLY_TRANS_CNT,RSI_TRANS_AMNT,AMOUNT, class_result from cc_fraud_data').limit(10000).toPandas()\n",
    "# Split into train/test\n",
    "train, test = train_test_split(df, test_size=0.2)\n",
    "\n",
    "# Train, save and deploy\n",
    "with mlflow.start_run(run_name='SKlearn'):\n",
    "    model = GradientBoostingClassifier(n_estimators=10, learning_rate=1.0)\n",
    "    X_train,y_train = train[train.columns[:-1]], train[train.columns[-1]]\n",
    "    y_train = y_train.map(lambda x: int(x)) # So the model outputs int format\n",
    "    X_test,y_test = test[test.columns[:-1]], test[test.columns[-1]]\n",
    "    \n",
    "    model.fit(X_train,y_train)\n",
    "    print('MSE:', mean_squared_error(y_test, model.predict(X_test)))\n",
    "    run_id = mlflow.current_run_id()\n",
    "    # Save the model for deployment or later use\n",
    "    mlflow.log_model(model, 'sklearn_model')\n",
    "    \n",
    "# Deploy the model\n",
    "schema = get_user()\n",
    "splice.dropTableIfExists(f'{schema}.sklearn_model')\n",
    "jid = mlflow.deploy_db(schema, 'sklearn_model', run_id, primary_key={'MOMENT_KEY': 'INT'}, df=df, model_cols=list(X_train.columns), \n",
    "                 create_model_table=True)\n",
    "mlflow.watch_job(jid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That's it! \n",
    "<blockquote>\n",
    "    It really was that easy. You may be thinking \"well, now what? How do I see my model in action?\" That's a great question, and that's easy too. If you look at the output above, you can see a table called <code>SKLEARN_TABLE</code> was created for you with the columns of your model as well as the primary key provided and an extra column for prediction.<br>\n",
    "    To invoke your model, simply insert a row of data<footer>Splice Machine</footer>\n",
    "</blockquote>\n",
    "\n",
    "#### Let's use the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "insert into sklearn_model (expected_weekly_trans_cnt, weekly_trans_cnt, rsi_trans_amnt, amount, moment_key) values (1.5, 2.2, 2.5, 4.4, 1);\n",
    "insert into sklearn_model (expected_weekly_trans_cnt, weekly_trans_cnt, rsi_trans_amnt, amount, moment_key) values (2342.7, 3334.0, -23.1, 11010.9, 2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "select * from sklearn_model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretty Cool!\n",
    "<blockquote>As you can see, the <code>deploy_db</code> function created the table and injected your ML model directly inside. It also added some automatic columns to track which model is making the predictions, who is using your model and when. If you deploy more complex models with probabilities, more columns will be created to handle that. We can tell MLManager which SKlearn function call to use by passing in the <code>library_specific</code> parameter. Let's try that next.<footer>Splice Machine</footer>\n",
    "</blockquote>\n",
    "    \n",
    "#### Deploy model with complex output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This SKLearn model can also output the probability of each column. Let's deploy out model so it contains those probabilities\n",
    "print(f'Model prediction of {X_test.iloc[0].values} with probabilities:', model.predict_proba(X_test)[0], '\\n')\n",
    "\n",
    "splice.dropTableIfExists(f'{schema}.sklearn_model_probs')\n",
    "# Deploy our model\n",
    "jid = mlflow.deploy_db(schema, 'sklearn_model_probs', run_id, primary_key={'MOMENT_KEY': 'INT'}, df=df, model_cols=list(X_train.columns), \n",
    "                 create_model_table=True, classes=['no_fraud','fraud'], library_specific={'predict_call': 'predict_proba'}) # Added sklearn args\n",
    "mlflow.watch_job(jid)\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's use the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "insert into sklearn_model_probs (expected_weekly_trans_cnt, weekly_trans_cnt, rsi_trans_amnt, amount, moment_key) values (6.3, 2.9, 5.6, 1.8, 1);\n",
    "select * from sklearn_model_probs;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Great!\n",
    "<blockquote>You can see that the probabilities of each class match that of the local model prediction two cells above. The prediction column contains the index of prediction class. <br>So, prediction a of 0 means that the model is predicting the 1st column, no_fraud (remember that indexes start at 0!), just like the model.<footer>Splice Machine</footer>\n",
    "</blockquote>\n",
    "\n",
    "#### Show the Prediction from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.predict([[6.3, 2.9, 5.6, 1.8]])[0])\n",
    "print(model.predict_proba([[6.3, 2.9, 5.6, 1.8]])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkML\n",
    "<blockquote>Let's try the same thing with Spark. Although Spark is typically used for big data processesing, their <a href='https://spark.apache.org/docs/2.4.0/ml-classification-regression.html'>ML Libraries</a> come with some pretty powerful models as well. And they can scale for massive datasets too.<footer>Splice Machine</footer>\n",
    "</blockquote>\n",
    "\n",
    "#### Build and deploy a SparkML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from splicemachine.stats import SpliceMultiClassificationEvaluator\n",
    "\n",
    "# Create our dataset\n",
    "spark_df = splice.df('select EXPECTED_WEEKLY_TRANS_CNT,WEEKLY_TRANS_CNT,RSI_TRANS_AMNT,AMOUNT, class_result from cc_fraud_data')\n",
    "spark_df.show(5)\n",
    "train, test = spark_df.randomSplit([0.8,0.2])\n",
    "\n",
    "with mlflow.start_run(run_name='spark'):\n",
    "    # Set our feature vector to be all column except the label\n",
    "    va = VectorAssembler(inputCols = train.columns[:-1], outputCol='features')\n",
    "    rf = RandomForestClassifier(labelCol='CLASS_RESULT', featuresCol='features')\n",
    "    pipeline = Pipeline(stages=[va,rf])\n",
    "    \n",
    "    trainedModel = pipeline.fit(train)\n",
    "    predictions = trainedModel.transform(test)\n",
    "    # Log our model for deployment or future use\n",
    "    mlflow.log_model(trainedModel, 'spark_model')\n",
    "    \n",
    "    ev = SpliceMultiClassificationEvaluator(spark, labelCol='CLASS_RESULT')\n",
    "    ev.input(predictions)\n",
    "    run_id = mlflow.current_run_id()\n",
    "    \n",
    "splice.dropTableIfExists(f'{schema}.spark_model')\n",
    "# Deploy our model\n",
    "jid = mlflow.deploy_db(schema, 'spark_model', run_id, primary_key={'MOMENT_KEY': 'INT'}, df=spark_df, model_cols=spark_df.columns[:-1], \n",
    "                 create_model_table=True, classes=['no_fraud','fraud'])\n",
    "    \n",
    "mlflow.watch_job(jid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try out our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "insert into spark_model (expected_weekly_trans_cnt, weekly_trans_cnt, rsi_trans_amnt, amount, moment_key) values (5.1, 3.5, 1.4, 0.2, 1);\n",
    "insert into spark_model (expected_weekly_trans_cnt, weekly_trans_cnt, rsi_trans_amnt, amount, moment_key) values (2.7, 4.0, 3.1, 1.9, 3);\n",
    "select * from spark_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last but not least, H2O\n",
    "<blockquote>Lastly, we'll build an H2O model for the same prediction task. <a href='http://docs.h2o.ai/h2o/latest-stable/h2o-py/docs/modeling.html#h2ocoxproportionalhazardsestimator'>H2O AI</a> is an extrememly powerful, distributed ML framework with a plethora of Machine Learning models. These models can handle massive data, just like Spark, and they have very sophisticated algorithms.We've pre-configured <a href='http://docs.h2o.ai/sparkling-water/2.1/latest-stable/doc/pysparkling.html'>H2O PySparkling Water</a> into our system so you can immediately use it.<footer>Splice Machine</footer>\n",
    "</blockquote>\n",
    "\n",
    "#### Build an H2O model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we start our PySparkling Cluster\n",
    "from pysparkling import *\n",
    "import h2o\n",
    "# Create H2O Cluster\n",
    "conf = H2OConf().setInternalClusterMode()\n",
    "hc = H2OContext.getOrCreate(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h2o.estimators.deeplearning import H2ODeepLearningEstimator\n",
    "\n",
    "# Get data\n",
    "hdf = hc.asH2OFrame(spark_df)\n",
    "hdf['CLASS_RESULT'] = hdf['CLASS_RESULT'].asfactor()\n",
    "train, test = hdf.split_frame(ratios=[0.8])\n",
    "\n",
    "with mlflow.start_run(run_name='h2o'):\n",
    "    model = H2ODeepLearningEstimator()\n",
    "    model.train(x=train.columns[:-1],\n",
    "                  y=train.columns[-1],\n",
    "                  training_frame=train)\n",
    "    print('logloss', model.logloss())\n",
    "    \n",
    "    mlflow.log_model(model, 'h2o_model')\n",
    "    run_id = mlflow.current_run_id()\n",
    "    \n",
    "splice.dropTableIfExists(f'{schema}.h2o_model')\n",
    "# Deploy our model\n",
    "jid = mlflow.deploy_db(schema, 'h2o_model', run_id, primary_key={'MOMENT_KEY': 'INT'}, df=spark_df, model_cols=spark_df.columns[:-1], \n",
    "                 create_model_table=True, classes=['no_fraud','fraud'])\n",
    "mlflow.watch_job(jid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invoke our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "insert into h2o_model (expected_weekly_trans_cnt, weekly_trans_cnt, rsi_trans_amnt, amount, moment_key) values (6.7, 3.1, 5.6, 2.4, 1);\n",
    "insert into h2o_model (expected_weekly_trans_cnt, weekly_trans_cnt, rsi_trans_amnt, amount, moment_key) values (4.9, 3.0, 1.4, 0.2, 2);\n",
    "insert into h2o_model (expected_weekly_trans_cnt, weekly_trans_cnt, rsi_trans_amnt, amount, moment_key) values (5.6, 3.0, 4.5, 1.5, 3);\n",
    "select * from h2o_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amazing! \n",
    "<blockquote>Just like that, you've deployed 3 models to the database, one of them in two different ways! If you'd like to see everything you've learned put together in an end-to-end example, check out our <a href='./Examples'>Example</a> notebooks. <footer>Splice Machine</footer>\n",
    "</blockquote>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "190.188px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
