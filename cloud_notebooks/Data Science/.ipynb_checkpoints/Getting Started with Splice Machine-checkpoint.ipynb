{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with Splice Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you have any questions about the platform, or if you would like to schedule a demo with a member of our team, you can do so using the chatbot at the bottom right corner of the landing page. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analytics and Machine Learning powered by Spark\n",
    "#### Splice Machine's platform is built around Spark. Begin by initializing a Spark Session, as well as our custom pysplice context.\n",
    "The pysplice context allows you to create a Spark Dataframe using our Native Spark DataSource. The Native Spark DataSource allows you to create a Spark dataframe directly and instantly. No need to stream data using a database driver. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Begin spark session \n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "#create pysplice context. Allows you to create a Spark dataframe using our Native Spark DataSource \n",
    "from splicemachine.spark import PySpliceContext\n",
    "splice = PySpliceContext(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add data directly to the Splice Machine Database\n",
    "#### Data can also be added to the database using the 'Data Import' tab on the landing page, through a Pandas/Spark dataframe, or through SQL, as shown below\n",
    "Documentation about importing data into the database can be found [here](https://doc.splicemachine.com/bestpractices_ingest_import.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "--This %% notation is a Jupyter magic command. It specifies that this cell is SQL, rather than the default Python.\n",
    "--With Splice Machine, you can also specify a cell to use R, Scala, Java, Groovy and more, all in the same notebook!\n",
    "\n",
    "-- Drop the sql table if it exists, and create a new one. \n",
    "drop table if exists cc_fraud_data; \n",
    "create table cc_fraud_data (\n",
    "    time_offset integer,\n",
    "    expected_weekly_trans_cnt double,\n",
    "    expected_weekly_trans_amnt double,\n",
    "    expected_daily_trans_cnt double,\n",
    "    expected_daily_trans_amnt double,\n",
    "    weekly_trans_cnt double,\n",
    "    weekly_trans_amnt double,\n",
    "    daily_trans_cnt double,\n",
    "    daily_trans_amnt double,\n",
    "    rolling_avg_weekly_trans_cnt double,\n",
    "    rolling_avg_weekly_trans_amnt double,\n",
    "    rolling_avg_daily_trans_cnt double,\n",
    "    rolling_avg_daily_trans_amnt double,\n",
    "    MACD_trans_amnt double,\n",
    "    MACD_trans_cnt double,\n",
    "    RSI_trans_amnt double,\n",
    "    RSI_trans_cnt double,\n",
    "    Aroon_trans_amnt double,\n",
    "    Aroon_trans_cnt double,\n",
    "    ADX_trans_amnt double,\n",
    "    ADX_trans_cnt double,\n",
    "    current_balance double,\n",
    "    rolling_avg_balance double,\n",
    "    MACD_balance double,\n",
    "    Aroon_balance double,\n",
    "    RSI_balance double,\n",
    "    ADX_balance double,\n",
    "    credit_score double,\n",
    "    credit_limit double,\n",
    "    amount decimal(10,2),\n",
    "    class_result int\n",
    ");\n",
    "\n",
    "-- insert data directly into this table from s3\n",
    "call SYSCS_UTIL.IMPORT_DATA (\n",
    "     null,\n",
    "     'cc_fraud_data',\n",
    "     null,\n",
    "     's3a://splice-demo/kaggle-fraud-data/creditcard.csv',\n",
    "     ',',\n",
    "     null,\n",
    "     null,\n",
    "     null,\n",
    "     null,\n",
    "     -1,\n",
    "     's3a://splice-demo/kaggle-fraud-data/bad',\n",
    "     null, \n",
    "     null);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use ANSI SQL to query the table we just created\n",
    "You can find documenation about our database [here](https://doc.splicemachine.com/sqlref_intro.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql \n",
    "SELECT \n",
    "    class_result, \n",
    "    AVG(expected_weekly_trans_cnt) as avg_expected_weekly_trans_cnt, \n",
    "    AVG(MACD_trans_amnt) as avg_MACD_trans_amnt, \n",
    "    AVG(RSI_trans_amnt) as avg_RSI_trans_amnt\n",
    "from cc_fraud_data\n",
    "group by class_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Spark dataframe using the Native Spark DataSource\n",
    "#### The Native Spark DataSource creates a Spark dataframe directly from the database. There is no need to stream the data from the database to the Jupyter notebook, and back again. This is much faster because there is no serialization or deserialization. Data is accessed directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = f\"SELECT * FROM cc_fraud_data\"\n",
    "df = splice.df(sql_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take a look at your dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splice Machine has embedded the Spark UI directly into our notebooks, allowing you to easily track your job status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyize your dataframe using Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "\n",
    "df.groupBy(\"CLASS_RESULT\").mean(\"CREDIT_SCORE\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with integrated, enhanced, MLflow\n",
    "#### What is MLflow?\n",
    "MLflow is an open source platform to manage the ML lifecycle, including experimentation, reproducibility, deployment, and a central model registry. Splice Machine has embedded MLflow directly into our platform, and made several unique enhancements, further simplifing the ML lifecycle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take a look at the MLflow UI\n",
    "If you haven't yet run any models, your UI will be mosly empty. Once models are run, you will see the results logged in this UI. \n",
    "\n",
    "In addition to being embedded in the notebook, the MLflow UI can be viewed in a seperate tab.\n",
    "\n",
    "You can view our MLflow documentation [here](https://doc.splicemachine.com/mlmanager_api.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splicemachine.mlflow_support import *\n",
    "mlflow.register_splice_context(splice)\n",
    "\n",
    "from splicemachine.notebook import get_mlflow_ui\n",
    "get_mlflow_ui()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a new MLflow experiment\n",
    "An experiment is a object that allows you to store multiple runs, or iterations, of your model training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splicemachine.mlflow_support import *\n",
    "mlflow.register_splice_context(splice)\n",
    "\n",
    "mlflow.set_experiment('Splice Machine Demo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train a model, tracking your steps using MLflow\n",
    "Any type of model can be tracked using MLflow. In this example, we are creating a simple SparkML Random Forrest Classifer, trained on a subset of the data we loaded above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from splicemachine.stats import SpliceMultiClassificationEvaluator\n",
    "from datetime import datetime\n",
    "\n",
    "# Create our dataset\n",
    "df_subset = df.select('WEEKLY_TRANS_CNT','CURRENT_BALANCE','CREDIT_SCORE','AMOUNT','CLASS_RESULT' )\n",
    "train, test = df_subset.randomSplit([0.8,0.2])\n",
    "\n",
    "#log all aspects of the model training process as a single model run\n",
    "with mlflow.start_run(run_name='sparkml run 1'):\n",
    "        \n",
    "    #Create MLflow tags. (any information about the modeling process we would like stored)\n",
    "    mlflow.set_tag('teammates', 'carol, daniel')\n",
    "    mlflow.lp('spark executors', '10')\n",
    "    \n",
    "    # Set our feature vector to be all column except the label\n",
    "    va = VectorAssembler(inputCols = train.columns[:-1], outputCol='features')\n",
    "    rf = RandomForestClassifier(labelCol='CLASS_RESULT', featuresCol='features')\n",
    "    pipeline = Pipeline(stages=[va,rf])\n",
    "    mlflow.log_feature_transformations(pipeline) #log what columns are used in the model \n",
    "    \n",
    "    #train the model\n",
    "    trainedModel = pipeline.fit(train)\n",
    "    mlflow.log_model(trainedModel, 'spark_model')# Log our model for deployment or future use\n",
    "    \n",
    "    #Calculate predictions on test data\n",
    "    predictions = trainedModel.transform(test)\n",
    "    \n",
    "    #Evaluate model performance metrics\n",
    "    ev = SpliceMultiClassificationEvaluator(spark,labelCol='CLASS_RESULT')\n",
    "    ev.input(predictions)\n",
    "    mlflow.log_metrics(ev.get_results(as_dict = True)) #log the metrics you calculate \n",
    "    \n",
    "    #log the Jupyter notebook itself to MLflow\n",
    "    mlflow.log_artifact(\"Getting Started with Splice Machine.ipynb\") \n",
    "    \n",
    "    #end the Mlflow run\n",
    "    run_id = mlflow.current_run_id()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Return to the MLflow UI, and see the model we have just trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_mlflow_ui()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database Model Deployment\n",
    "#### Deploy the Machine Learning model you have created as a table in the database. When new data is inserted into this table, a prediction is automatically generated and stored in that same database table. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example below, we are deploying our model as a table called ```{schema}.spark_model```. The model itself is specified by run_id. The run_id was created by ```mlflow.log_model(trainedModel, 'spark_model') ``` in the code above. \n",
    "\n",
    "you can find documtation about Database Model Deployment here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use your username as your database schema\n",
    "from splicemachine.mlflow_support.utilities import get_user\n",
    "schema = get_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the database table if it already exists \n",
    "splice._dropTableIfExists(f'{schema}.spark_model')\n",
    "\n",
    "#Create a new table called {schema}.spark_model. Deploy the mode specified by run_id into that table. \n",
    "jid = mlflow.deploy_db(f'{schema}', 'spark_model', run_id, primary_key={'MOMENT_KEY': 'INT'}, df=df_subset, model_cols=df_subset.columns[:-1], create_model_table=True, classes= ['No Fraud', 'Fraud'])\n",
    "\n",
    "#View logs of deployment process\n",
    "mlflow.watch_job(jid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert two rows of data into the table we created above. Select from that table to see the output\n",
    "Only the features and a primary key are inserted into the table. Based on the features, the table automatically generates, and stores, the result of the Machine Learning Model. Additionally, metadata, such as who ran the model and when, are stored in the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "insert into spark_model (WEEKLY_TRANS_CNT,CURRENT_BALANCE,CREDIT_SCORE,AMOUNT, moment_key) values (-5.1, 3.5, 1.4, 0.2, 1);\n",
    "insert into spark_model (WEEKLY_TRANS_CNT,CURRENT_BALANCE,CREDIT_SCORE,AMOUNT, moment_key) values (-5.1, 3.8, 100, 300, 2);\n",
    "select * from spark_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insert two additional rows of data into the table\n",
    "Each new row much have a unique primary key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "insert into spark_model (WEEKLY_TRANS_CNT,CURRENT_BALANCE,CREDIT_SCORE,AMOUNT, moment_key) values (-4.1, 3.9, 1.4, -3, 3);\n",
    "insert into spark_model (WEEKLY_TRANS_CNT,CURRENT_BALANCE,CREDIT_SCORE,AMOUNT, moment_key) values (8.5, 114, 1120, 300, 4);\n",
    "select * from spark_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End your Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For more in-depth demos on how to use Splice Machine, including about our Feature Store, automatic model re-training, and Beakerx integraration, explore our other Jupyter Notebooks. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
